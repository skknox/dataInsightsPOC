{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import random as rng\n",
    "import nltk\n",
    "import fnmatch\n",
    "import docx\n",
    "from lxml import etree\n",
    "\n",
    "import docx2txt\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "import sys as sys\n",
    "from sys import platform\n",
    "import re as re\n",
    "from statistics import mode\n",
    "import datefinder\n",
    "from nltk import ne_chunk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "    if(platform == \"win32\"):\n",
    "        import win32com.client as win32\n",
    "        from win32com.client import constants\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "import subprocess\n",
    "import shutil\n",
    "import glob\n",
    "from docx.api import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Test Cell for Validating Document Conversion\n",
    "\n",
    "test_path = \"C:\\\\fileValidation\"\n",
    "doc_Path = os.path.join(test_path, 'TestDocConversion.doc')\n",
    "try:\n",
    "    if(platform == \"win32\"):\n",
    "        word = win32.Dispatch(\"Word.application\")\n",
    "        #word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "        doc = word.Documents.Open(doc_Path)\n",
    "        doc.Activate()\n",
    "\n",
    "        # Rename path with .docx\n",
    "        new_file_abs = os.path.abspath(doc_Path)\n",
    "        new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "\n",
    "        # Save and Close\n",
    "        word.ActiveDocument.SaveAs(\n",
    "            new_file_abs, FileFormat=16\n",
    "        )\n",
    "        doc.Close(False)\n",
    "        \n",
    "except Exception as e:\n",
    "    #doc.Close(False)\n",
    "    errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Test Cell for Validating Document Extraction\n",
    "test_Path = \"C:\\\\fileValidation\"\n",
    "doc_Path = os.path.join(test_path, 'TestDocXExtraction.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###Test Cell for Validating Path Creation\n",
    "test_Path = \"C:\\\\fileValidation\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## File Prep Functions\n",
    "Functions to prepare the file and text for metadata extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Is it docx or pdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def checkFileType(filename):\n",
    "        if(filename.lower().endswith(('.docx'))):\n",
    "            return 0\n",
    "        elif(filename.lower().endswith(('.pdf'))):\n",
    "            return 1\n",
    "        elif(filename.lower().endswith(('.doc'))):\n",
    "            return 2\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    text = text.replace(\"\\t\", ' ')\n",
    "    #text = text.replace(\",\", ' ')\n",
    "    \n",
    "    dblSpacesRemaining = True\n",
    "    while(dblSpacesRemaining):\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        if not \"  \" in text:\n",
    "            dblSpacesRemaining = False\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Process a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Process a .doc file on a Windows machine\n",
    "###### Requires Microsoft Word to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDocFileWindows(filepath, errorFile):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.application\")\n",
    "        doc = word.Documents.Open(filepath)\n",
    "        doc.Activate()\n",
    "\n",
    "        # Rename path with .docx\n",
    "        new_file_abs = os.path.abspath(filepath)\n",
    "        new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "\n",
    "        # Save and Close\n",
    "        word.ActiveDocument.SaveAs(\n",
    "            new_file_abs, FileFormat=16\n",
    "        )\n",
    "        doc.Close(False)\n",
    "\n",
    "        return new_file_abs\n",
    "        \n",
    "    except Exception as e:\n",
    "        errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "        print(str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Process a .doc file on a Mac/Linux OS\n",
    "##### Uses command line function textutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDocFileMac(filepath, errorFile):\n",
    "    try:\n",
    "        subprocess.run([\"textutil\", \"-convert\", \"docx\", filepath])\n",
    "        newFilePath = re.sub(r'\\.\\w+$', '.docx', filepath)\n",
    "        return newFilePath\n",
    "    except Exception as e:\n",
    "        print(\"Error processDocFileMac:  \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Begin Processing of .doc or .docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDocFile(filePath, errorFile):\n",
    "    \n",
    "    try:\n",
    "        if(platform == \"win32\"):\n",
    "            newFilePath = processDocFileWindows(filePath, errorFile)\n",
    "        else:\n",
    "            newFilePath = processDocFileMac(filePath, errorFile)\n",
    "        processedFilePath = processDocxFile(newFilePath, errorFile)\n",
    "        if(processedFilePath):\n",
    "            os.remove(filePath)\n",
    "    except Exception as e:\n",
    "        print(\"Error processDocFile:  \" + str(e))\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    return processedFilePath\n",
    "\n",
    "\n",
    "def processDocxFile(filePath, errorFile):\n",
    "    try:\n",
    "        docxText = docx2txt.process(filePath)\n",
    "        \n",
    "        replacedText = cleanText(docxText)\n",
    "        \n",
    "        fileName = os.path.split(filePath)[1]\n",
    "        \n",
    "        baseFileName = fileName[0:-5]\n",
    "        cwd = os.getcwd()\n",
    "        newFilePath = os.path.join(cwd,'output',  baseFileName + \".txt\")\n",
    "\n",
    "        singleFileDocx=open(newFilePath, 'wb+')\n",
    "        singleFileDocx.write(replacedText.encode(\"utf-8\"))\n",
    "        singleFileDocx.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are using a docx specific library to process word documents, and it doesn't work on .doc files. To get around this, we found the most reliable solution (on Windows machines) was to use a Microsoft Word module to open and resave the document in the docx format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Process pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processPDFfile(filePath, errorFile):\n",
    "    password = \"\"\n",
    "    extracted_text = \"\"\n",
    "    try:\n",
    "        #print(filePath)\n",
    "        fileName = os.path.split(filePath)[1]#.split('/')[-1]\n",
    "        #print(fileName)\n",
    "        fileName = fileName.replace(\",\", \" \")\n",
    "        baseFileName = fileName[0:-4]\n",
    "    \n",
    "        fp = open(filePath, \"rb\")\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser, password)\n",
    "        \n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "            \n",
    "        # Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "\n",
    "        # set parameters for analysis\n",
    "        laparams = LAParams()\n",
    "\n",
    "        # Create a PDFDevice object which translates interpreted information into desired format\n",
    "        # Device needs to be connected to resource manager to store shared resources\n",
    "        # device = PDFDevice(rsrcmgr)\n",
    "        # Extract the decive to page aggregator to get LT object elements\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "        # Create interpreter object to process page content from PDFDocument\n",
    "        # Interpreter needs to be connected to resource manager for shared resources and device \n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            # As the interpreter processes the page stored in PDFDocument object\n",
    "            interpreter.process_page(page)\n",
    "            # The device renders the layout from interpreter\n",
    "            layout = device.get_result()\n",
    "            # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "            for lt_obj in layout:\n",
    "                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    newText = lt_obj.get_text()\n",
    "                    newText = newText.replace('\\n', ' ')\n",
    "                    extracted_text += newText\n",
    "\n",
    "        #close the pdf file\n",
    "        fp.close()\n",
    "        \n",
    "        extracted_text = cleanText(extracted_text)#extracted_text.replace(\"\\n\", ' ')\n",
    "        cwd = os.getcwd()\n",
    "        newFilePath = os.path.join(cwd,'output', baseFileName + '-pdf' + \".txt\")\n",
    "        #print(newFilePath)\n",
    "        with open(newFilePath, 'wb+') as singleFilePDF:\n",
    "            singleFilePDF.write(extracted_text.encode(\"utf-8\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        errorFile.write(filePath + \", \" + str(e) + \"\\n\")\n",
    "        print(\"Error processing pdf: \" + str(e))\n",
    "        return None\n",
    "        #temp_df = processDF('./data/output/' + baseFileName + \".txt\")\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NLTK  Functions\n",
    "Functions involving the Natural Language Tool Kit module and concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: DD specific text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ddCleanText(text):\n",
    "    newText = text.replace('Delta Dental', 'DeltaDental')\n",
    "    newText = newText.replace('DELTA DENTAL', 'DELTADENTAL')\n",
    "    newText = newText.replace('DeltaDental Insurance Company', 'DeltaDentalInsuranceCompany')\n",
    "    return newText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    customStopWords = set(stopwords.words('english')+list(punctuation))\n",
    "    wordsWOStop=[word for word in words if word not in customStopWords]\n",
    "    \n",
    "    return wordsWOStop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized Sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSents(text):\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBigrams(tokens):\n",
    "    bigram_measures=nltk.collocations.BigramAssocMeasures();\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    sorted_bgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "    \n",
    "    return sorted_bgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTrigrams(tokens):\n",
    "    trigram_measures =nltk.collocations.TrigramAssocMeasures();\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    sorted_tgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "\n",
    "    return sorted_tgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFooter(txtFile):\n",
    "    try:\n",
    "        if not(\"-pdf.\" in txtFile):\n",
    "            docxFileName = (os.path.split(txtFile)[1]).replace(\".txt\",\".docx\")\n",
    "            filePath = os.path.join(os.getcwd(),\"processed\",docxFileName)\n",
    "            doc = docx.Document(filePath)\n",
    "            footerXML = [x.blob.decode() for x in doc.part.package.parts if x.partname.find('footer')>0]\n",
    "\n",
    "            footer = []\n",
    "            for i in range(0,len(footerXML)):\n",
    "                root = etree.XML(footerXML[i].split(\"\\n\",1)[1].replace(\"w:\", \"\"))\n",
    "                footer.append('')\n",
    "                for p in root:\n",
    "                    for r in p:\n",
    "                    #print(r.get(\"t\"))\n",
    "                        for t in r:\n",
    "                            if(t.tag == \"t\"):\n",
    "                                footer[i] = footer[i] + t.text\n",
    "\n",
    "            return footer\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### NLTK synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: translate from syn POS to nltk POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Use POS to cull wordnet synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonyms_usingPOS(word_tuple):\n",
    "    #print(word_tuple)\n",
    "    word_tagged = word_tuple[0]\n",
    "    word_pos = get_wordnet_pos(word_tuple[1])\n",
    "    syns = wn.synsets(word_tagged, pos=word_pos)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    #print(syns)\n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get synonyms of a single word. Helper function to Bigram and Trigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## You can't cull this one down with the POS b/c you can't tag a single word as a Part of Speech\n",
    "def getSyns(word):\n",
    "    syns1 = wn.synsets(word)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns1:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    \n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get a similar bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarBigrams(word1, word2):\n",
    "    \n",
    "    tagged_words = nltk.pos_tag([word1,word2])\n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            word_set.add(\" \".join([word1, word2]))\n",
    "    \n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get a similar trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarTrigrams(word1, word2, word3):\n",
    "    tagged_words = nltk.pos_tag([word1,word2,word3])\n",
    "    \n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    set3 = getSynonyms_usingPOS(tagged_words[2])\n",
    "    if not len(set3):\n",
    "        set3.add(word3)\n",
    "    \n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            for word3 in set3:\n",
    "                word_set.add(\" \".join([word1, word2, word3]))\n",
    "    #print(word_set)\n",
    "    \n",
    "    return word_set\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get synonyms from a list of key words. Returns more keywords/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonymsFromList(keywords):\n",
    "    matches = []\n",
    "\n",
    "    for kw in keywords:\n",
    "        try:\n",
    "            words = word_tokenize(kw)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        \n",
    "        if(len(words) == 1):\n",
    "           \n",
    "            syns = getSyns(words[0])\n",
    "            for syn in syns:\n",
    "                matches.append(syn)\n",
    "            \n",
    "        elif(len(words) == 2):\n",
    "           \n",
    "            syns = getSimilarBigrams(words[0],words[1])\n",
    "            matches.extend(getSimilarBigrams(words[0],words[1]))\n",
    "        elif(len(words) == 3):\n",
    "            \n",
    "            matches.extend(getSimilarTrigrams(words[0],words[1],words[2]))\n",
    "        else:\n",
    "            print(\"keyword string too long\")\n",
    "       \n",
    "    keywords.extend(matches)\n",
    "    keywords = set(keywords)\n",
    "\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Getting synonymous search terms requires a bit of filtering. If you simply pass each word into the wordnet, you get too many results, many of which have opposing definitions. One way we tried to mitigate this is by filtering on word type, which is also given by nltk. An option for future work would be to build our own corpus of words based on all of the contracts and use that to build a frequency distribution. Using this frequency distribution we could pick out synonyms that are most likely to be mentioned in a contract. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### NLTK wrapper for Stanford NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NLTK has a wrapper for the Stanford NER tagger\n",
    "\tDownload the model jar file\n",
    "\t\n",
    "The stanford NER tagger is working a bit better\n",
    "http://www.nltk.org/api/nltk.tag.html#nltk.tag.stanford.StanfordTagger\n",
    "https://nlp.stanford.edu/software/stanford-ner-2018-02-27.zip (download of jar files)\n",
    "https://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages\n",
    "\n",
    "The stanford one takes a long time to execute\n",
    "\tThere is a faster version in CoreNLP but that's all in Java and the wrapper does not interact with it\n",
    "\n",
    "Option: Create our own trained model from some of the files we already have and see how that does with the Stanford tagger. It could end up being faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MetaData\n",
    "Individual metadata extraction functions as well as components for batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Individual Metadata Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get file name\n",
    "Get the filename from a full path. Determines the OS and splits the string correctly based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFileName(fullPath):\n",
    "    try:\n",
    "        if(platform == \"win32\"):\n",
    "            fileName = fullPath.split(\"\\\\\")[-1]\n",
    "        else:\n",
    "            fileName = fullPath.split(\"/\")[-1]\n",
    "        return fileName\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing filepath: \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get group number\n",
    "Uses regex's made from keywords to attempt to find a group number in the file. Failing that, it searches the filename for the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getGroupNumber(sents_tokens, filePath):\n",
    "    \n",
    "    group_keywords = [\"group number\", \"groupnumber\"]\n",
    "    regex_exps = []\n",
    "    poss_nums = []\n",
    "    finalGN = None\n",
    "    \n",
    "    try:\n",
    "        #Create regex exps out of group number keywords\n",
    "        for kw in group_keywords:\n",
    "                temp_re = kw + \"\\W\\s*(?P<gn>\\d+)(?P<gn2>[-\\s]\\d+)\"\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "        #For each sentence, search for the expression, if found add the number to\n",
    "        #list of possible group numbers\n",
    "        for sent in sents_tokens:\n",
    "            #print(sent)\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    temp_gn = result.group('gn')\n",
    "                    temp_gn2 = result.group('gn2')\n",
    "                    if(temp_gn2):\n",
    "                        if(len(temp_gn2[1:])>len(temp_gn)):\n",
    "                            poss_nums.append(temp_gn2[1:])\n",
    "                        else:\n",
    "                            poss_nums.append(temp_gn)\n",
    "                    else:\n",
    "                        #temp_gn2 = temp_gn2[1:]\n",
    "                        #print(temp_gn2)\n",
    "                        \n",
    "                        poss_nums.append(temp_gn)\n",
    "\n",
    "        #Try and get the number from the file name, looking for list of numeric chars\n",
    "        num_regex = re.compile(\"(?P<gn>\\d+)(?P<gn2>[-\\s]+\\d+)?\")\n",
    "        fileName = getFileName(filePath)\n",
    "        fileGN = num_regex.search(fileName)\n",
    "\n",
    "        if not fileGN==None:#if they filename has a number sequence\n",
    "            temp_gn = None\n",
    "            if(fileGN.group('gn2')):\n",
    "                if(len(fileGN.group('gn'))>=len(fileGN.group('gn2')[1:])):\n",
    "                    temp_gn = fileGN.group('gn')\n",
    "                else:\n",
    "                    temp_gn = fileGN.group('gn2')[1:]\n",
    "\n",
    "            else:\n",
    "                temp_gn = fileGN.group()\n",
    "            if (temp_gn in poss_nums):#then if the file group number matches one in the document, choose it\n",
    "                finalGN = temp_gn\n",
    "            else:\n",
    "                poss_nums.append(temp_gn)#otherwise add the filename one to the list and try to get the most co\n",
    "                try:\n",
    "                    finalGN = mode(poss_nums)\n",
    "                except Exception as e:\n",
    "                    return -1\n",
    "        else: #it is none and there was no group number in the filename\n",
    "            try:\n",
    "                finalGN = mode(poss_nums)\n",
    "            except Exception as e:\n",
    "                #no mode found, couldn't find a group number\n",
    "                return -1\n",
    "    except Exception as e:\n",
    "        print(\"Error finding group number: \" + str(e))\n",
    "        return None\n",
    "    \n",
    "    return finalGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get contract start\n",
    "Uses regex and a list of keywords to attempt to find the start date of the contract. It makes multiple passes based on patterns seen in contract samples so far.\n",
    "\n",
    "Some of the passes are necessary to filter out non-date numbers that the datefinder incorrectly parses to dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractStart(sents_tokens):\n",
    "    start_keywords = [\"effective date\\S\\s*\\S\", \"effective\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:20]\n",
    "                    subset = \" \".join(subset)\n",
    "                    poss_dates.append(subset)\n",
    "                    if \"effective date\" in subset.lower():\n",
    "                        poss_dates.append(subset)\n",
    "\n",
    "                \n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\"]\n",
    "    \n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:6])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-6:])\n",
    "                    subset = subset_2 + subset_1\n",
    "\n",
    "                    m = datefinder.find_dates(subset, strict=True)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "                    \n",
    "    ## Second pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "            #print(sent)\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.search(sent)\n",
    "\n",
    "            if not year==None:\n",
    "                #print(sent)\n",
    "                #print(year.group())\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                for match in m:\n",
    "                    if match.year >= 1966:\n",
    "                        matches.append(match)\n",
    "## Last pass: try to find the most common date. If there is more than one mode, choose the earliest date\n",
    "##.           this seems to occur when it is finding the contract start and end in equal quantities\n",
    "        #print(matches)\n",
    "        try:\n",
    "            finalDate = mode(matches)\n",
    "        except ValueError as e:\n",
    "            #print(str(e))\n",
    "            if matches:\n",
    "                earliestMatch = matches[0]\n",
    "                for match in matches:\n",
    "                    if(match < earliestMatch):\n",
    "                        earliestMatch = match\n",
    "                finalDate = earliestMatch\n",
    "            else:\n",
    "                finalDate = datetime.datetime(1066, 1, 1)\n",
    "        except Exception as e:\n",
    "            print(\"Error finding Contract start date: \" + str(e))\n",
    "            return None #i.e. not only could they not find a start date, something failed\n",
    "    except Exception as e:\n",
    "        print(\"Error finding Contract start date: \" + str(e))\n",
    "        return None\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Findings during development:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With contract start date I began by searching through the tokenized sentences with a regex expression. \n",
    "I found a datefinder module to use on each flagged sentence to pull out the dates\n",
    "    Issue: the datefinder module works poorly on large, run -on sentences which are common in the contracts. It tends to find other numbers that aren't dates and try to make a date out of them.\n",
    "    \n",
    "    Sol: only take a subset, starting at the flagged word\n",
    "    \n",
    "Sometimes a match isn't found with the keywords I've seen related to the start date\n",
    "    Sol: look for keywords related to contract term and take the earlier date from that sentence\n",
    "    \n",
    "Issue: Datefinder focusing on numbers that aren't dates\n",
    "    Sol: filter for sentences that have a year (ie four digits in a row) and dates that are after Delta Dental existed (>= 1966)\n",
    "    \n",
    "Issue: Sometimes there are multiple modes. Usually I saw this when there were equal mentions of the end date\n",
    "    Sol: if there are multiple modes, take the earliest date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "####  get Contract End\n",
    "Similar to get contract start, it uses regex and keywords over multiple passes to attempt and find the contract end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractEnd(sents_tokens):\n",
    "    \n",
    "    start_keywords = [\"contract term\\S\\s*\\S\", \"contract term \", \"contract end\", \"termination date\"]\n",
    "    #start_keywords = [\"termination date\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates based on keywords\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:30]\n",
    "                    #print(subset[2])\n",
    "                    if not (subset[2]=='beginning'):\n",
    "                        subset = \" \".join(subset)\n",
    "                       # print(subset)\n",
    "                        poss_dates.append(subset)\n",
    "\n",
    "\n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\", \"\\d\\d\\d\\d\\sto\\s\\S\"]\n",
    "\n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:12])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-12:])\n",
    "                    subset = subset_2 + subset_1\n",
    "                    #print(subset)\n",
    "                    m = datefinder.find_dates(subset, strict=True)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        #print(match)\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        \n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "\n",
    "    ## Pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "            #print(sent)\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.findall(sent)\n",
    "            #print(year)\n",
    "            if len(year)>=2:\n",
    "                #print(year)\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                maxMatch = datetime.datetime(1066,1,1)\n",
    "                if not m == None:\n",
    "                    #print(m)\n",
    "                    for match in m:\n",
    "                        if match > maxMatch:\n",
    "                            maxMatch = match\n",
    "                    if maxMatch.year >= 1966:\n",
    "                        matches.append(match)\n",
    "                        #print(matches)\n",
    "            elif len(year)==1:\n",
    "                find_year_re = re.compile(\"termination date\")\n",
    "                valid_sent = find_year_re.findall(sent.lower())\n",
    "           #     print(sent)\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                if not m == None:\n",
    "          #          print(m)\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                          #  print(matches)\n",
    "                            matches.append(match)\n",
    "\n",
    "        #if(len(matches)):\n",
    "        #    print(matches)\n",
    "\n",
    "    ### If there are exactly two matches, try to find a max. If error b/c they're the same, choose one\n",
    "        if(len(matches) == 2):\n",
    "            try:\n",
    "                finalDate = max(matches)\n",
    "            except ValueError as e:\n",
    "                finalDate = matches[0]\n",
    "            except Exception as e:\n",
    "                print(\"Error finding contract end date: \" + str(e))\n",
    "                return None\n",
    "\n",
    "    ## If there are more, try and find the top two most mentioned and take the later. else just take the latest            \n",
    "        elif(len(matches) > 2):\n",
    "\n",
    "            try:\n",
    "                date1 = mode(matches)\n",
    "                matches.remove(date1)\n",
    "\n",
    "                date2 = mode(matches)\n",
    "                matches.remove(date2)\n",
    "\n",
    "                finalDate = max([date1, date2])\n",
    "            except ValueError as e:\n",
    "                #print(str(e))\n",
    "                if matches:\n",
    "                    latestMatch = matches[0]\n",
    "                    for match in matches:\n",
    "                        if(match > latestMatch):\n",
    "                            latestMatch = match\n",
    "                    finalDate = latestMatch\n",
    "            except Exception as e:\n",
    "                print(\"Error finding Contract end date: \" + str(e))\n",
    "                return None\n",
    "        else:\n",
    "            finalDate = mode(matches)\n",
    "            #return datetime.datetime(1066, 1, 1)\n",
    "            #print(\"could not find contract end for file\")\n",
    "    except Exception as e:\n",
    "        print(\"Error finding Contract end date: \" + str(e))\n",
    "        return None\n",
    "    #print(\"\\n\")\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get Contract Duration\n",
    "Uses the functions getContractStart and getContractEnd to calculate a duration if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractDuration(sents_tokens):\n",
    "    start = None\n",
    "    end = None\n",
    "    duration = None\n",
    "    \n",
    "    try:\n",
    "        start = getContractStart(sents_tokens)\n",
    "        end = getContractEnd(sents_tokens)\n",
    "        \n",
    "        if(start and end):\n",
    "            if (start.year==1066) or (end.year == 1066):\n",
    "                return -1\n",
    "            else:\n",
    "                duration = (end - start).days\n",
    "                if duration <= 0:\n",
    "                    return -1\n",
    "        else:\n",
    "            return -1\n",
    "    except Exception as e:\n",
    "        print(\"Error finding contract duration: \" + str(e))\n",
    "        return None\n",
    "\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get Client State\n",
    "Attempts to find the state most often mentioned in conjunction with statements containing keywords related to the contract holder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Helper function to create location data set from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def makeLocationDataStruct():\n",
    "    categories = []\n",
    "    location_data = {}\n",
    "    \n",
    "    us_filename = 'us_cities_states_counties.csv'\n",
    "    cwd = os.getcwd()\n",
    "    filepath = os.path.join(cwd, us_filename)\n",
    "    #print(filepath)\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='|',escapechar=',')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                for cat in row:\n",
    "                    categories.append(cat)\n",
    "                    location_data[cat]=set()\n",
    "                category_row = 0\n",
    "            else:\n",
    "                \n",
    "                for item in range(len(row)):\n",
    "                    #print(row[item])\n",
    "                    if len(row[item]):\n",
    "                        location_data[categories[item]].add(row[item])\n",
    "        location_data['State full'].add(\"Washington , DC\")\n",
    "        location_data['State full'].add(\"Washington , D.C.\")\n",
    "    csvfile.close()\n",
    "    \n",
    "    states_filename = 'state_abbrv_to_name.csv'\n",
    "    filepath = os.path.join(cwd, states_filename)\n",
    "    \n",
    "    location_data['translate_s2l'] = {}\n",
    "    location_data['translate_l2s'] = {}\n",
    "    \n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                category_row = 0\n",
    "            else:\n",
    "                location_data['translate_s2l'][row[0]] = row[1]\n",
    "                location_data['translate_l2s'][row[1]] = row[0]\n",
    "    csvfile.close()\n",
    "    #print(location_data)\n",
    "    return location_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Call function to set up location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location_data = makeLocationDataStruct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get Client State Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "code_folding": [
     0,
     7
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def checkIfCity(loc_str, loc_data, isDelta, isContractholder):\n",
    "    cities = []\n",
    "    \n",
    "    if (loc_str in loc_data['City']):\n",
    "        cities.append(loc_str)\n",
    "    return cities\n",
    "\n",
    "def checkIfState(loc_str, loc_data):\n",
    "    states = []\n",
    "    \n",
    "    if(loc_str in loc_data['State full']):\n",
    "        states.append(loc_str.lower())\n",
    "    if(loc_str in location_data['State short']):\n",
    "        try:\n",
    "            states.append(location_data['translate_s2l'][loc_str].lower())\n",
    "        except Exception as e:\n",
    "            return states\n",
    "    return states\n",
    "    \n",
    "def getClientLocation(sents_tokens, bgs, tgs, location_data, filename):\n",
    "    loc_sents = set()\n",
    "    cities = []\n",
    "    states = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for sent in sents_tokens:\n",
    "\n",
    "            sent_cities = []\n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to contractholder\" in sent.lower()\n",
    "            DCtext = [\"washington , d.c.\",\"washington , dc\", \"washington dc\",\"district of columbia\",\"washington d.c.\"]\n",
    "\n",
    "            #Used to filter out sentences in ALL caps. They interfere with the Abbreviated States lists\n",
    "            if sent.isupper():\n",
    "                sent = sent.lower()\n",
    "\n",
    "\n",
    "            text = nltk.word_tokenize(sent)\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            while (i < len(text)) and (len(text) > 2):\n",
    "                if(i < len(text)-2):\n",
    "                    text_bg = \" \".join([text[i], text[i+1]])\n",
    "                else:\n",
    "                    text_bg = \"\"\n",
    "                if(i < len(text)-2):\n",
    "                    text_tg = \" \".join([text[i], text[i+1], text[i+2]])\n",
    "                else:\n",
    "                    text_tg = \"\"\n",
    "\n",
    "                sent_cities.extend(checkIfCity(text[i], location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_bg, location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_tg, location_data, isDelta, isContractholder))\n",
    "\n",
    "                sent_states.extend(checkIfState(text[i], location_data))\n",
    "                sent_states.extend(checkIfState(text_bg, location_data))\n",
    "                sent_states.extend(checkIfState(text_tg, location_data))\n",
    "\n",
    "                i+=1\n",
    "\n",
    "            if (len(sent_states)>0) and not isDelta:\n",
    "                \n",
    "                if bool(set(sent_states).intersection(DCtext)):\n",
    "                    \n",
    "                    for st in sent_states:\n",
    "                        if st in DCtext:\n",
    "                            states.append(st.lower())\n",
    "                            if(isNotice):\n",
    "                                states.append(st.lower())\n",
    "                           \n",
    "                short_sent_states = []\n",
    "                for st in sent_states:\n",
    "                    try:\n",
    "                        short_sent_states.append(location_data['translate_l2s'][st.title()].lower())\n",
    "                    except Exception as e:\n",
    "                        e=e\n",
    "\n",
    "                for city in sent_cities:\n",
    "                    for state in sent_states+short_sent_states:\n",
    "                        \n",
    "                        if (len(checkIfState(city + \" \" + state.title(), location_data))==0) and not (city is \"New York\"):\n",
    "                                \n",
    "                                add_regex_str = str(city) + \"[-,\\s]+\" + str(state) + \"[-,\\s]\"\n",
    "                                add_regex = re.compile(add_regex_str, re.IGNORECASE)\n",
    "                                matches = add_regex.findall(sent)\n",
    "                                if matches:\n",
    "                                    cities.append(city)\n",
    "                                    states.append(state.lower())\n",
    "                                    if(isNotice):\n",
    "                                        states.append(state.lower())\n",
    "                                    \n",
    "            \n",
    "        FN_chunks = re.findall(r\"[\\w]+|[-\\s_]\", filename)\n",
    "        \n",
    "        for chunk in FN_chunks:\n",
    "            FN_state = checkIfState(chunk, location_data)\n",
    "            if len(FN_state)>0:\n",
    "                for i in range(0,5):\n",
    "                    states.append(FN_state[0].lower())\n",
    "                    i+=1\n",
    "                \n",
    "              \n",
    "        try:\n",
    "            for st in states:\n",
    "                st = st.lower()\n",
    "            final_state = mode(states)\n",
    "            #print(final_state)\n",
    "            if(len(final_state)<3):\n",
    "                try:\n",
    "                    final_state = location_data['translate_s2l'][final_state.upper()].lower()\n",
    "                except Exception as e:\n",
    "                    #print(str(e))\n",
    "                    final_state = final_state\n",
    "        except ValueError as ve:\n",
    "            #print(str(ve))\n",
    "            if(len(states)>0):\n",
    "                for st in states:\n",
    "                    st = st.lower()\n",
    "                try:\n",
    "                    final_state = mode(states)\n",
    "                except Exception as e:\n",
    "                    final_state = \"not_a_state\"\n",
    "            else:\n",
    "                final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(str(e))\n",
    "            print(\"Error finding Client location: \" + str(e))\n",
    "            final_state = None\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error finding Client location: \" + str(e))\n",
    "        return None #error in function execution\n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Findings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using Regular expressions to extract a full address from a file proves difficult. When looking for the client location, it is possible to narrow down the address location in the file by using keywords like 'contractholder' and avoiding keywords like 'deltadental.'\n",
    "\n",
    "Instead of finding the full address from scratch, we moved into identifying states and cities associated with the client in the text. This worked well for states, but less so for cities, since the variety of common words that can also be city names is wide. \n",
    "\n",
    "We've found that working from a hardcoded list is even more effective, and enables you to get more of the address. However, unlike in the case of Delta Dental office locations, a hardcoded list of all client locations is less reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Delta Office Involved:\n",
    "Uses regex combined with a list of possible states to find the (valid) state most often listed in conjunction with Delta Dental. This model is more effective than the client state model. The Client State function should be changed to mirror this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getDeltaOffice(sents_tokens, bgs, tgs, location_data):\n",
    "    loc_sents = set()\n",
    "    states = []\n",
    "    try:\n",
    "        for sent in sents_tokens:\n",
    "           \n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to delta dental\" in sent.lower()\n",
    "\n",
    "            if isDelta:\n",
    "                \n",
    "                if sent.isupper():\n",
    "                    sent = sent.lower()\n",
    "\n",
    "                #valid_states_regex = [\"[wW]ashington *,? +[dD].?[cC].?\",\"[dD]istrict [oO]f [cC]olumbia\",\"[, ]+[gG][aA][\\s,]+\",\"[gG]eorgia\",\"[, ]+[cC][aA][\\s,]+\",\"[cC]alifornia\",\"[, ]+[pP][aA][\\s,]+\",\"[pP]ennsylvania\",\"[, ]+[nN][yY][\\s,]+\",\"[nN]ew [yY]ork\",\"[, ]+[wW][vV][\\s,]+\",\"[wW]est [vV]irginia\",\"[, ]+[uU][tT][\\s,]+\",\"[uU]tah\"]                       \n",
    "                valid_states_regex = [\"[, ]+(?P<state>[gG][aA])[\\s,]+\",\"(?P<state>[gG]eorgia)\",\"[, ]+(?P<state>[cC][aA])[\\s,]+\",\"(?P<state>[cC]alifornia)\",\"[, ]+(?P<state>[pP][aA])[\\s,]+\",\"(?P<state>[pP]ennsylvania)\"]                       \n",
    "                for r in valid_states_regex:\n",
    "                    r = re.compile(r)\n",
    "                    results = r.search(sent)\n",
    "                    if(results):\n",
    "                        states.append(results.group('state'))\n",
    "            else:#(?P<word>\\b\\w+\\b)\n",
    "                state_and_cities_regex = [\"[Aa]lpharetta[, ]+(?P<state>[gG][aA])[\\s,]+\",\"[aA]lpharetta[, ]+(?P<state>[gG]eorgia)\",\n",
    "                                          \"[sS]acramento[, ]+(?P<state>[cC][aA])[\\s,]+\",\"[sS]acramento[, ]+(?P<state>[cC]alifornia)\",\n",
    "                                          \"[sS]an [dD]iego[, ]+(?P<state>[cC][aA])[\\s,]+\",\"[sS]an [dD]iego[, ]+(?P<state>[cC]alifornia)\",\n",
    "                                          \"[sS]an [fF]rancisco[, ]+(?P<state>[cC][aA])[\\s,]+\",\"[sS]an [fF]rancisco[, ]+(?P<state>[cC]alifornia)\",\n",
    "                                          \"[mM]echanicsburg[, ]+(?P<state>[pP][aA])[\\s,]+\",\"[mM]echanicsburg[, ]+(?P<state>[pP]ennsylvania)\"] \n",
    "                for r in state_and_cities_regex:\n",
    "                    r = re.compile(r)\n",
    "                    results = r.search(sent)\n",
    "                    if(results):\n",
    "                        states.append(results.group('state'))\n",
    "\n",
    "        #print(valid_states)\n",
    "        temp_states = []\n",
    "        for state in states:\n",
    "            if(len(state)==2):\n",
    "                try:\n",
    "                    long_state = location_data['translate_s2l'][state.upper()]\n",
    "                    temp_states.append(long_state.lower())\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "            else:\n",
    "                temp_states.append(state.lower())\n",
    "        states = temp_states\n",
    "        \n",
    "        try:\n",
    "            final_state = mode(states)\n",
    "            if(len(final_state)<3):\n",
    "                try:\n",
    "                    final_state = location_data['translate_s2l'][final_state.upper()]\n",
    "                except:\n",
    "                    final_state = final_state\n",
    "        except ValueError as ve:\n",
    "            final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            print(\"Error finding Delta Dental office location: \" + str(e))\n",
    "            final_state = None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error finding Delta Dental office location: \" + str(e))\n",
    "        final_state = None\n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Get contractholder name\n",
    "Searches the sentences for patterns and keywords pertaining to the contractholder name, as well as looking for matches between the filename and the first few sentences of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Helper function to find the longest matching substring from the filename to the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def longestSubstring(filename, sent):\n",
    "    filename = filename.replace(\"docx\", \"\")\n",
    "    filename = filename.replace(\"pdf\", \"\")\n",
    "    filename = filename.replace(\"txt\", \"\")\n",
    "    filename = re.sub(r'[-,()._]', r' ',filename)\n",
    "    filename = re.sub(r'\\d+', r' ', filename)\n",
    "    #print(filename)\n",
    "    fn_words = nltk.word_tokenize(filename.lower())\n",
    "    #print(fn_words)\n",
    "    \n",
    "    sent = sent.replace(\"docx\", \"\")\n",
    "    sent = sent.replace(\"pdf\", \"\")\n",
    "    sent = sent.replace(\"txt\", \"\")\n",
    "    sent = re.sub(r'[-,()._]', r' ',sent)\n",
    "    sent = re.sub(r'\\d+', r' ', sent)\n",
    "    sent_words = nltk.word_tokenize(sent.lower())\n",
    "    matches = []\n",
    "    match = []\n",
    "    i=0\n",
    "    j=0\n",
    "    \n",
    "    while(i < len(fn_words)):\n",
    "        while(j < len(sent_words)):\n",
    "            if fn_words[i] == sent_words[j]:\n",
    "                #match found\n",
    "                #match.append(fn_words[i])\n",
    "                m=i\n",
    "                k=j\n",
    "                while((m<len(fn_words)) and (k<len(sent_words)) and (fn_words[m] == sent_words[k])):\n",
    "                    match.append(fn_words[m])\n",
    "                    m+=1\n",
    "                    k+=1\n",
    "                if(len(match)>1):\n",
    "                    #re.match(r\"hello[0-9]+\", 'hello1')\n",
    "                    file_kw = [r'schedule\\s\\w{1,2}', r'attachment\\s[a-zA-Z]', r'appendix\\s[a-zA-Z]']\n",
    "                    falseMatch = 0\n",
    "                    for fm in file_kw:\n",
    "                        if(re.match(fm, \" \".join(match))):\n",
    "                            falseMatch = 1\n",
    "                    if not falseMatch:\n",
    "                        matches.append(match)\n",
    "                match = []\n",
    "                j+=1\n",
    "            else:\n",
    "                j+=1\n",
    "        j=0\n",
    "        i+=1\n",
    "            \n",
    "        \n",
    "    longest_match = []\n",
    "    if(len(matches)):\n",
    "        #print(matches)\n",
    "        \n",
    "        for m in matches:\n",
    "            m = \" \".join(m)\n",
    "            if(len(m) > len(longest_match)):\n",
    "                longest_match = m\n",
    "        \n",
    "        return longest_match\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Function searches each sentence for patterns and keywords, as well as searching the filename for the longest match among the first five sentences of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractholder(sents_tokens, fileName):\n",
    "    start_keywords = [\"contractholder name\\s?[-:]+\\s+(?P<name>[\\w\\s]+)[gG]roup\\s?[nN]umber\", \"contractholder\\s?[-:]\\s+(?P<name>[\\w\\s]+)[gG]roup\\s?[nN]umber\"]\n",
    "    regex_exps = []\n",
    "    poss_names = []\n",
    "    finalName = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = result.group('name')\n",
    "                    if not (subset.startswith('the employer')):\n",
    "                        poss_names.append(subset)\n",
    "            \n",
    "            \n",
    "            \n",
    "        for sent in sents_tokens[0:5]:\n",
    "            substring = longestSubstring(fileName, sent)\n",
    "            if substring:\n",
    "                poss_names.append(substring)\n",
    "        \n",
    "        try:\n",
    "            finalName = mode(poss_names)\n",
    "        except ValueError as e:\n",
    "            e=e\n",
    "        except Exception as e:\n",
    "            print(\"Error finding contractholder name: \" + str(e))\n",
    "            return None #i.e. not only could they not find a start date, something failed\n",
    "    except Exception as e:\n",
    "        print(\"Error finding contractholder name: \" + str(e))\n",
    "        return None\n",
    "    return finalName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### File type functions\n",
    "Classify the file based on filename and footer information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "code_folding": [
     0,
     13,
     33,
     52,
     73,
     95,
     121,
     137,
     152,
     171,
     190,
     211
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isEnterprise(fileName, footer):\n",
    "\n",
    "    isEnterprise = 0\n",
    "\n",
    "    enterpriseList = [\"-ENT\",\"ENT-\",\"E-\"]\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in enterpriseList])):\n",
    "                isEnterprise = 1\n",
    "\n",
    "    return isEnterprise\n",
    "\n",
    "\n",
    "def isASC(filePath, footer):\n",
    "    \n",
    "    isASC = 0\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "    ascList = [\"ASC-\", \"-ASC\",\"ASO-\",\"-ASO\",\" ASC\", \"ASC \",\" ASO\", \"ASO \"]\n",
    "    ascRegex = [\"[- (]ASC\"]\n",
    "    for r in ascRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isASC = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in ascList])):\n",
    "                isASC = 1\n",
    "\n",
    "    return isASC\n",
    "\n",
    "\n",
    "def isEOC(filePath, footer):\n",
    "    \n",
    "    isEOC = 0\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "    eocList = [\"EOC-\", \"-EOC\"]\n",
    "    eocRegex = [\"[- (]EOC\"]\n",
    "    for r in eocRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isEOC = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in eocList])):\n",
    "                isEOC = 1\n",
    "    return isEOC\n",
    "\n",
    "\n",
    "def isEBB(filePath, footer):\n",
    "    isEBB = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "\n",
    "    ebbList = [\"EBB-\", \"-EBB\"]\n",
    "    ebbRegex = [\"[- (]EBB\"]\n",
    "    for r in ebbRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isEBB = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in ebbList])):\n",
    "                isEBB = 1\n",
    "\n",
    "    return isEBB\n",
    "\n",
    "\n",
    "def isSchedule(filePath, footer):\n",
    "    isSchedule = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    schRegex = [\"[sS]chedule\\s+I\",\"[ (]S[I]+[) ]\",\"[sS]ch[ed]*[ ]+[I12]+\"]\n",
    "    if \"schedule\" in filename.lower():\n",
    "        isSchedule = 1\n",
    "        #print(\"in filename\")\n",
    "    else:\n",
    "        for r in schRegex:\n",
    "            r = re.compile(r)\n",
    "            results = r.search(filename)\n",
    "            if results:\n",
    "                isSchedule = 1\n",
    "                #print(filename)\n",
    "                #print(results.group())\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    return isSchedule\n",
    "\n",
    "\n",
    "def isContract(filePath, footer, sentTokens):\n",
    "    isContract = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    #print(sentTokens[0])\n",
    "    #print(\"\\n\")\n",
    "    #schRegex = [\"[cC]ontract\",\"CONTRACT\"]\n",
    "    contractList = [\"MC-\", \"-MC\"]\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in contractList])):\n",
    "                isContract = 1\n",
    "\n",
    "    if \"contract\" in filename.lower():\n",
    "        isContract = 1\n",
    "        #print(\"in filename\")\n",
    "        #print(\"\\n\")\n",
    "    if (len(sentTokens)>5):\n",
    "        for sent in sentTokens[0:5]:\n",
    "            if \"this contract is entered into\" in sent.lower():\n",
    "                isContract = 1\n",
    "\n",
    "    return isContract\n",
    "\n",
    "\n",
    "def isAttachment(filePath, footer):\n",
    "    isAttachment = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    attachRegex = [\"[aA]ttach[ment]*\"]\n",
    "\n",
    "    for r in attachRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if results:\n",
    "            isAttachment = 1\n",
    "            \n",
    "    return isAttachment\n",
    "\n",
    "\n",
    "def isAppendix(filePath, footer):\n",
    "    isAppendix = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    appRegex = [\"[aA]ppendix\"]\n",
    "\n",
    "    for r in appRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if results:\n",
    "            isAppendix = 1\n",
    "\n",
    "    return isAppendix\n",
    "\n",
    "\n",
    "def isRider(filePath, footer):\n",
    "    isRider = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    riderRegex = [\"[-( ]R\\d[\\d]?\"]\n",
    "\n",
    "    for r in riderRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if results:\n",
    "            isRider = 1\n",
    "            #print(filename)\n",
    "            #print(results.group())\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    return isRider\n",
    "\n",
    "\n",
    "def isTaxModification(filePath, footer):\n",
    "    isTaxModification = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    print(filename)\n",
    "    tmRegex = [\"[tT]ax[- ]+[mM]odif[ication]*\",\"TAX[- ]+MODIF[ICATION]*\"]\n",
    "\n",
    "    for r in tmRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if results:\n",
    "            isTaxModification = 1\n",
    "            #print(filename)\n",
    "            print(results.group())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return isTaxModification\n",
    "\n",
    "\n",
    "def isSBCModification(filePath, footer):\n",
    "    isSBC = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "\n",
    "    sbcList = [\"SBC-\", \"-SBC\"]\n",
    "    sbcRegex = [\"[- (_]SBC\"]\n",
    "    for r in sbcRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isSBC = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in sbcList])):\n",
    "                isSBC = 1\n",
    "\n",
    "    return isSBC\n",
    "\n",
    "\n",
    "def isPremiumAgreement(filePath, footer):\n",
    "    isPremiumAgreement = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "\n",
    "    PAList = [\"PRM-\", \"-PRM\"]\n",
    "    PARegex = [\"[- (]PREM AGMT\", \"[Pp]remium[ ]+[Aa]greement\"]\n",
    "    for r in PARegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isPremiumAgreement = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in PAList])):\n",
    "                isPremiumAgreement = 1\n",
    "\n",
    "    return isPremiumAgreement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use all above functions to classify the input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFileTypes(filePath, footer, sentTokens):\n",
    "    types = []\n",
    "\n",
    "    if(isEOC(filePath, footer)):\n",
    "        types.append(\"EOC\")\n",
    "\n",
    "    if(isEBB(filePath, footer)):\n",
    "        types.append(\"EBB\")\n",
    "\n",
    "    if(isSchedule(filePath, footer)):\n",
    "        types.append(\"Schedule\")\n",
    "\n",
    "    if(isContract(filePath, footer, sentTokens)):\n",
    "        types.append(\"Contract\")\n",
    "\n",
    "    if(isAttachment(filePath, footer)):\n",
    "        types.append(\"Attachment\")\n",
    "\n",
    "    if(isRider(filePath, footer)):\n",
    "        types.append(\"Rider\")\n",
    "\n",
    "    if(isTaxModification(filePath, footer)):\n",
    "        types.append(\"TaxModification\")\n",
    "\n",
    "    if(isSBCModification(filePath, footer)):\n",
    "        types.append(\"SBCModification\")\n",
    "\n",
    "    if(isPremiumAgreement(filePath, footer)):\n",
    "        types.append(\"PremiumAgreement\")\n",
    "    \n",
    "    if(isAppendix(filePath, footer)):\n",
    "        types.append(\"Appendix\")\n",
    "\n",
    "    return types\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Components for batch processing to get Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Batch pre process: Clean text and output to .txt files\n",
    "Takes a folder with all of the files to be processed and creates a .txt file of the contents. This also involves the conversion from .doc to .docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchPreProcess(errorFile, pathToData):\n",
    "    cwd = os.getcwd()\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    if(os.path.isdir(pathToData)):\n",
    "\n",
    "        for file in os.listdir(pathToData):\n",
    "            filepath = os.path.join(pathToData, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                print(\"pre-processing: \" + file)\n",
    "                try:\n",
    "                    \n",
    "                    if(checkFileType(filepath) == 0):\n",
    "                        processedTextPath = processDocxFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" +  filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 1):\n",
    "                        processedTextPath = processPDFfile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 2):\n",
    "                        processedTextPath = processDocFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    else:\n",
    "                        errorFile.write(filepath + \", pre-processing: invalid filetype\\n\")\n",
    "                        raise TypeError('This path does not lead to a valid file type!')                     \n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    errorFile.write(filepath + \", pre-processing,\" + str(e) + \"\\n\")\n",
    "                    print(\"Error pre-processing file: \" + filepath)\n",
    "\n",
    "    else:\n",
    "        print(\"Folder data/raw doesn't exist\")\n",
    "        return None\n",
    "    return \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Batch processing: Get basic information from .txt file\n",
    "Returns file information as an array of objects containing key:value information about the file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchGetTokens(errorFile, dataPath):\n",
    "    all_tokens = []\n",
    "    cwd = os.getcwd()\n",
    "    processedTextPath = \"\"\n",
    "        \n",
    "    if(os.path.isdir(dataPath)):\n",
    "\n",
    "        for file in os.listdir(dataPath):\n",
    "            filepath = os.path.join(dataPath, file)\n",
    "            if(os.path.isfile(filepath) and file.endswith(\".txt\")):\n",
    "                try:\n",
    "\n",
    "                    temp_obj = {}\n",
    "\n",
    "                    with open(filepath, 'r', encoding='utf-8') as txtFile:\n",
    "                        text = txtFile.read()\n",
    "\n",
    "                    temp_obj['filepath'] = filepath\n",
    "\n",
    "                    text = ddCleanText(text)\n",
    "                    temp_obj['cleanText'] = text\n",
    "\n",
    "                    wordTokens = getTokens(text)\n",
    "                    sentTokens = getSents(text)\n",
    "                    temp_obj['wordTokens'] = wordTokens\n",
    "                    temp_obj['sentTokens'] = sentTokens\n",
    "\n",
    "                    bgs = getBigrams(wordTokens)\n",
    "                    tgs = getTrigrams(wordTokens)\n",
    "                    temp_obj['bgs'] = bgs\n",
    "                    temp_obj['tgs'] = tgs\n",
    "\n",
    "                    temp_obj['footer'] = getFooter(filepath)\n",
    "                    txtFile.close()\n",
    "                    all_tokens.append(temp_obj)\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Folder /output doesn't exist. Pre-processing failed.\")\n",
    "        return None\n",
    "    return all_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get metadata attributes\n",
    "This function takes in a single files info and calls all of the metadata functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getMetaDataAtt(file_info):\n",
    "    #print(file_info)\n",
    "    file_attr = {}\n",
    "    file_attr['filepath'] = file_info['filepath']\n",
    "\n",
    "    fileName = getFileName(file_info['filepath'])\n",
    "    if not fileName:\n",
    "        fileName = os.path.split(file_info['filepath'])[1]\n",
    "        file_attr['fileName'] = fileName\n",
    "    else:\n",
    "        #print(fileName)\n",
    "        file_attr['fileName'] = fileName\n",
    "\n",
    "    fileType = isEnterprise(fileName, file_info['footer'])\n",
    "    file_attr['fileType'] = fileType\n",
    "\n",
    "    groupNumber = getGroupNumber(file_info['sentTokens'], file_info['filepath'])\n",
    "    file_attr['groupNumber'] = groupNumber\n",
    "\n",
    "\n",
    "    contractStartDate = getContractStart(file_info['sentTokens'])\n",
    "    file_attr['contractStartDate'] = contractStartDate\n",
    "\n",
    "\n",
    "    contractEndDate = getContractEnd(file_info['sentTokens'])\n",
    "    file_attr['contractEndDate'] = contractEndDate\n",
    "\n",
    "\n",
    "    contractDuration = getContractDuration(file_info['sentTokens'])\n",
    "    file_attr['contractDuration'] = contractDuration\n",
    "\n",
    "    \n",
    "    clientLocation = getClientLocation(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data, fileName)\n",
    "    clientLocation = clientLocation.replace(\",\",\"\")\n",
    "    file_attr['clientLocation'] = clientLocation\n",
    "\n",
    "\n",
    "    deltaOfficeLocation = getDeltaOffice(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data)\n",
    "    deltaOfficeLocation = deltaOfficeLocation.replace(\",\",\"\")\n",
    "    file_attr['deltaOfficeLocation'] = deltaOfficeLocation\n",
    "\n",
    "\n",
    "    contractholderName = getContractholder(file_info['sentTokens'], fileName)\n",
    "    file_attr['contractholderName'] = contractholderName\n",
    "\n",
    "    ent = isEnterprise(file_info['filepath'], file_info['footer'])\n",
    "    file_attr['isENT'] = ent\n",
    "\n",
    "    asc = isASC(file_info['filepath'], file_info['footer'])\n",
    "    file_attr['isASC'] = asc\n",
    "\n",
    "    fileTypes = getFileTypes(file_info['filepath'], file_info['footer'], file_info['sentTokens'])\n",
    "    typeRanks = [\"Contract\",\"EOC\",\"Attachment\",\"Schedule\",\"EBB\",\"Appendix\",\"Rider\",\"TaxModification\",\"SBCModification\",\"PremiumAgreement\"]\n",
    "    mainFileType = \"\"\n",
    "    for rank in typeRanks:\n",
    "        if rank in fileTypes:\n",
    "            mainFileType = rank\n",
    "            break\n",
    "    file_attr['mainFileType'] = mainFileType\n",
    "    file_attr['fileTypes'] = fileTypes\n",
    "\n",
    "\n",
    "    file_attr['footer'] = file_info['footer']\n",
    "    eoc = isEOC(file_info['filepath'], file_info['footer'])\n",
    "    file_attr['isEOC'] = eoc\n",
    "\n",
    "\n",
    "    return file_attr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Workspace Prep Functions\n",
    "These functions set up the workspace if it does not yet exist. These must be run first before calling any processing functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Initial setup\n",
    "    Just sets up the expected folder structure. Fills nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def setupWorkspace(rawPath, errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    print(\"cwd: \" + cwd)\n",
    "    \n",
    "    \n",
    "    processedPath = os.path.join(cwd, \"processed\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    \n",
    "    try:\n",
    "        if not(os.path.isdir(rawPath)):\n",
    "            raise Exception(rawPath + \" doesn't exist.\")\n",
    "        elif not(os.listdir(rawPath)):\n",
    "            raise Exception(rawPath + \" is empty.\")\n",
    "        else:\n",
    "        \n",
    "            if(os.path.isdir(processedPath)):\n",
    "                raise Exception(processedPath + \" already exists.\")\n",
    "            else:\n",
    "                print(\"creating \" + processedPath + \"...\") \n",
    "                os.makedirs(processedPath)\n",
    "\n",
    "            if(os.path.isdir(outputPath)):\n",
    "                raise Exception(outputPath + \" already exists.\")\n",
    "            else:\n",
    "                print(\"creating \" + outputPath + \"...\")\n",
    "                os.makedirs(outputPath)\n",
    "\n",
    "            if(os.path.isdir(dataPath)):\n",
    "                raise Exception(dataPath + \" already exists.\")\n",
    "            else:\n",
    "                print(\"creating \" + dataPath + \"...\")\n",
    "                os.makedirs(dataPath)\n",
    "            return \"Success\"\n",
    "    except Exception as e:\n",
    "        print(\"Error setting up environment: \" + str(e))\n",
    "        return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Move to processed folder\n",
    "    Moves all files in the raw folder to the processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def moveToProcessedFolder(rawPath, errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    print(\"cwd: \" + cwd)\n",
    "    \n",
    "    processedPath = os.path.join(cwd, \"processed\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    \n",
    "    try:\n",
    "        if(os.path.isdir(rawPath) and os.path.isdir(processedPath)):\n",
    "            #is it empty?\n",
    "            if not os.listdir(rawPath):\n",
    "                raise Exception(\"raw data folder is empty.\")\n",
    "            else:\n",
    "                for file in os.listdir(rawPath):\n",
    "                    print(\"processing: \" + file)\n",
    "                    #move a copy to processed Path\n",
    "                    try:\n",
    "                        #print(\"moving: \" + file)\n",
    "                        shutil.copy(os.path.join(rawPath, file), os.path.join(processedPath, file))\n",
    "                    #catch copy exception here so it doesn't stop all files (?)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error copying file to processed folder: \" + str(e))\n",
    "                return processedPath\n",
    "                        \n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Expected file structure doesn't exist.\")\n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error filling processed folder: \" + str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Fill output folder with .txt files\n",
    "    Process all files in the Processed folder and output resulting txt to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createTxtFiles(pathToData, errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    #dataPath = os.path.join(cwd, \"processed\")\n",
    "    if(os.path.isdir(pathToData)):\n",
    "\n",
    "        for file in os.listdir(pathToData):\n",
    "            filepath = os.path.join(pathToData, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                print(\"pre-processing: \" + file)\n",
    "                try:\n",
    "                    \n",
    "                    if(checkFileType(filepath) == 0):\n",
    "                        processedTextPath = processDocxFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" +  filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 1):\n",
    "                        processedTextPath = processPDFfile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 2):\n",
    "                        processedTextPath = processDocFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    else:\n",
    "                        #errorFile.write(filepath + \", pre-processing: invalid filetype\\n\")\n",
    "                        raise TypeError('This path does not lead to a valid file type!')                     \n",
    "                except Exception as e:\n",
    "                    print(\"Error pre-processing: \" + str(e))\n",
    "                    #errorFile.write(filepath + \", pre-processing,\" + str(e) + \"\\n\")\n",
    "                    print(\"Error pre-processing file: \" + filepath)\n",
    "\n",
    "    else:\n",
    "        print(\"Folder data/raw doesn't exist\")\n",
    "        return None\n",
    "    return \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rawToWorkspace(pathToRawData):\n",
    "    errorFilePath = os.path.join(os.getcwd(),'cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "    \n",
    "    setup = setupWorkspace(pathToRawData,errorFile)\n",
    "    if(setup):\n",
    "        moveToProcessed = moveToProcessedFolder(pathToRawData,errorFile)\n",
    "        if(moveToProcessed):\n",
    "            createTxt = createTxtFiles(os.path.join(os.getcwd(), \"processed\"),errorFile)\n",
    "            if(createTxt):\n",
    "                errorFile.close()\n",
    "                return \"Success\"\n",
    "    errorFile.close()\n",
    "    return None\n",
    "    \n",
    "#rawToWorkspace(\"C:\\\\Users\\\\Sydney.knox\\\\Documents\\\\rawDataDI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data by Group Number\n",
    "These functions explored taking the information found by the metadata attributes and deviding the files into a folder structure by: Group Number --> Contract Holder Name --> Singular Contract\n",
    "\n",
    "These functions are not currently being utilized in the final workflow, but are functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Create Group folders and move group files in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getGroupFolders():\n",
    "    cwd = os.getcwd()\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    \n",
    "    errorFilePath = os.path.join(os.getcwd(), 'cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "    \n",
    "    try:\n",
    "        if not(os.path.isdir(dataPath)):\n",
    "            raise Exception(dataPath + \" doesn't exist.\")\n",
    "        elif not(os.path.isdir(outputPath)):\n",
    "            raise Exception(outputPath + \" doesn't exist.\")\n",
    "        elif not(os.listdir(outputPath)):\n",
    "            raise Exception(dataPath + \" is empty.\")\n",
    "        else:\n",
    "            #get all of the base info for each file\n",
    "            base_info = batchGetTokens(errorFile, outputPath)\n",
    "            if not base_info:\n",
    "                print(\"Error in getting tokens\")\n",
    "            else:\n",
    "                \n",
    "                sorted_file_info = {}\n",
    "                #print(sorted_file_info)\n",
    "                #Get all the meta data for each file\n",
    "                for file in base_info:\n",
    "                    destPath = dataPath\n",
    "                    \n",
    "                    file_attr = getMetaDataAtt(file)\n",
    "                    if((file_attr['groupNumber']) and not(int(file_attr['groupNumber']) is -1)):\n",
    "                        gn = file_attr['groupNumber']\n",
    "                        destPath = os.path.join(destPath, str(int(gn)))\n",
    "                    else:\n",
    "                        gn = -1\n",
    "                    #destPath = os.path.join(destPath, \"no_group_number\")\n",
    "                    try:\n",
    "                        sorted_file_info[str(int(gn))].append(file_attr)\n",
    "                    except KeyError as e:\n",
    "                        sorted_file_info[str(int(gn))] = []\n",
    "                        sorted_file_info[str(int(gn))].append(file_attr)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error finding group number folder for file: \" + str(e))\n",
    "\n",
    "                for group in sorted_file_info:\n",
    "                    print(group)\n",
    "                    group_start_dates = set()\n",
    "                    group_ch_names = set()\n",
    "                    oneStartDate = 0\n",
    "                    oneCHName = 0\n",
    "                    \n",
    "                    \n",
    "                    for file in sorted_file_info[group]:\n",
    "                        if(file['contractStartDate'].year > 1950):\n",
    "                            group_start_dates.add(file['contractStartDate'].date())\n",
    "                        if(len(file['contractholderName'])>0):\n",
    "                            group_ch_names.add(file['contractholderName'].rstrip())\n",
    "                \n",
    "                \n",
    "                    if(len(group_start_dates)==1):\n",
    "                        oneStartDate = 1\n",
    "                    elif(len(group_start_dates)==0):\n",
    "                        oneStartDate = 1\n",
    "                        group_start_dates.add(\"unknown_startDate\")\n",
    "                    if(len(group_ch_names)==1):\n",
    "                        oneCHName = 1\n",
    "                    elif(len(group_ch_names)==0):\n",
    "                        oneCHName = 1\n",
    "                        group_ch_names.add(\"unknown_name\")\n",
    "\n",
    "                    for file in sorted_file_info[group]:\n",
    "                        if(int(group) == -1):\n",
    "                            destPath = os.path.join(dataPath, \"no_group_number\")\n",
    "                        else:\n",
    "                            destPath = os.path.join(dataPath, str(int(group)))\n",
    "                        #print(destPath)\n",
    "                        if oneCHName:\n",
    "                            temp = group_ch_names.pop()\n",
    "                            destPath = os.path.join(destPath, temp)\n",
    "                            #print(destPath)\n",
    "                            group_ch_names.add(temp)\n",
    "                        else:\n",
    "                            if(len(file['contractholderName'])>0):\n",
    "                                destPath = os.path.join(destPath, file['contractholderName'])\n",
    "                            else:\n",
    "                                destPath = os.path.join(destPath, \"unknown_name\")\n",
    "                        if oneStartDate:\n",
    "                            temp = group_start_dates.pop()\n",
    "                            destPath = os.path.join(destPath, str(temp))\n",
    "                            #print(destPath)\n",
    "                            group_start_dates.add(temp)\n",
    "                        else:\n",
    "                            destPath = os.path.join(destPath, str(file['contractStartDate'].date()))\n",
    "                        \n",
    "                        try:\n",
    "                            print(destPath)\n",
    "                            os.makedirs(destPath, exist_ok=True)\n",
    "                            #print(file['filepath'])\n",
    "                            shutil.copy(file['filepath'], destPath)\n",
    "                        except Exception as e:\n",
    "                            print(\"Error creating folder: \" + str(e))\n",
    "        \n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(\"Error organizing data into group and contract folders: \" + str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Run 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getGroupFolders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Collect information and output a csv using group folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "code_folding": [
     0,
     47
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createGroupCSV():\n",
    "    cwd = os.getcwd()\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "    #attrByGroupFilePath = os.path.join(os.getcwd(), 'data', 'raw_attr_data_byGroup.csv')\n",
    "    #attrByGroupFile = open(attrByGroupFilePath, 'w')\n",
    "\n",
    "    try:\n",
    "        if(os.path.isdir(dataPath)):\n",
    "\n",
    "            for group in os.listdir(dataPath):\n",
    "                print(group)            \n",
    "                groupPath = os.path.join(dataPath, group)\n",
    "                if(os.path.isdir(groupPath)):\n",
    "                    base_info = batchGetTokens(errorFile, groupPath)\n",
    "                    if not base_info:\n",
    "                        print(\"Error in getting tokens\")\n",
    "                    else:\n",
    "                        outputFilePath = os.path.join(groupPath,'group_'+ group + '_attribute_data.csv')\n",
    "                        outputFile=open(outputFilePath, 'w')    \n",
    "                        first_row = 1\n",
    "\n",
    "                        for file in base_info:\n",
    "                            file_attr = getMetaDataAtt(file)\n",
    "                            if first_row:\n",
    "                                for key in file_attr:\n",
    "                                    outputFile.write(key + \",\")\n",
    "                                outputFile.write(\"\\n\")\n",
    "                                first_row = 0\n",
    "                            else:\n",
    "                                for attr in file_attr:\n",
    "                                    outputFile.write(str(file_attr[attr])+ \",\")\n",
    "                            outputFile.write(\"\\n\")\n",
    "\n",
    "                        outputFile.close()\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"data folder doesn't exist.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error printing individual group attributes csv: \" + str(e))\n",
    "\n",
    "    errorFile.close()\n",
    "    \n",
    "    \n",
    "def collectGroupInfo():\n",
    "    cwd = os.getcwd()\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "    attrByGroupFilePath = os.path.join(os.getcwd(), 'data', 'raw_attr_data_byGroup.csv')\n",
    "    attrByGroupFile = open(attrByGroupFilePath, 'w')\n",
    "\n",
    "    try:\n",
    "        if(os.path.isdir(dataPath)):\n",
    "\n",
    "            for group in os.listdir(dataPath):\n",
    "                \n",
    "                \n",
    "                \n",
    "                groupPath = os.path.join(dataPath, group)\n",
    "                if(os.path.isdir(groupPath)):\n",
    "                    print(group)\n",
    "                    group_attr = {}\n",
    "                    first_file = 1\n",
    "                    \n",
    "                    base_info = batchGetTokens(errorFile, groupPath)\n",
    "                    if not base_info:\n",
    "                        print(\"Error in getting tokens\")\n",
    "                    else:\n",
    "                        \n",
    "                        for file in base_info:\n",
    "                            file_attr = getMetaDataAtt(file)\n",
    "                            #print(file_attr)\n",
    "                            if first_file:\n",
    "                                for key in file_attr:\n",
    "                                    try:\n",
    "                                        group_attr[key] = []\n",
    "                                    except Exception as e:\n",
    "                                        print(str(e))\n",
    "                                first_file = 0\n",
    "                            \n",
    "                            print(group_attr)\n",
    "                            \n",
    "                            for attr in file_attr:\n",
    "                                #print(attr)\n",
    "                                #print(file_attr[attr] == \"-1\")\n",
    "                                #print(attr)\n",
    "                                #print((attr is 'groupNumber') or (attr is 'contractDuration'))\n",
    "                                if((attr is 'groupNumber') or (attr is 'contractDuration')):\n",
    "                                    #print(file_attr[attr] == \"-1\")\n",
    "                                    #print(file_attr[attr] == -1)\n",
    "                                    if not ((file_attr[attr]) == -1):\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                                if((attr is 'contractStartDate') or (attr is 'contractEndDate')):\n",
    "                                    #print((file_attr[attr] == datetime.datetime(1066,1,1)))\n",
    "                                    if not (file_attr[attr] == datetime.datetime(1066,1,1)):\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                                if((attr is 'clientLocation') or (attr is 'deltaOfficeLocation')):\n",
    "                                    #print(file_attr[attr] is 'not_a_state')\n",
    "                                    if not file_attr[attr] is 'not_a_state':\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                                if(attr is 'contractHolderName'):\n",
    "                                    if not file_attr[attr] is '':\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                            print(group_attr)\n",
    "                        #attrByGroupFile.write(\"\\n\")\n",
    "                        for attr in group_attr:\n",
    "                            #print(group_attr[attr])\n",
    "                            try:\n",
    "                                attrMode = mode(group_attr[attr])\n",
    "                                attrByGroupFile.write(str(attrMode) + \",\")\n",
    "                                print(attrMode)\n",
    "                            except ValueError as ve:\n",
    "                                if(len(group_attr[attr])>0):\n",
    "                                    attrByGroupFile.write(str(set(group_attr[attr])) + \",\")\n",
    "                                else:\n",
    "                                    attrByGroupFile.write(\",\")\n",
    "                            except Exception as e:\n",
    "                                #print(attr + \"has no mode.\")\n",
    "                                attrByGroupFile.write(\",\")\n",
    "                                print(\"Error with attr \" + str(attr) + \": \" + str(e))\n",
    "                        attrByGroupFile.write(\"\\n\")\n",
    "                            \n",
    "\n",
    "        else:\n",
    "            raise Exception(\"data folder doesn't exist.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error creating consolidated group csv: \" + str(e))\n",
    "\n",
    "    errorFile.close()\n",
    "    attrByGroupFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collectGroupInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Rate Table Extraction\n",
    "All functions for extracting the rate tables and outputting the information to csv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define if table object is a Rate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isRateTable(table):\n",
    "    count = 0\n",
    "    paymentFlag = 0\n",
    "    percentFind = re.compile(\"%\")\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            result = re.findall(percentFind, cell.text)\n",
    "            if(len(result)):\n",
    "                count += len(result)\n",
    "            if(((cell.text).lower()).find('contractholder shall pay') > -1) or (((cell.text).lower()).find('primary enrollee shall pay') > -1):\n",
    "                paymentFlag = 1\n",
    "    if (count >= 2) and (paymentFlag == 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define if table object is a Multi-plan table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isMultiPlan(table):\n",
    "    isMultiPlan = False\n",
    "    multiPlanList = [\"high plan\", \"enhanced plan\", \"plan design\"]\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            if(any([x in (cell.text).lower() for x in multiPlanList])):\n",
    "                isMultiPlan = True\n",
    "    return isMultiPlan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Process a table object identified as a normalized, single plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processSinglePlan(table):\n",
    "    first_row=1\n",
    "    first_row_found = 0\n",
    "    title_row=1\n",
    "    temp_obj = []\n",
    "    row_template = []\n",
    "\n",
    "    for row in table.rows:\n",
    "\n",
    "        if(((row.cells[0]).text).lower().find(\"contract benefit level\") > -1):\n",
    "            first_row_found = 1\n",
    "        else:\n",
    "            if first_row_found:\n",
    "\n",
    "                if(title_row):\n",
    "                    row_template = []\n",
    "                    for cell in row.cells:\n",
    "                        row_template.append(cell.text)\n",
    "                    title_row = 0\n",
    "                else:\n",
    "                    new_row = {}\n",
    "                    for cat in row_template:\n",
    "                        new_row[cat] = \"\"\n",
    "                    for index, cell in enumerate(row.cells):\n",
    "                        new_row[row_template[index]] = cell.text\n",
    "\n",
    "                    temp_obj.append(new_row)\n",
    "\n",
    "    if not first_row_found:#never found contract benefit levels\n",
    "         for row in table.rows:\n",
    "            ppoFlag = 0\n",
    "            non_ppoFlag = 0\n",
    "\n",
    "            ppoList = [\"ppo providers\", \"pposm providers\", \"dpo providers\", \"delta dental ppo\", \"in-network\"]\n",
    "            non_ppoList = [\"non-delta dental providers\",\"out-of-network\"]\n",
    "\n",
    "            for cell in row.cells:\n",
    "                #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in ppoList]):\n",
    "                    ppoFlag = 1\n",
    "                    #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in non_ppoList]):\n",
    "                    non_ppoFlag = 1\n",
    "                    #print(cell.text)\n",
    "\n",
    "            if(ppoFlag and non_ppoFlag):\n",
    "                first_row_found = 1\n",
    "\n",
    "            if first_row_found:\n",
    "                if(title_row):\n",
    "                    row_template = []\n",
    "                    for cell in row.cells:\n",
    "                        row_template.append(cell.text)\n",
    "                    title_row = 0\n",
    "                    #print(row_template)\n",
    "                else:\n",
    "                    new_row = {}\n",
    "                    for cat in row_template:\n",
    "                        new_row[cat] = \"\"\n",
    "                    for index, cell in enumerate(row.cells):\n",
    "                        new_row[row_template[index]] = cell.text\n",
    "\n",
    "                    #print(new_row)\n",
    "                    temp_obj.append(new_row)\n",
    "\n",
    "    #print(temp_obj)\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    plan_obj = []\n",
    "\n",
    "    for row in temp_obj:\n",
    "        cat = {}\n",
    "        categoryIdentifiers = [\"categor\",\"benefits\"]\n",
    "        for cell in row:\n",
    "            if (len(cell)==0) or (any([x in cell.lower() for x in categoryIdentifiers])):\n",
    "                if (\"diagnostic\" in (row[cell]).lower()) and (\"preventive\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"DandP\"\n",
    "                elif (\"basic\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"basic\"\n",
    "                elif (\"major\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"major\"\n",
    "                elif (\"orthodontic\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"orthodontic\"\n",
    "\n",
    "                else:\n",
    "                    if(len(row)>=3):\n",
    "                        cat['category'] = row[cell].lower().replace(\"\\n\",\" \")\n",
    "\n",
    "            elif(\" ppo\" in cell.lower()) or (\"in-network\" in cell.lower()) or (\" dpo \" in cell.lower()):\n",
    "                kw = [\"%\",\"not covered\"]\n",
    "\n",
    "                if any([x in (row[cell]).lower() for x in kw]):\n",
    "                    cat['PPO_rate'] = row[cell]\n",
    "\n",
    "            elif(\"non-delta\" in cell.lower()) or (\"out-of-network\" in cell.lower()):\n",
    "                kw = [\"%\",\"not covered\"]\n",
    "\n",
    "                if any([x in (row[cell]).lower() for x in kw]):\n",
    "                    cat['non-PPO_rate'] = row[cell]\n",
    "\n",
    "        if(len(cat)>=3):\n",
    "            plan_obj.append(cat)\n",
    "\n",
    "    return plan_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Process a table object identified as a normalized with multiple plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processMultiPlan(table):\n",
    "    first_row=1\n",
    "    first_row_found = 0\n",
    "    plan_row=1\n",
    "    title_row=1\n",
    "    temp_obj = {}\n",
    "    row_template = []\n",
    "\n",
    "    for row in table.rows:\n",
    "        if not first_row_found:\n",
    "            for cell in row.cells:\n",
    "                if((cell.text).lower().find(\"contract benefit level\") == 0):\n",
    "                    first_row_found = 1\n",
    "        else:\n",
    "            if(plan_row):\n",
    "                plan_template = []\n",
    "                for cell in row.cells:\n",
    "                    plan_template.append(cell.text)\n",
    "                plan_row = 0\n",
    "               # print(plan_template)\n",
    "                for plan in plan_template:\n",
    "                    temp_obj[plan] = []\n",
    "               # print(temp_obj)\n",
    "            elif(title_row):\n",
    "                row_template = []\n",
    "                for cell in row.cells:\n",
    "                    row_template.append(cell.text)\n",
    "                title_row = 0\n",
    "                #print(row_template)\n",
    "            else:\n",
    "                for plan in temp_obj:\n",
    "                    if (\"plan\" in plan.lower()) and not (\"delta dental will pay\" in (row.cells[0].text).lower()):\n",
    "                       # print(plan)\n",
    "                        new_row = {}\n",
    "                        for index, cat in enumerate(row_template):\n",
    "                            if not(\"plan\" in plan_template[index].lower()):\n",
    "                                new_row[cat] = \"\"\n",
    "                            if(plan_template[index] == plan):\n",
    "                                new_row[cat] = \"\"\n",
    "                        for index, cell in enumerate(row.cells):\n",
    "                            if not(\"plan\" in plan_template[index].lower()):\n",
    "                                new_row[row_template[index]] = cell.text\n",
    "                            if(plan_template[index] == plan):\n",
    "                                new_row[row_template[index]] = cell.text\n",
    "                        #print(new_row)\n",
    "                        temp_obj[plan].append(new_row)\n",
    "\n",
    "    if not first_row_found:#never found contract benefit levels\n",
    "        highPlanFlag = 0\n",
    "        lowPlanFlag = 0\n",
    "        for row in table.rows:\n",
    "            #ppoFlag = 0\n",
    "            #non_ppoFlag = 0\n",
    "            if(len(set(row.cells))==1):\n",
    "                #print(row)\n",
    "                break\n",
    "\n",
    "            #ppoList = [\"ppo providers\", \"pposm providers\", \"dpo providers\", \"delta dental ppo\", \"in-network\"]\n",
    "            #non_ppoList = [\"non-delta dental providers\",\"out-of-network\"]\n",
    "            highPlanList = [\"high plan\", \"enhanced plan\", \"premier plan\"]\n",
    "            lowPlanList = [\"low plan\", \"standard plan\", \"ppo plan\"]\n",
    "\n",
    "            for cell in row.cells:\n",
    "                #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in highPlanList]):\n",
    "                    highPlanFlag = 1\n",
    "                    #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in lowPlanList]):\n",
    "                    lowPlanFlag = 1\n",
    "                    #print(cell.text)\n",
    "\n",
    "            if(highPlanFlag and lowPlanFlag):\n",
    "                if(plan_row):\n",
    "                    plan_template = []\n",
    "                    for cell in row.cells:\n",
    "                        plan_template.append(cell.text)\n",
    "                    plan_row = 0\n",
    "                   # print(plan_template)\n",
    "                    for plan in plan_template:\n",
    "                        temp_obj[plan] = []\n",
    "                   # print(temp_obj)\n",
    "                elif(title_row):\n",
    "                    row_template = []\n",
    "                    for cell in row.cells:\n",
    "                        row_template.append(cell.text)\n",
    "                    title_row = 0\n",
    "                    #print(row_template)\n",
    "                else:\n",
    "                    for plan in temp_obj:\n",
    "                        if (\"plan\" in plan.lower()) and not (\"delta dental will pay\" in (row.cells[0].text).lower()):\n",
    "                           # print(plan)\n",
    "                            new_row = {}\n",
    "                            for index, cat in enumerate(row_template):\n",
    "                                if not(\"plan\" in plan_template[index].lower()):\n",
    "                                    new_row[cat] = \"\"\n",
    "                                if(plan_template[index] == plan):\n",
    "                                    new_row[cat] = \"\"\n",
    "                            for index, cell in enumerate(row.cells):\n",
    "                                if not(\"plan\" in plan_template[index].lower()):\n",
    "                                    new_row[row_template[index]] = cell.text\n",
    "                                if(plan_template[index] == plan):\n",
    "                                    new_row[row_template[index]] = cell.text\n",
    "                            #print(new_row)\n",
    "                            temp_obj[plan].append(new_row)\n",
    "\n",
    "    #print(temp_obj)\n",
    "    plan_set = {}\n",
    "\n",
    "    for plan in temp_obj:\n",
    "        #print(plan)\n",
    "        plan_obj = []\n",
    "        for row in temp_obj[plan]:\n",
    "            #print(row)\n",
    "            cat = {}\n",
    "            for cell in row:\n",
    "                if(\" ppo\" in cell.lower()) or (\"in-network\" in cell.lower()) or (\" dpo \" in cell.lower()):\n",
    "                    kw = [\"%\",\"not covered\",\"n/a\"]\n",
    "\n",
    "                    if any([x in (row[cell]).lower() for x in kw]):\n",
    "                        cat['PPO_rate'] = row[cell]\n",
    "                elif(\"non-delta\" in cell.lower()) or (\"out-of-network\" in cell.lower()):\n",
    "                    kw = [\"%\",\"not covered\",\"n/a\"]\n",
    "\n",
    "                    if any([x in (row[cell]).lower() for x in kw]):\n",
    "                        cat['non-PPO_rate'] = row[cell]\n",
    "\n",
    "                else:#(\"categor\" in cell.lower()) or (len(cell)==0):\n",
    "                    #print(cell.lower())\n",
    "                    #print((row[cell]).lower())\n",
    "                    if (\"diagnostic\" in (row[cell]).lower()) and (\"preventive\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"DandP\"\n",
    "                    elif (\"basic benefit\" in (row[cell]).lower()) or (\"basic service\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"basic\"\n",
    "                    elif (\"major benefit\" in (row[cell]).lower()) or (\"major service\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"major\"\n",
    "                    elif (\"orthodontic benefit\" in (row[cell]).lower()) or (\"orthodontic service\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"orthodontic\"\n",
    "\n",
    "                    else:\n",
    "                        cat['category'] = (row[cell]).lower().replace(\"\\n\",\" \")\n",
    "\n",
    "            if(len(cat)>=3):\n",
    "                plan_obj.append(cat)\n",
    "        if(len(plan_obj)>0):\n",
    "            plan_set[plan] = plan_obj\n",
    "\n",
    "    #print(plan_set)\n",
    "    return plan_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define if a table object represents a normalized plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isNormalized(table):\n",
    "    isNormalized = False\n",
    "    for row in table.rows:\n",
    "        if(((row.cells[0].text).lower()).find('contract benefit levels')):\n",
    "            isNormalized = True  \n",
    "    return isNormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Define if a table object represents a denormalized plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isDenormalized(sentTokens):\n",
    "    isDeNormalized = False\n",
    "    BCSfound = False\n",
    "\n",
    "    for index, sent in enumerate(sentTokens):\n",
    "        if(\"benefit summary chart\" in sent.lower()):\n",
    "            count = 0\n",
    "            regex = re.compile(\"%\")\n",
    "            for i in range(0,20):\n",
    "                try:\n",
    "                    numPercents = regex.findall(sentTokens[index + i])\n",
    "                    #print(len(numPercents))\n",
    "                    count += len(numPercents)\n",
    "                except Exception as e:\n",
    "                    print(\"Error determining normalization: \" + str(e))\n",
    "            #print(count)\n",
    "            if(count > 20):\n",
    "                    isDeNormalized = 1\n",
    "    return isDeNormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Standardize the wording in a plan object\n",
    "The wording in the rate tables is diverse, this standardizes the data points to allow analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def standardizePlanObject(plan_obj, outputFile, row):\n",
    "    \n",
    "    categoryList = ['DandP','basic','major','orthodontic']\n",
    "    try:\n",
    "        for cat in categoryList:\n",
    "            found=0\n",
    "            for plan_row in plan_obj:\n",
    "                if (len(plan_row)>=3) and (plan_row['category'] == cat):\n",
    "                    found = 1\n",
    "                    row[str(cat)+\" PPO\"] = plan_row['PPO_rate']\n",
    "                    row[str(cat)+\" non-PPO\"] = plan_row['non-PPO_rate']\n",
    "\n",
    "        for plan_row in plan_obj:\n",
    "            if (plan_row['category'] not in categoryList) and (len(plan_row)>=3 and plan_row['PPO_rate'] and plan_row['non-PPO_rate']):\n",
    "                row[str(plan_row['category']) + \" PPO\"] = plan_row['PPO_rate']\n",
    "                row[str(plan_row['category']) + \" non-PPO\"] = plan_row['non-PPO_rate']\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(row['Filename'])\n",
    "        print(plan_obj)\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Extract all rate tables from a DocX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getRateTables(file_info):\n",
    "    plan_objs = []\n",
    "    try:\n",
    "        filePath = file_info['filepath']\n",
    "        fileName = file_info['fileName']\n",
    "\n",
    "        if not \"-pdf.\" in fileName:\n",
    "            docxFileName = fileName.replace(\".txt\", \".docx\")\n",
    "            docxFilePath = os.path.join(os.getcwd(), \"processed\", docxFileName)\n",
    "\n",
    "            document = docx.Document(docxFilePath)\n",
    "            docRateTables = []\n",
    "\n",
    "            for table in document.tables:\n",
    "                if(isRateTable(table)):\n",
    "                    if(isMultiPlan(table)):\n",
    "                        plan_objs = processMultiPlan(table)\n",
    "                    else:\n",
    "                        plan_obj = processSinglePlan(table)\n",
    "                        plan_objs.append(plan_obj)\n",
    "    except Exception as e:\n",
    "        print(\"Error extracting rate tables: \" + str(e))\n",
    "        return None\n",
    "    return plan_objs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Create object of table_rows to be written to the comprehensive Rate output csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def rateTableCSV():\n",
    "    from docx import Document\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "    base_info = None\n",
    "    #pp = batchPreProcess(errorFile, os.path.join(cwd, \"processed\"))\n",
    "    #if pp == None:\n",
    "    #    print(\"Error in pp\")\n",
    "    #else:\n",
    "    dataPath = os.path.join(cwd, \"output\")\n",
    "    base_info = batchGetTokens(errorFile, dataPath)\n",
    "    if not base_info:\n",
    "        print(\"Error in getting tokens\")\n",
    "    else:\n",
    "        outputFilePath = os.path.join(os.getcwd(),'rate_table_output.csv')\n",
    "        outputFile=open(outputFilePath, 'w')\n",
    "        #first_row = 1\n",
    "        table_rows = []\n",
    "        #outputFile.write(\"Filename,hasRateTable,isMultiplan,isNormalized,DandP PPO,DandP non-PPO,basic PPO,basic non-PPO,major PPO,major non-PPO,orthodontics PPO,orthodontics non-PPO\\n\")\n",
    "\n",
    "        for file in base_info:#os.listdir(os.path.join(cwd, \"processed\")):\n",
    "            row_template = {\n",
    "                        \"Filename\":\"\",\n",
    "                        \"hasRateTable\":\"\",\n",
    "                        \"isMultiplan\":\"\",\n",
    "                        \"isNormalized\":\"\",\n",
    "                        \"DandP PPO\":\"\",\n",
    "                        \"DandP non-PPO\":\"\",\n",
    "                        \"basic PPO\":\"\",\n",
    "                        \"basic non-PPO\":\"\",\n",
    "                        \"major PPO\":\"\",\n",
    "                        \"major non-PPO\":\"\",\n",
    "                        \"orthodontic PPO\":\"\",\n",
    "                        \"orthodontic non-PPO\":\"\"\n",
    "                    }\n",
    "            row = row_template\n",
    "            try:\n",
    "                #file['fileName'] = os.path.split(file['filepath'])[1]\n",
    "                row['Filename'] = os.path.split(file['filepath'])[1]\n",
    "                if(isDenormalized(file['sentTokens'])):\n",
    "                    #outputFile.write(\"\\\"\"+file['fileName'] + \"\\\"\" + \",True,False,False,\\n\")\n",
    "                    row['hasRateTable'] = \"True\"\n",
    "                    row['isMultiplan'] = \"False\"\n",
    "                    row['isNormalized'] = \"False\"\n",
    "                    table_rows.append(row)\n",
    "                elif not(row['Filename'].endswith(\"-pdf.txt\")):\n",
    "                    docxFileName = row['Filename'].replace(\"txt\",\"docx\")\n",
    "                    document = Document(os.path.join(cwd, \"processed\", docxFileName))\n",
    "                    docRateTables = []\n",
    "                    #print(docxFileName)#outputFile.write(file+\",\")\n",
    "                    if(len(document.tables)>0):\n",
    "                        rateTableFound = 0\n",
    "                        for table in document.tables:\n",
    "                            if(isRateTable(table)):\n",
    "                                rateTableFound = 1\n",
    "                                #print(\"is rate table\")\n",
    "                                if(isMultiPlan(table)):\n",
    "                                    #print(\"is Multiplan\\n\")\n",
    "                                    list_plan_objs = processMultiPlan(table)\n",
    "                                    #print(len(list_plan_objs))\n",
    "                                    for plan_obj in list_plan_objs:\n",
    "                                        row = row_template\n",
    "                                        row['Filename'] = os.path.split(file['filepath'])[1]\n",
    "                                        #outputFile.write(\"\\\"\" + docxFileName + \"\\\"\" + \",True,True,True,\")\n",
    "                                        row['hasRateTable'] = \"True\"\n",
    "                                        row['isMultiplan'] = \"True\"\n",
    "                                        row['isNormalized'] = \"True\"\n",
    "                                        row = standardizePlanObject(list_plan_objs[plan_obj], outputFile, row)\n",
    "                                        table_rows.append(row)\n",
    "                                else:\n",
    "                                    #print(\"is single plan\")\n",
    "                                    plan_obj = processSinglePlan(table)\n",
    "                                    #outputFile.write(\"\\\"\" + docxFileName + \"\\\"\"+\",True,False,True,\")\n",
    "                                    row['hasRateTable'] = \"True\"\n",
    "                                    row['isMultiplan'] = \"False\"\n",
    "                                    row['isNormalized'] = \"True\"\n",
    "                                    row = standardizePlanObject(plan_obj, outputFile, row)\n",
    "                                    table_rows.append(row)\n",
    "                        if not rateTableFound:\n",
    "                            #outputFile.write(\"\\\"\" + docxFileName + \"\\\"\"+\",False,,,,,,,,,,\\n\")\n",
    "                            row['hasRateTable'] = \"False\"\n",
    "                            table_rows.append(row)\n",
    "                    else:\n",
    "                        #outputFile.write(\"\\\"\" + docxFileName + \"\\\"\"+\",False,,,,,,,,,,\\n\")\n",
    "                        row['hasRateTable'] = \"False\"\n",
    "                        table_rows.append(row)\n",
    "                else:\n",
    "                    #outputFile.write(\"\\\"\"+file['fileName'] + \"\\\"\"+\",False,\\n\")\n",
    "                    row['hasRateTable'] = \"False\"\n",
    "                    table_rows.append(row)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Error collection rate table information: \" + str(e))\n",
    "\n",
    "        outputFile.close()\n",
    "    errorFile.close()\n",
    "\n",
    "    #for row in table_rows:\n",
    "    #    print(row)\n",
    "\n",
    "    return table_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Write out object from rateTableCSV() to an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def printRateTableCSV(table_rows):\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "    outputFilePath = os.path.join(os.getcwd(),'rate_table_output.csv')\n",
    "    outputFile=open(outputFilePath, 'w')\n",
    "\n",
    "    categoryList = [\"Filename\",\"hasRateTable\",\"isMultiplan\",\"isNormalized\",\"DandP PPO\",\"DandP non-PPO\",\"basic PPO\",\"basic non-PPO\",\"major PPO\",\"major non-PPO\",\"orthodontic PPO\",\"orthodontic non-PPO\"]\n",
    "\n",
    "    for row in table_rows:\n",
    "        for cat in row:\n",
    "            if not (cat in categoryList):\n",
    "                categoryList.append(cat)\n",
    "\n",
    "    #print(categoryList)\n",
    "    for cat in categoryList:\n",
    "        outputFile.write(cat + \",\")\n",
    "    outputFile.write(\"\\n\")\n",
    "\n",
    "    for row in table_rows:\n",
    "        for index,category in enumerate(categoryList):\n",
    "            found = 0\n",
    "            for attr in row:\n",
    "                if attr == category:\n",
    "                    outputFile.write(\"\\\"\"+row[attr]+\"\\\"\" + \",\")\n",
    "                    found = 1\n",
    "            if not found:\n",
    "                outputFile.write(\",\")\n",
    "        outputFile.write(\"\\n\")\n",
    "\n",
    "\n",
    "    outputFile.close()\n",
    "    errorFile.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### A function for testing Rate Table extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def findingRateTablesTestFunction():\n",
    "    from docx import Document\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "    base_info = None\n",
    "    #pp = batchPreProcess(errorFile, os.path.join(cwd, \"processed\"))\n",
    "    #if pp == None:\n",
    "    #    print(\"Error in pp\")\n",
    "    #else:\n",
    "    dataPath = os.path.join(cwd, \"output\")\n",
    "    base_info = batchGetTokens(errorFile, dataPath)\n",
    "    if not base_info:\n",
    "        print(\"Error in getting tokens\")\n",
    "    else:\n",
    "        outputFilePath = os.path.join(os.getcwd(),'rate_table_output.csv')\n",
    "        outputFile=open(outputFilePath, 'w')\n",
    "        first_row = 1\n",
    "\n",
    "        outputFile.write(\"Filename,hasRateTable,isMultiplan,isNormalized,DandP PPO,DandP non-PPO,basic PPO,basic non-PPO,major PPO,major non-PPO,orthodontics PPO,orthodontics non-PPO\\n\")\n",
    "\n",
    "        for file in base_info:#os.listdir(os.path.join(cwd, \"processed\")):\n",
    "            count = 0\n",
    "            try:\n",
    "                file['fileName'] = os.path.split(file['filepath'])[1]\n",
    "                if(isDenormalized(file['sentTokens'])):\n",
    "                    outputFile.write(\"\\\"\"+file['fileName'] + \"\\\"\" + \",True,False,False,\\n\")\n",
    "\n",
    "                elif not(file['fileName'].endswith(\"-pdf.txt\")):\n",
    "                    docxFileName = file['fileName'].replace(\"txt\",\"docx\")\n",
    "                    document = Document(os.path.join(cwd, \"processed\", docxFileName))\n",
    "                    docRateTables = []\n",
    "                    print(docxFileName)#outputFile.write(file+\",\")\n",
    "                    if(len(document.tables)>0):\n",
    "                        rateTableFound = 0\n",
    "                        for table in document.tables:\n",
    "                            if(isRateTable(table)):\n",
    "                                rateTableFound = 1\n",
    "                                #print(\"is rate table\")\n",
    "                                if(isMultiPlan(table)):\n",
    "                                    print(\"is Multiplan\\n\")\n",
    "                                    list_plan_objs = processMultiPlan(table)\n",
    "                                    print(len(list_plan_objs))\n",
    "                                    for plan_obj in list_plan_objs:\n",
    "                                        outputFile.write(\"\\\"\" + docxFileName + \"\\\"\" + \",True,True,True,\")\n",
    "                                        standardizePlanObject(list_plan_objs[plan_obj], outputFile)\n",
    "                                else:\n",
    "                                    print(\"is single plan\")\n",
    "                                    plan_obj = processSinglePlan(table)\n",
    "                                    outputFile.write(\"\\\"\" + docxFileName + \"\\\"\"+\",True,False,True,\")\n",
    "                                    standardizePlanObject(plan_obj, outputFile)\n",
    "                        if not rateTableFound:\n",
    "                            outputFile.write(\"\\\"\" + docxFileName + \"\\\"\"+\",False,,,,,,,,,,\\n\")\n",
    "                    else:\n",
    "                        outputFile.write(\"\\\"\" + docxFileName + \"\\\"\"+\",False,,,,,,,,,,\\n\")\n",
    "                else:\n",
    "                    outputFile.write(\"\\\"\"+file['fileName'] + \"\\\"\"+\",False,\\n\")\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "        outputFile.close()\n",
    "    errorFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Deductible Extraction\n",
    "All functions so far for extracting deductible information from table objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Determine if table object is a Deductible table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isDeductibleTable(table):\n",
    "    isDeductible = False\n",
    "    keywords = [\"deductible\", \"deductibles and maximum\"]\n",
    "    for kw in keywords:\n",
    "        regex = re.compile(kw)\n",
    "        if table.rows:\n",
    "            for row in table.rows:\n",
    "                for cell in row.cells:\n",
    "\n",
    "                    results = re.search(regex, (cell.text).lower())\n",
    "                    #print(results)\n",
    "                    if results:\n",
    "                        isDeductible = True\n",
    "                    #print(cell.text)\n",
    "        #if not isDeductible:\n",
    "\n",
    "\n",
    "\n",
    "    return isDeductible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Process Deductible Table object        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDeductibleTable(table):\n",
    "\n",
    "    tableInfo = {}\n",
    "    categories = [\"annual deductible\",\"annual maximum\",\"orthodontic maximum\"]\n",
    "    if isMultiPlan(table):\n",
    "        print(\"is multiplan\")\n",
    "    else:\n",
    "        first_row=1\n",
    "        first_row_found = 0\n",
    "        title_row=1\n",
    "        cat_row = {}\n",
    "\n",
    "        for row in table.rows:\n",
    "\n",
    "            if(((row.cells[0]).text).lower().find(\"deductible\") > -1) and not first_row_found:\n",
    "                first_row_found = 1\n",
    "            else:\n",
    "                if(title_row):\n",
    "                    ppoFlag = 0\n",
    "                    non_ppoFlag = 0\n",
    "\n",
    "                    ppoList = [\"ppo providers\", \"pposm providers\", \"dpo providers\", \"delta dental ppo\", \"in-network\"]\n",
    "                    non_ppoList = [\"non-delta dental providers\",\"out-of-network\"]\n",
    "\n",
    "                    for cell in row.cells:\n",
    "                        if any([x in (cell.text).lower() for x in ppoList]):\n",
    "                            ppoFlag = 1\n",
    "                        if any([x in (cell.text).lower() for x in non_ppoList]):\n",
    "                            non_ppoFlag = 1\n",
    "                    if ppoFlag and non_ppoFlag:\n",
    "\n",
    "                        cat_row[\"categories\"] = \"\"\n",
    "                        for cell in range(1,len(row.cells)):\n",
    "                            cat_row[\"categories\"] = \";\".join([cat_row[\"categories\"], row.cells[cell].text])\n",
    "\n",
    "                    title_row = 0\n",
    "\n",
    "                if first_row_found:\n",
    "                    new_row = {}\n",
    "                    for cat in categories:\n",
    "                        if cat in (row.cells[0].text).lower():\n",
    "                            tableInfo[cat] = \"\"\n",
    "                            for cell in range(1, len(row.cells)):\n",
    "                                tableInfo[cat] = \";\".join([tableInfo[cat], row.cells[cell].text])\n",
    "                   \n",
    "        plan = {}\n",
    "        if(len(cat_row)>0):\n",
    "            cats = cat_row[\"categories\"].split(\";\")\n",
    "            print(cats)\n",
    "            for cat in cats:\n",
    "                plan[cat] = {}\n",
    "            for row in tableInfo:\n",
    "                if row is \"annual deductible\":\n",
    "                    cells = tableInfo[row].split(\";\")\n",
    "                    moneyRegex = re.compile(\"\\$\\d+[,]?\\d*\")\n",
    "                    for index, cell in enumerate(cells):\n",
    "                        results = re.search(moneyRegex, cell)\n",
    "                        if(results):\n",
    "                            plan[cats[index]] = cell\n",
    "        print(plan)\n",
    "        \n",
    "        return tableInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function used to test deductible extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def findingDeductibleTablesTestFunction():\n",
    "    from docx import Document\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "    base_info = None\n",
    "    #pp = batchPreProcess(errorFile, os.path.join(cwd, \"processed\"))\n",
    "    #if pp == None:\n",
    "    #    print(\"Error in pp\")\n",
    "    #else:\n",
    "    dataPath = os.path.join(cwd, \"output\")\n",
    "    base_info = batchGetTokens(errorFile, dataPath)\n",
    "    if not base_info:\n",
    "        print(\"Error in getting tokens\")\n",
    "    else:\n",
    "        #outputFilePath = os.path.join(os.getcwd(),'rate_table_output.csv')\n",
    "        #outputFile=open(outputFilePath, 'w')\n",
    "        #first_row = 1\n",
    "\n",
    "        #outputFile.write(\"Filename,isRateTable,isMultiplan,isNormalized,DandP PPO,DandP non-PPO,basic PPO,basic non-PPO,major PPO,major non-PPO,orthodontics PPO,orthodontics non-PPO\\n\")\n",
    "\n",
    "        for file in os.listdir(os.path.join(cwd, \"processed\")):\n",
    "            #count = 0\n",
    "            try:\n",
    "                if(file.endswith(\".docx\")):\n",
    "                    document = Document(os.path.join(cwd, \"processed\", file))\n",
    "                    docRateTables = []\n",
    "                    #print(file)#outputFile.write(file+\",\")\n",
    "                    #print(file)\n",
    "                    for table in document.tables:\n",
    "                        if isDeductibleTable(table):#print(table)\n",
    "                            print(file)\n",
    "                            if(len(table.rows[0].cells[0].text)>0):\n",
    "                                #print(file)\n",
    "                                #print(table.rows[0].cells[0].text)\n",
    "                                #print(\"\\n\")\n",
    "                                deductibleInfo = processDeductibleTable(table)\n",
    "                        #outputFile.write(file+\",\")\n",
    "                        #if(isDeductibleTable(table)):\n",
    "                        #    print(file)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "        #outputFile.close()\n",
    "    errorFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run full workflow\n",
    "Using all the functions, set up the workspace, process the files and extract metadata and rate table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### This function is to be used after the output folder is filled with .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def testOutputAttrCSV():\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "    base_info = None\n",
    "    #pp = batchPreProcess(errorFile, os.path.join(cwd, \"processed\"))\n",
    "    #if pp == None:\n",
    "    #    print(\"Error in pp\")\n",
    "    #else:\n",
    "    dataPath = os.path.join(cwd, \"output\")\n",
    "    base_info = batchGetTokens(errorFile, dataPath)\n",
    "    if not base_info:\n",
    "        print(\"Error in getting tokens\")\n",
    "    else:\n",
    "        outputFilePath = os.path.join(os.getcwd(),'data','raw_attribute_data.csv')\n",
    "        outputFile=open(outputFilePath, 'w')\n",
    "        first_row = 1\n",
    "        for file in base_info:\n",
    "            \n",
    "            file_attr = getMetaDataAtt(file)\n",
    "            #plans = getRateTables(file_attr)\n",
    "            #if(plans and len(plans)>0):\n",
    "            #    file_attr['hasRateTable'] = \"True\"\n",
    "            #else:\n",
    "            #    file_attr['hasRateTables'] = \"False\"\n",
    "\n",
    "            try:\n",
    "                filename = (os.path.split(file['filepath'])[1])\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "            if first_row:\n",
    "                for key in file_attr:\n",
    "                    #print(key)\n",
    "                    outputFile.write(key + \",\")\n",
    "                outputFile.write(\"\\n\")\n",
    "                for attr in file_attr:\n",
    "                    #print(attr + \": \")\n",
    "                    attrStr = \"\"\n",
    "                    if ((attr == 'fileTypes') or (attr == 'footer')) and file_attr[attr]:\n",
    "                        for substr in file_attr[attr]:\n",
    "                            attrStr = attrStr + substr + \";\"\n",
    "                        outputFile.write(attrStr+\",\")\n",
    "                    else:\n",
    "                        outputFile.write(str(file_attr[attr])+ \",\")\n",
    "                outputFile.write(\"\\n\")\n",
    "                first_row = 0\n",
    "            else:\n",
    "                for attr in file_attr:\n",
    "                    #print(attr + \": \" + file_attr[attr])\n",
    "                    attrStr = \"\"\n",
    "                    if ((attr == 'fileTypes') or (attr == 'footer')) and file_attr[attr]:\n",
    "                        for substr in file_attr[attr]:\n",
    "                            attrStr = attrStr + substr + \";\"\n",
    "                        outputFile.write(attrStr+\",\")\n",
    "                    else:\n",
    "                        outputFile.write(\"\\\"\" + str(file_attr[attr])+ \"\\\",\")\n",
    "            #    outputFile.write(filename + \",\")\n",
    "            #    outputFile.write(str(count) + \",\")\n",
    "                outputFile.write(\"\\n\")#\n",
    "\n",
    "        outputFile.close()\n",
    "    errorFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### This function assumes files are in the processed folder, but there does not have to be any .txt files created in the output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def fullOutputAttrCSV():\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "    base_info = None\n",
    "    pp = batchPreProcess(errorFile, os.path.join(cwd, \"processed\"))\n",
    "    if pp == None:\n",
    "        print(\"Error in pp\")\n",
    "    else:\n",
    "        dataPath = os.path.join(cwd, \"output\")\n",
    "        base_info = batchGetTokens(errorFile, dataPath)\n",
    "        if not base_info:\n",
    "            print(\"Error in getting tokens\")\n",
    "        else:\n",
    "            outputFilePath = os.path.join(os.getcwd(),'data','raw_attribute_data.csv')\n",
    "            outputFile=open(outputFilePath, 'w')\n",
    "            first_row = 1\n",
    "            for file in base_info:\n",
    "                file_attr = getMetaDataAtt(file)\n",
    "                plans = getRateTables(file_attr)\n",
    "                if(plans and len(plans)>0):\n",
    "                    file_attr['hasRateTable'] = \"True\"\n",
    "                else:\n",
    "                    file_attr['hasRateTables'] = \"False\"\n",
    "\n",
    "                try:\n",
    "                    filename = (os.path.split(file['filepath'])[1])\n",
    "                    #print(filename)\n",
    "        #\n",
    "                except Exception as e:\n",
    "                    print(str(e))###\n",
    "    ##\n",
    "    #\n",
    "        #    #print(count)\n",
    "                if first_row:\n",
    "                    for key in file_attr:\n",
    "                        #print(key)\n",
    "                        outputFile.write(key + \",\")\n",
    "                    outputFile.write(\"\\n\")\n",
    "                    for attr in file_attr:\n",
    "                        #print(attr + \": \")\n",
    "                        attrStr = \"\"\n",
    "                        if ((attr == 'fileTypes') or (attr == 'footer')) and file_attr[attr]:\n",
    "                            for substr in file_attr[attr]:\n",
    "                                attrStr = attrStr + substr + \";\"\n",
    "                            outputFile.write(attrStr+\",\")\n",
    "                        else:\n",
    "                            outputFile.write(str(file_attr[attr])+ \",\")\n",
    "                    outputFile.write(\"\\n\")\n",
    "                    first_row = 0\n",
    "                else:\n",
    "                    for attr in file_attr:\n",
    "                        #print(attr + \": \" + file_attr[attr])\n",
    "                        attrStr = \"\"\n",
    "                        if ((attr == 'fileTypes') or (attr == 'footer')) and file_attr[attr]:\n",
    "                            for substr in file_attr[attr]:\n",
    "                                attrStr = attrStr + substr + \";\"\n",
    "                            outputFile.write(attrStr+\",\")\n",
    "                        else:\n",
    "                            outputFile.write(\"\\\"\" + str(file_attr[attr])+ \"\\\",\")\n",
    "                #    outputFile.write(filename + \",\")\n",
    "                #    outputFile.write(str(count) + \",\")\n",
    "                    outputFile.write(\"\\n\")#\n",
    "\n",
    "            outputFile.close()\n",
    "    errorFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run full process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will overwrite current data, do you want to continue? [Y/N]\tY\n",
      "False\n",
      "Are you sure you want to overwrite current data? [Y/N]\tY\n",
      "cwd: /Users/sydneyknox/Documents/data-insights/data\n",
      "creating /Users/sydneyknox/Documents/data-insights/data/processed...\n",
      "creating /Users/sydneyknox/Documents/data-insights/data/output...\n",
      "creating /Users/sydneyknox/Documents/data-insights/data/data...\n",
      "cwd: /Users/sydneyknox/Documents/data-insights/data\n",
      "processing: 19168 Contract.pdf\n",
      "processing: 10041 Appendix C EOCc Meridian Union (Janb2014).pdf\n",
      "processing: 10041 Appendix E EOC Daingerfield Union (Jan2018).pdf\n",
      "processing: 19168 EOC.docx\n",
      "processing: 01094 EOC  7-1-16.pdf\n",
      "processing: 10041 Appendix F EOC Ardmore Union (Jan2012).doc\n",
      "processing: 01094 - SI - Eff 7-1-17 to 6-30-19.doc\n",
      "processing: 19168 Attachment B High.docx\n",
      "processing: 01094 (SI) Eff. 7-1-15.doc\n",
      "processing: 01094 (Tax Modfication).docx\n",
      "processing: 01094 _SBC Modification_.pdf\n",
      "processing: 1094 Full Contract (Eff. 7-1-07).pdf\n",
      "processing: 10041 Appendix G EOC Franklin Union (Jan2012).doc\n",
      "processing: 19168 EOC.pdf\n",
      "processing: 10041 Appendix C EOC Meridian Union (Jan2014).doc\n",
      "processing: 19168 Attachment A.docx\n",
      "processing: 10041 Appendix D EOC Hampton Union (Janb2012).doc\n",
      "processing: 10294-01002 & 02001 PPO EOC (07-01-12).doc\n",
      "processing: 10041 Appendix B EOC Non-Union (Jan2018).doc\n",
      "processing: 01094 (Schedule I) July 1, 2013.doc\n",
      "processing: 01094 (Schedule I and Tax Modification) July 1, 2013.pdf\n",
      "processing: 10041 Appendix G EOC Franklin Union (Jan2012).pdf\n",
      "processing: 19168 Attachment C.docx\n",
      "processing: 10294-01002 & 02001 PPO EOC (07-01-12).pdf\n",
      "processing: 10041 Appendix B EOC Non-Union (Jan2018).pdf\n",
      "processing: 10041 Appendix D EOCc Hampton Union (Jan2012).pdf\n",
      "processing: 01094 - SI - Eff 7-1-17 to 6-30-19.pdf\n",
      "processing: 19168 Attachment B Low.docx\n",
      "processing: 10041 Appendix F EOC Ardmore Union (Jan2012).pdf\n",
      "processing: 19168 Contract.docx\n",
      "processing: 10041 Appendix E EOC Daingerfield Union (Jan2018).doc\n",
      "processing: 01094 EOC  7-1-16.doc\n",
      "processing: 01094 (SBC Modification).doc\n",
      "pre-processing: 19168 Contract.pdf\n",
      "pre-processing: 10041 Appendix C EOCc Meridian Union (Janb2014).pdf\n",
      "pre-processing: 10041 Appendix E EOC Daingerfield Union (Jan2018).pdf\n",
      "pre-processing: 19168 EOC.docx\n",
      "pre-processing: 01094 EOC  7-1-16.pdf\n",
      "pre-processing: 10041 Appendix F EOC Ardmore Union (Jan2012).doc\n",
      "pre-processing: 01094 - SI - Eff 7-1-17 to 6-30-19.doc\n",
      "pre-processing: 19168 Attachment B High.docx\n",
      "pre-processing: 01094 (SI) Eff. 7-1-15.doc\n",
      "pre-processing: 01094 (Tax Modfication).docx\n",
      "pre-processing: 01094 _SBC Modification_.pdf\n",
      "pre-processing: 1094 Full Contract (Eff. 7-1-07).pdf\n",
      "pre-processing: 10041 Appendix G EOC Franklin Union (Jan2012).doc\n",
      "pre-processing: 19168 EOC.pdf\n",
      "pre-processing: 10041 Appendix C EOC Meridian Union (Jan2014).doc\n",
      "pre-processing: 19168 Attachment A.docx\n",
      "pre-processing: 10041 Appendix D EOC Hampton Union (Janb2012).doc\n",
      "pre-processing: 10294-01002 & 02001 PPO EOC (07-01-12).doc\n",
      "pre-processing: 10041 Appendix B EOC Non-Union (Jan2018).doc\n",
      "pre-processing: 01094 (Schedule I) July 1, 2013.doc\n",
      "pre-processing: 01094 (Schedule I and Tax Modification) July 1, 2013.pdf\n",
      "pre-processing: 10041 Appendix G EOC Franklin Union (Jan2012).pdf\n",
      "pre-processing: 19168 Attachment C.docx\n",
      "pre-processing: 10294-01002 & 02001 PPO EOC (07-01-12).pdf\n",
      "pre-processing: 10041 Appendix B EOC Non-Union (Jan2018).pdf\n",
      "pre-processing: 10041 Appendix D EOCc Hampton Union (Jan2012).pdf\n",
      "pre-processing: 01094 - SI - Eff 7-1-17 to 6-30-19.pdf\n",
      "pre-processing: 19168 Attachment B Low.docx\n",
      "pre-processing: 10041 Appendix F EOC Ardmore Union (Jan2012).pdf\n",
      "pre-processing: 19168 Contract.docx\n",
      "pre-processing: 10041 Appendix E EOC Daingerfield Union (Jan2018).doc\n",
      "pre-processing: 01094 EOC  7-1-16.doc\n",
      "pre-processing: 01094 (SBC Modification).doc\n",
      "pre-processing: 19168 Contract.pdf\n",
      "pre-processing: 10041 Appendix G EOC Franklin Union (Jan2012).docx\n",
      "pre-processing: 10041 Appendix B EOC Non-Union (Jan2018).docx\n",
      "pre-processing: 10041 Appendix C EOCc Meridian Union (Janb2014).pdf\n",
      "pre-processing: 10041 Appendix E EOC Daingerfield Union (Jan2018).pdf\n",
      "pre-processing: 19168 EOC.docx\n",
      "pre-processing: 01094 EOC  7-1-16.pdf\n"
     ]
    }
   ],
   "source": [
    "def main(rawDataPath=None):\n",
    "    rdp = \"\"    \n",
    "    if rawDataPath is not None:\n",
    "        rdp = rawDataPath\n",
    "    elif(len(sys.argv)>1):\n",
    "        rdp = sys.argv[1]\n",
    "    else:\n",
    "        print(\"Raw Data folder location required.\")\n",
    "        return\n",
    "    \n",
    "    outputPath = os.path.join(os.getcwd(), \"output\")\n",
    "    dataPath = os.path.join(os.getcwd(),\"data\")\n",
    "    processedPath = os.path.join(os.getcwd(),\"processed\")\n",
    "\n",
    "    if(os.path.isdir(dataPath) or os.path.isdir(outputPath) or os.path.isdir(processedPath)):\n",
    "        if(os.path.isdir(rdp)):\n",
    "            clearData = input(\"This will overwrite current data, do you want to continue? [Y/N]\\t\")\n",
    "            if clearData.strip().upper() == \"Y\":\n",
    "                conf = input(\"Are you sure you want to overwrite current data? [Y/N]\\t\")\n",
    "                if conf.strip().upper() == \"Y\":\n",
    "                    try:\n",
    "                        paths = []\n",
    "                        paths.append(os.path.join(os.getcwd(), \"processed\"))\n",
    "                        paths.append(os.path.join(os.getcwd(), \"data\"))\n",
    "                        paths.append(os.path.join(os.getcwd(), \"output\"))\n",
    "\n",
    "                        for path in paths:\n",
    "                            shutil.rmtree(path)\n",
    "\n",
    "                        rawToWorkspace(rdp)\n",
    "                        fullOutputAttrCSV()\n",
    "                        tableRows = rateTableCSV()\n",
    "                        printRateTableCSV(tableRows)\n",
    "                    except Exception as e:\n",
    "                        print(str(e))\n",
    "            elif clearData.strip().upper() == \"N\":\n",
    "                print(\"Execution canceled.\")\n",
    "            else:\n",
    "                print(\"incorrect input.\")\n",
    "        else:\n",
    "            print(\"Path to raw data does not exist.\")\n",
    "    else:\n",
    "        if(os.path.isdir(rdp)):\n",
    "            rawToWorkspace(rdp)\n",
    "            fullOutputAttrCSV()\n",
    "            tableRows = rateTableCSV()\n",
    "            printRateTableCSV(tableRows)\n",
    "        else:\n",
    "            print(\"Path to raw data does not exist.\")\n",
    "\n",
    "main(\"/Users/sydneyknox/Documents/data-insights/testDataRaw\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Modification Tests\n",
    "This is code that sets up and runs through the environments used in our tests on being able to modify original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def modificationTest(pathToRawFile):\n",
    "        import stat\n",
    "        try:\n",
    "            if os.path.isfile(pathToRawFile):\n",
    "\n",
    "                dirPath = os.path.join(os.getcwd(),\"modificationTestDir\")\n",
    "                if not os.path.isdir(dirPath):\n",
    "                    os.makedirs(dirPath)\n",
    "\n",
    "                        #print(\"moving: \" + file)\n",
    "                    shutil.copy(pathToRawFile, os.path.join(dirPath, os.path.split(pathToRawFile)[1]))\n",
    "\n",
    "                    os.chmod(dirPath, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n",
    "                    os.chmod(os.path.join(dirPath, os.path.split(pathToRawFile)[1]), stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n",
    "                    rawToWorkspace(\"C:\\\\Users\\\\Sydney.knox\\\\Documents\\\\data-insights\\\\modificationTestDir\")\n",
    "                    testOutputAttrCSV()\n",
    "                    tableRows = rateTableCSV()\n",
    "                    printRateTableCSV(tableRows)\n",
    "                    #catch copy exception here so it doesn't stop all files (?)\n",
    "            else:\n",
    "                print(\"not a file\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "def modificationTest_changeFile(pathToRawFile):\n",
    "        import stat\n",
    "        try:\n",
    "            if os.path.isfile(pathToRawFile):\n",
    "\n",
    "                dirPath = os.path.join(os.getcwd(),\"modificationTestDir\")\n",
    "                if not os.path.isdir(dirPath):\n",
    "                    os.makedirs(dirPath)\n",
    "\n",
    "                filepath = os.path.join(dirPath, os.path.split(pathToRawFile)[1])\n",
    "\n",
    "                if not os.path.isfile(filepath):\n",
    "                    shutil.copy(pathToRawFile, filepath)\n",
    "\n",
    "                os.chmod(dirPath, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n",
    "                os.chmod(filepath, stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "                word = win32.Dispatch(\"Word.application\")\n",
    "                #word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "                doc = word.Documents.Open(filepath)\n",
    "                doc.Activate()\n",
    "\n",
    "                # Rename path with .docx\n",
    "                #new_file_abs = os.path.abspath(filepath)\n",
    "                #new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "                word.Selection.Find.Text = \"the\"\n",
    "                word.Selection.Find.Replacement.Text = \"Pariveda Solutions\"\n",
    "                word.Selection.Find.Execute(Replace=2, Forward=True)\n",
    "                # Save and Close\n",
    "                word.ActiveDocument.SaveAs(\n",
    "                    filepath, FileFormat=16\n",
    "                )\n",
    "\n",
    "                word.Quit()\n",
    "                    #catch copy exception here so it doesn't stop all files (?)\n",
    "            else:\n",
    "                print(\"not a file\")\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "\n",
    "#pathToRawFile = \"C:\\\\Users\\\\Sydney.knox\\\\Documents\\\\rawDataDI\\\\TX 17404 Contract Regional (7.2.18).docx\"\n",
    "#modificationTest(pathToRawFile)\n",
    "#modificationTest_changeFile(pathToRawFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Services Extraction\n",
    "This code was created by a separate Pariveda team that was on furlough for a couple days. It is the beginnings of services extraction, but needs work to be fully flexible and integrated with the rest of our codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "code_folding": [
     5,
     13,
     17,
     37,
     69,
     78,
     93,
     101,
     121
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categories=['Diagnostic and Preventive Services', 'Basic Services', 'Major Services', 'Orthodontic Services']\n",
    "planTypes=['PREVENTIVE PLAN', 'ENHANCED PLAN']\n",
    "coverageNames=['Diagnostic:', 'Preventive:', 'Sealants:', 'Oral Surgery:', 'General Anesthesia or IV Sedation:', 'Endodontics:', 'Periodontics:', 'Palliative:', 'Restorative:', 'Specialist Consultations:', 'Night Guards/Occlusal Guard:', 'Night Guards/Occlusal Guards:', 'Crowns, Inlays/Onlays and Cast Restorations:', 'Other Basic Services:', 'Crowns and Inlays/Onlays:', 'Crowns and Inlays/Onlays and Gold Fillings:', 'Prosthodontics:', 'Denture Repairs:', 'Other Services:', 'Temporomandibular Joint (TMJ):', 'Other Major Services:']\n",
    "\n",
    "\n",
    "def getFilesFromDir(path, extension):\n",
    "    try:\n",
    "        x = glob.glob(path + '/*.' + extension)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    return x\n",
    "\n",
    "def getFileBaseName(path, filename):\n",
    "    return filename[len(path) + 1:]\n",
    "\n",
    "#For the given docx file, search for the Contract Benefit Levels table and output the rows as a 2D array\n",
    "def createPercentageTableList(docName):\n",
    "    print('Printing table for ' + docName)\n",
    "    document = Document(docName)\n",
    "    percentages = []\n",
    "    currPlan = 'DEFAULT'\n",
    "    for i in range(len(document.tables)):\n",
    "        table = document.tables[i]\n",
    "        for x in range(len(table.rows)):\n",
    "            row = table.rows[x]\n",
    "            firstCell = row.cells[0].text.strip()\n",
    "            if firstCell.upper() in planTypes:\n",
    "                currPlan = firstCell.upper()\n",
    "            if firstCell in categories:\n",
    "                entry = [currPlan]\n",
    "                for y in range(len(row.cells)):\n",
    "                    cell = row.cells[y].text\n",
    "                    entry.append(cell)\n",
    "                percentages.append(entry)\n",
    "    return percentages\n",
    "\n",
    "def createDescriptionTableList(docName):\n",
    "    print('Printing table for ' + docName)\n",
    "    document = Document(docName)\n",
    "    descriptions = []\n",
    "    currPlan = 'DEFAULT'\n",
    "    categoryCounter = 0\n",
    "    incrementCounter = False\n",
    "    for i in range(len(document.tables)):\n",
    "        table = document.tables[i]\n",
    "        if incrementCounter:\n",
    "            categoryCounter += 1\n",
    "            incrementCounter = False\n",
    "        firstCell = table.rows[0].cells[0].text.strip()\n",
    "        if firstCell.upper() in planTypes:\n",
    "            currPlan = firstCell.upper()\n",
    "            categoryCounter = 0\n",
    "        elif isDescriptionTable(table):\n",
    "            for x in range(len(table.rows)):\n",
    "                row = table.rows[x]\n",
    "                for y in range(len(row.cells)):\n",
    "                    cell = row.cells[y].text.strip()\n",
    "                    if cell in coverageNames and len(row.cells) > (y+1):\n",
    "                        nextCell = row.cells[y+1].text.strip()\n",
    "                        if nextCell not in coverageNames:\n",
    "                            entry = [currPlan, categories[categoryCounter]]\n",
    "                            incrementCounter = True\n",
    "                            entry.append(cell[:-1])\n",
    "                            entry.append(nextCell)\n",
    "                            descriptions.append(entry)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "def isDescriptionTable(table):\n",
    "    for x in range(len(table.rows)):\n",
    "        row = table.rows[x]\n",
    "        for y in range(len(row.cells)):\n",
    "            cell = row.cells[y].text.strip()\n",
    "            if cell in coverageNames:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def parseOrthodontics(filePath):\n",
    "    document = Document(filePath)\n",
    "    planIndex = 0\n",
    "    orthodontics = []\n",
    "    entry = []\n",
    "    planList = getOrthodonticPlans(filePath)\n",
    "    for x in range(len(document.paragraphs)):\n",
    "        paragraph = document.paragraphs[x]\n",
    "        if paragraph.text == 'Orthodontic Services' or paragraph.text == 'Orthodontic Benefits:':\n",
    "            entry = [planList[planIndex], 'Orthodontic Services', '', document.paragraphs[x+1].text]\n",
    "            orthodontics.append(entry)\n",
    "            if (planIndex + 1) < len(planList):\n",
    "                planIndex += 1\n",
    "    return orthodontics\n",
    "\n",
    "def multiplePlans(filePath):\n",
    "    document = Document(filePath)\n",
    "    for table in document.tables:\n",
    "        firstCell = table.rows[0].cells[0].text.strip()\n",
    "        if firstCell.upper() in planTypes:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def getOrthodonticPlans(filePath):\n",
    "    document = Document(filePath)\n",
    "    categoryIndex = 0\n",
    "    planIndex = 0\n",
    "    usedCategories = []\n",
    "    planList = []\n",
    "    if not multiplePlans(filePath):\n",
    "        return ['DEFAULT']\n",
    "    for paragraph in document.paragraphs:\n",
    "        testCategory = paragraph.text.strip()\n",
    "        if testCategory in categories:\n",
    "            if testCategory not in usedCategories:\n",
    "                usedCategories.append(testCategory)\n",
    "            else:\n",
    "                usedCategories = []\n",
    "                planIndex += 1\n",
    "            if testCategory == 'Orthodontic Services':\n",
    "                planList.append(planTypes[planIndex])\n",
    "    return planList\n",
    "\n",
    "def main(path):\n",
    "    try:\n",
    "        files = getFilesFromDir(path, 'docx')\n",
    "\n",
    "        with open('plans.tsv', 'w') as output:\n",
    "            for file in files:\n",
    "                filename = getFileBaseName(path, file)\n",
    "                rows = createDescriptionTableList(file)\n",
    "                ortho = parseOrthodontics(file)\n",
    "\n",
    "                rows = rows + ortho\n",
    "\n",
    "                for columns in rows:\n",
    "                    columns.insert(0, filename)\n",
    "                    rowString = '\\t'.join(columns)\n",
    "                    output.write(rowString + '\\n')\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "#main('/Users/sydneyknox/Documents/data-insights/data/raw')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
