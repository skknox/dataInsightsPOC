{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import random as rng\n",
    "import nltk\n",
    "import fnmatch\n",
    "\n",
    "import docx2txt\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "import sys as sys\n",
    "from sys import platform\n",
    "import re as re\n",
    "from statistics import mode\n",
    "import datefinder\n",
    "from nltk import ne_chunk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "    if(platform == \"win32\"):\n",
    "        import win32com.client as win32\n",
    "        from win32com.client import constants\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "import subprocess\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Psuedocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Analysis Function\n",
    "\n",
    "## Function which calculates the number of combinations (Not Permutations) of distinct values in a dataset. Would \n",
    "## take in a Pandas DataFrame ideally and multiply all distinct value counts for all available columns.\n",
    "\n",
    "# Expected input: Pandas DataFrame\n",
    "# Expected output: Int, the number of possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function to collect mentions of States/Districts/Territories in the United States (DC and Puerto Rico are included)\n",
    "## and rank their relevence, frequency should be a good factor. Note: California, Pennsylvania, and Georgia could \n",
    "## appear multiple times from addresses.\n",
    "\n",
    "# Expected input: A single file, text.\n",
    "# Expected output: A list of locations mentioned, ranked by most used to least used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function for searching a tokenized sentence dictionary for words or phrases. There's probably \n",
    "## already something for this.\n",
    "\n",
    "# Expected input: A tokenized list, and a phrase to search for.\n",
    "# Expected output: A list of sentences containing the phrase searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function to take a file and produce the Acronyms from them.\n",
    "\n",
    "# Expected Input: Text File to be parsed, the Acronym to search\n",
    "# Expected output: A list of possible phrases, most likely first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function to take Acronyms and match them to the best possible N-gram for them from a document. Not all will be \n",
    "## possible, but a list of options will help.\n",
    "\n",
    "# Expected Input: Text File to be parsed, the Acronym to search\n",
    "# Expected output: A list of possible phrases, sorted most likely first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Generally important\n",
    "\n",
    "## Need to expand stopwords to include States and Districts, also Delta Dental adjacent names as another available set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Function to load in two different text extracted contracts and return a comparison metric between \n",
    "## the two (Similarity, possibly as a percent?)\n",
    "\n",
    "# Expected Input: Two contracts for Comparison\n",
    "# Expected Output: A measure, some kind of decimal to represent similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Prep Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Is it docx or pdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def checkFileType(filename):\n",
    "        if(filename.lower().endswith(('.docx'))):\n",
    "            return 0\n",
    "        elif(filename.lower().endswith(('.pdf'))):\n",
    "            return 1\n",
    "        elif(filename.lower().endswith(('.doc'))):\n",
    "            return 2\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def makeFilePath(docName):\n",
    "    raw_data_path = os.path.join(os.getcwd(), 'data', 'raw')\n",
    "    return os.path.join(raw_data_path, docName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    text = text.replace(\"\\t\", ' ')\n",
    "    #text = text.replace(\",\", ' ')\n",
    "    \n",
    "    dblSpacesRemaining = True\n",
    "    while(dblSpacesRemaining):\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        if not \"  \" in text:\n",
    "            dblSpacesRemaining = False\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: process dataFrame and group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDF(txtFile):\n",
    "    df = pd.read_csv(txtFile, sep=\" \", header=None) #this doesn't work for me bc of variable number of cols\n",
    "    df = df.T \n",
    "    df = df.dropna()\n",
    "\n",
    "    df['SingleRow']=1\n",
    "\n",
    "    df=df.rename(columns={0 : 'Words'})\n",
    "    print(\"in processDF \" + txtFile)\n",
    "    df.describe(include=\"all\")\n",
    "    #print(df.groupby('Words').SingleRow.sum().sort_values())\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Process a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDocFileWindows(filepath, errorFile):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.application\")\n",
    "        #word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "        doc = word.Documents.Open(filepath)\n",
    "        doc.Activate ()\n",
    "\n",
    "        # Rename path with .docx\n",
    "        new_file_abs = os.path.abspath(filepath)\n",
    "        new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "\n",
    "        # Save and Close\n",
    "        word.ActiveDocument.SaveAs(\n",
    "        new_file_abs, FileFormat=constants.wdFormatXMLDocument\n",
    "        )\n",
    "        doc.Close(False)\n",
    "\n",
    "        return new_file_abs\n",
    "        \n",
    "    except Exception as e:\n",
    "        errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    \n",
    "def processDocFileMac(filepath, errorFile):\n",
    "    #textutil -convert docx ~/Desktop/mypage.webarchive\n",
    "    try:\n",
    "        subprocess.run([\"textutil\", \"-convert\", \"docx\", filepath])\n",
    "        newFilePath = re.sub(r'\\.\\w+$', '.docx', filepath)\n",
    "        #print(\"in Mac doc file process: \" + newFilePath)\n",
    "        return newFilePath\n",
    "    except Exception as e:\n",
    "        errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "        print(\"Error processDocFileMac:  \" + str(e))\n",
    "        return None\n",
    "    \n",
    "\n",
    "def processDocFile(filePath, errorFile):\n",
    "    \n",
    "    try:\n",
    "        if(platform == \"win32\"):\n",
    "            newFilePath = processDocFileWindows(filePath, errorFile)\n",
    "        else:\n",
    "            newFilePath = processDocFileMac(filePath, errorFile)\n",
    "        #print(\"return from mac vs windows: \" + newFilePath)\n",
    "        processedFilePath = processDocxFile(newFilePath, errorFile)\n",
    "    except Exception as e:\n",
    "        errorFile.write(filePath + \", \" + str(e) + \"\\n\")\n",
    "        print(\"Error processDocFile:  \" + str(e))\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    return processedFilePath\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processDocxFile(filePath, errorFile):\n",
    "    #print(filePath)\n",
    "    try:\n",
    "        docxText = docx2txt.process(filePath)\n",
    "        #print(docxText)\n",
    "        replacedText = cleanText(docxText)\n",
    "        #print(replacedText)\n",
    "        #print(filePath)\n",
    "        fileName = os.path.split(filePath)[1]#.split('/')[-1]\n",
    "        #print(fileName)\n",
    "        fileName = fileName.replace(\",\",\"\")\n",
    "        #print(fileName)\n",
    "        baseFileName = fileName[0:-5]\n",
    "        #print(baseFileName)\n",
    "        cwd = os.getcwd()\n",
    "        \n",
    "        newFilePath = os.path.join(cwd,'output',  baseFileName + \".txt\")\n",
    "        \n",
    "        #print(newFilePath)\n",
    "        singleFileDocx=open(newFilePath, 'wb+')    \n",
    "        singleFileDocx.write(replacedText.encode(\"utf-8\"))\n",
    "        singleFileDocx.close()\n",
    "    except Exception as e:\n",
    "        errorFile.write(filePath + \", \" + str(e) + \"\\n\")\n",
    "        return None\n",
    "\n",
    "    #temp_df = processDF('singleTextDocx.txt')\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Process pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processPDFfile(filePath, errorFile):\n",
    "    password = \"\"\n",
    "    extracted_text = \"\"\n",
    "    try:\n",
    "        #print(filePath)\n",
    "        fileName = os.path.split(filePath)[1]#.split('/')[-1]\n",
    "        #print(fileName)\n",
    "        fileName = fileName.replace(\",\", \" \")\n",
    "        baseFileName = fileName[0:-4]\n",
    "    \n",
    "        fp = open(filePath, \"rb\")\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser, password)\n",
    "        \n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "            \n",
    "        # Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "\n",
    "        # set parameters for analysis\n",
    "        laparams = LAParams()\n",
    "\n",
    "        # Create a PDFDevice object which translates interpreted information into desired format\n",
    "        # Device needs to be connected to resource manager to store shared resources\n",
    "        # device = PDFDevice(rsrcmgr)\n",
    "        # Extract the decive to page aggregator to get LT object elements\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "        # Create interpreter object to process page content from PDFDocument\n",
    "        # Interpreter needs to be connected to resource manager for shared resources and device \n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            # As the interpreter processes the page stored in PDFDocument object\n",
    "            interpreter.process_page(page)\n",
    "            # The device renders the layout from interpreter\n",
    "            layout = device.get_result()\n",
    "            # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "            for lt_obj in layout:\n",
    "                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    newText = lt_obj.get_text()\n",
    "                    newText = newText.replace('\\n', ' ')\n",
    "                    extracted_text += newText\n",
    "\n",
    "        #close the pdf file\n",
    "        fp.close()\n",
    "        \n",
    "        extracted_text = cleanText(extracted_text)#extracted_text.replace(\"\\n\", ' ')\n",
    "        cwd = os.getcwd()\n",
    "        newFilePath = os.path.join(cwd,'output', baseFileName + '-pdf' + \".txt\")\n",
    "        #print(newFilePath)\n",
    "        with open(newFilePath, 'wb+') as singleFilePDF:\n",
    "            singleFilePDF.write(extracted_text.encode(\"utf-8\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        errorFile.write(filePath + \", \" + str(e) + \"\\n\")\n",
    "        return None\n",
    "        #temp_df = processDF('./data/output/' + baseFileName + \".txt\")\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NLTK Tokenizing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: DD specific text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ddCleanText(text):\n",
    "    newText = text.replace('Delta Dental', 'DeltaDental')\n",
    "    newText = newText.replace('DELTA DENTAL', 'DELTADENTAL')\n",
    "    newText = newText.replace('DeltaDental Insurance Company', 'DeltaDentalInsuranceCompany')\n",
    "    return newText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    customStopWords = set(stopwords.words('english')+list(punctuation))\n",
    "    wordsWOStop=[word for word in words if word not in customStopWords]\n",
    "    \n",
    "    return wordsWOStop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized Sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSents(text):\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBigrams(tokens):\n",
    "    bigram_measures=nltk.collocations.BigramAssocMeasures();\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    sorted_bgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "    \n",
    "    return sorted_bgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTrigrams(tokens):\n",
    "    trigram_measures =nltk.collocations.TrigramAssocMeasures();\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    sorted_tgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "\n",
    "    return sorted_tgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MetaData and Attribute Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Filename\n",
    "Contract Start\n",
    "Contract End\n",
    "Contract Duration\n",
    "State\n",
    "Delta Office Involved\n",
    "\n",
    "#### Group Information\n",
    "(Group Number)\n",
    "\n",
    "#### Numeric attributes Only\n",
    "Basics\n",
    "Diagnostics\n",
    "Major\n",
    "Endo\n",
    "Oral\n",
    "Perio\n",
    "Prostho\n",
    "Ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##Establish a dataframe to capture the attributes\n",
    "#d = {'key': 'file','value':fileName}\n",
    "#{'key':'state', 'value':state}\n",
    "#df = pd.DataFrame(d, index=['uid'])\n",
    "d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### NLTK synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### I kept this to processing single words, bigrams or trigrams so as to keep the complexity down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: translate from syn POS to nltk POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Use POS to cull wordnet synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonyms_usingPOS(word_tuple):\n",
    "    #print(word_tuple)\n",
    "    word_tagged = word_tuple[0]\n",
    "    word_pos = get_wordnet_pos(word_tuple[1])\n",
    "    syns = wn.synsets(word_tagged, pos=word_pos)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    #print(syns)\n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get synonyms of a single word. Helper function to Bigram and Trigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## You can't cull this one down with the POS b/c you can't tag a single word\n",
    "def getSyns(word):\n",
    "    syns1 = wn.synsets(word)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns1:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    \n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get a similar bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarBigrams(word1, word2):\n",
    "    \n",
    "    tagged_words = nltk.pos_tag([word1,word2])\n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            word_set.add(\" \".join([word1, word2]))\n",
    "    \n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get a similar trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarTrigrams(word1, word2, word3):\n",
    "    tagged_words = nltk.pos_tag([word1,word2,word3])\n",
    "    \n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    set3 = getSynonyms_usingPOS(tagged_words[2])\n",
    "    if not len(set3):\n",
    "        set3.add(word3)\n",
    "    \n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            for word3 in set3:\n",
    "                word_set.add(\" \".join([word1, word2, word3]))\n",
    "    #print(word_set)\n",
    "    \n",
    "    return word_set\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get synonyms from a list of key words. Returns more keywords/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonymsFromList(keywords):\n",
    "    matches = []\n",
    "\n",
    "    for kw in keywords:\n",
    "        try:\n",
    "            words = word_tokenize(kw)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        #print(words)\n",
    "        if(len(words) == 1):\n",
    "            #print(\"is 1\")\n",
    "            syns = getSyns(words[0])\n",
    "            for syn in syns:\n",
    "                matches.append(syn)\n",
    "            #keywords.append(list(syns))\n",
    "        elif(len(words) == 2):\n",
    "            #print(\"is 2\")\n",
    "            syns = getSimilarBigrams(words[0],words[1])\n",
    "            #print(syns)\n",
    "            matches.extend(getSimilarBigrams(words[0],words[1]))\n",
    "        elif(len(words) == 3):\n",
    "            #print(\"is 3\")\n",
    "            matches.extend(getSimilarTrigrams(words[0],words[1],words[2]))\n",
    "        else:\n",
    "            print(\"keyword string too long\")\n",
    "        #print(matches)\n",
    "    keywords.extend(matches)\n",
    "    keywords = set(keywords)\n",
    "\n",
    "    #print(start_keywords)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get file name\n",
    "Get the filename from a full path. Determines the OS and splits the string correctly based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFileName(fullPath):\n",
    "    try:\n",
    "        if(platform == \"win32\"):\n",
    "            fileName = fullPath.split(\"\\\\\")[-1]\n",
    "        else:\n",
    "            fileName = fullPath.split(\"/\")[-1]\n",
    "        return fileName\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get group number\n",
    "Uses regex's made from keywords to attempt to find a group number in the file. Failing that, it searches the filename for the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getGroupNumber(sents_tokens, filePath):\n",
    "    \n",
    "    group_keywords = [\"group number\", \"groupnumber\"]\n",
    "    regex_exps = []\n",
    "    poss_nums = []\n",
    "    finalGN = None\n",
    "    \n",
    "    try:\n",
    "        #Create regex exps out of group number keywords\n",
    "        for kw in group_keywords:\n",
    "                temp_re = kw + \"\\W\\s*(?P<gn>\\d+)(?P<gn2>[-\\s]\\d+)\"\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "        #For each sentence, search for the expression, if found add the number to\n",
    "        #list of possible group numbers\n",
    "        for sent in sents_tokens:\n",
    "            #print(sent)\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    temp_gn = result.group('gn')\n",
    "                    temp_gn2 = result.group('gn2')\n",
    "                    if(temp_gn2):\n",
    "                        if(len(temp_gn2[1:])>len(temp_gn)):\n",
    "                            poss_nums.append(temp_gn2[1:])\n",
    "                        else:\n",
    "                            poss_nums.append(temp_gn)\n",
    "                    else:\n",
    "                        #temp_gn2 = temp_gn2[1:]\n",
    "                        #print(temp_gn2)\n",
    "                        \n",
    "                        poss_nums.append(temp_gn)\n",
    "\n",
    "        #Try and get the number from the file name, looking for list of numeric chars\n",
    "        num_regex = re.compile(\"(?P<gn>\\d+)(?P<gn2>[-\\s]+\\d+)?\")\n",
    "        fileName = getFileName(filePath)\n",
    "        fileGN = num_regex.search(fileName)\n",
    "\n",
    "        if not fileGN==None:#if they filename has a number sequence\n",
    "            #print(fileGN.group('gn'))\n",
    "            #print(fileGN.group('gn2'))\n",
    "            temp_gn = None\n",
    "            if(fileGN.group('gn2')):\n",
    "                if(len(fileGN.group('gn'))>=len(fileGN.group('gn2')[1:])):\n",
    "                    temp_gn = fileGN.group('gn')\n",
    "                else:\n",
    "                    temp_gn = fileGN.group('gn2')[1:]\n",
    "\n",
    "            #print(fileGN.group('gn'))\n",
    "            #print(fileGN.group('gn2'))\n",
    "            #print(fileGN.group())\n",
    "            else:\n",
    "                temp_gn = fileGN.group()\n",
    "            if (temp_gn in poss_nums):#then if the file group number matches one in the document, choose it\n",
    "                finalGN = temp_gn\n",
    "            else:\n",
    "                poss_nums.append(temp_gn)#otherwise add the filename one to the list and try to get the most co\n",
    "                try:\n",
    "                    finalGN = mode(poss_nums)\n",
    "                except Exception as e:\n",
    "                    #print(str(e))\n",
    "                    #print(poss_nums)\n",
    "                    return -1\n",
    "                    #print(\"Unexpected error: Cannot determine group number of file: \" + filePath)\n",
    "        else: #it is none and there was no group number in the filename\n",
    "            try:\n",
    "                finalGN = mode(poss_nums)\n",
    "            except Exception as e:\n",
    "                #no mode found, couldn't find a group number\n",
    "                #print(str(e))\n",
    "                #print(poss_nums)\n",
    "                return -1\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return None\n",
    "    \n",
    "    return finalGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get contract start\n",
    "Uses regex and a list of keywords to attempt to find the start date of the contract. It makes multiple passes based on patterns seen in contract samples so far.\n",
    "\n",
    "Some of the passes are necessary to filter out non-date numbers that the datefinder incorrectly parses to dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractStart(sents_tokens):\n",
    "    start_keywords = [\"effective date\\S\\s*\\S\", \"effective\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:20]\n",
    "                    subset = \" \".join(subset)\n",
    "                    poss_dates.append(subset)\n",
    "                    if \"effective date\" in subset.lower():\n",
    "                        poss_dates.append(subset)\n",
    "\n",
    "                \n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\"]\n",
    "    \n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:6])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-6:])\n",
    "                    subset = subset_2 + subset_1\n",
    "\n",
    "                    m = datefinder.find_dates(subset, strict=True)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "                    \n",
    "    ## Second pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "            #print(sent)\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.search(sent)\n",
    "\n",
    "            if not year==None:\n",
    "                #print(sent)\n",
    "                #print(year.group())\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                for match in m:\n",
    "                    if match.year >= 1966:\n",
    "                        matches.append(match)\n",
    "## Last pass: try to find the most common date. If there is more than one mode, choose the earliest date\n",
    "##.           this seems to occur when it is finding the contract start and end in equal quantities\n",
    "        #print(matches)\n",
    "        try:\n",
    "            finalDate = mode(matches)\n",
    "        except ValueError as e:\n",
    "            #print(str(e))\n",
    "            if matches:\n",
    "                earliestMatch = matches[0]\n",
    "                for match in matches:\n",
    "                    if(match < earliestMatch):\n",
    "                        earliestMatch = match\n",
    "                finalDate = earliestMatch\n",
    "            else:\n",
    "                finalDate = datetime.datetime(1066, 1, 1)\n",
    "        except Exception as e:\n",
    "            return None #i.e. not only could they not find a start date, something failed\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "####  get Contract End\n",
    "Similar to get contract start, it uses regex and keywords over multiple passes to attempt and find the contract end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractEnd(sents_tokens):\n",
    "    \n",
    "    start_keywords = [\"contract term\\S\\s*\\S\", \"contract term \", \"contract end\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates based on keywords\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:30]\n",
    "                    subset = \" \".join(subset)\n",
    "                    #print(subset)\n",
    "                    poss_dates.append(subset)\n",
    "\n",
    "\n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\"]\n",
    "\n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:12])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-12:])\n",
    "                    subset = subset_2 + subset_1\n",
    "\n",
    "                    m = datefinder.find_dates(subset, strict=True)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "\n",
    "    ## Pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "            #print(sent)\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.findall(sent)\n",
    "            #print(year)\n",
    "            if len(year)>=2:\n",
    "                #print(year)\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                maxMatch = datetime.datetime(1066,1,1)\n",
    "                if not m == None:\n",
    "                    #print(m)\n",
    "                    for match in m:\n",
    "                        if match > maxMatch:\n",
    "                            maxMatch = match\n",
    "                    if maxMatch.year >= 1966:\n",
    "                        matches.append(match)\n",
    "\n",
    "\n",
    "        #print(matches)\n",
    "\n",
    "    ### If there are exactly two matches, try to find a max. If error b/c they're the same, choose one\n",
    "        if(len(matches) == 2):\n",
    "            try:\n",
    "                finalDate = max(matches)\n",
    "            except ValueError as e:\n",
    "                finalDate = matches[0]\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "    ## If there are more, try and find the top two most mentioned and take the later. else just take the latest            \n",
    "        elif(len(matches) > 2):\n",
    "\n",
    "            try:\n",
    "                date1 = mode(matches)\n",
    "                matches.remove(date1)\n",
    "\n",
    "                date2 = mode(matches)\n",
    "                matches.remove(date2)\n",
    "\n",
    "                finalDate = max([date1, date2])\n",
    "            except ValueError as e:\n",
    "                #print(str(e))\n",
    "                if matches:\n",
    "                    latestMatch = matches[0]\n",
    "                    for match in matches:\n",
    "                        if(match > latestMatch):\n",
    "                            latestMatch = match\n",
    "                    finalDate = latestMatch\n",
    "            except Exception as e:\n",
    "                return None\n",
    "        else:\n",
    "            return datetime.datetime(1066, 1, 1)\n",
    "            #print(\"could not find contract end for file\")\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    #print(\"\\n\")\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get Contract Duration\n",
    "Uses the functions getContractStart and getContractEnd to calculate a duration if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractDuration(sents_tokens):\n",
    "    start = None\n",
    "    end = None\n",
    "    duration = None\n",
    "    \n",
    "    try:\n",
    "        start = getContractStart(sents_tokens)\n",
    "        end = getContractEnd(sents_tokens)\n",
    "        \n",
    "        if(start and end):\n",
    "            if (start.year==1066) or (end.year == 1066):\n",
    "                return -1\n",
    "            else:\n",
    "                duration = (end - start).days\n",
    "                if duration <= 0:\n",
    "                    return -1\n",
    "        else:\n",
    "            return -1\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get State/Location: -- Not Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Helper function to create location data set from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def makeLocationDataStruct():\n",
    "    categories = []\n",
    "    location_data = {}\n",
    "    \n",
    "    us_filename = 'us_cities_states_counties.csv'\n",
    "    cwd = os.getcwd()\n",
    "    filepath = os.path.join(cwd, us_filename)\n",
    "    #print(filepath)\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='|',escapechar=',')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                for cat in row:\n",
    "                    categories.append(cat)\n",
    "                    location_data[cat]=set()\n",
    "                category_row = 0\n",
    "            else:\n",
    "                \n",
    "                for item in range(len(row)):\n",
    "                    #print(row[item])\n",
    "                    if len(row[item]):\n",
    "                        location_data[categories[item]].add(row[item])\n",
    "        location_data['State full'].add(\"Washington , DC\")\n",
    "        location_data['State full'].add(\"Washington , D.C.\")\n",
    "    csvfile.close()\n",
    "    \n",
    "    states_filename = 'state_abbrv_to_name.csv'\n",
    "    filepath = os.path.join(cwd, states_filename)\n",
    "    \n",
    "    location_data['translate_s2l'] = {}\n",
    "    location_data['translate_l2s'] = {}\n",
    "    \n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                category_row = 0\n",
    "            else:\n",
    "                location_data['translate_s2l'][row[0]] = row[1]\n",
    "                location_data['translate_l2s'][row[1]] = row[0]\n",
    "    csvfile.close()\n",
    "    #print(location_data)\n",
    "    return location_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Call function to set up location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Idaho', 'Indiana', 'New Jersey', 'California', 'Palau', 'Alaska', 'Washington , D.C.', 'Mississippi', 'US Armed Forces Europe', 'Virginia', 'Nevada', 'Iowa', 'Georgia', 'Michigan', 'Maryland', 'Arkansas', 'Montana', 'Federated States of Micronesia', 'Missouri', 'West Virginia', 'North Carolina', 'Nebraska', 'Guam', 'Washington , DC', 'Wyoming', 'American Samoa', 'Florida', 'Virgin Islands', 'Puerto Rico', 'New Mexico', 'Washington DC', 'New York', 'Alabama', 'Minnesota', 'North Dakota', 'Washington', 'New Hampshire', 'Maine', 'Louisiana', 'Ohio', 'Illinois', 'Kentucky', 'Rhode Island', 'US Armed Forces Pacific', 'Pennsylvania', 'Oregon', 'Northern Mariana Islands', 'South Carolina', 'Connecticut', 'Tennessee', 'Hawaii', 'Marshall Islands', 'Washington D.C.', 'Delaware', 'Arizona', 'Wisconsin', 'Kansas', 'Massachusetts', 'Vermont', 'Oklahoma', 'District of Columbia', 'Utah', 'Texas', 'South Dakota', 'Colorado'}\n"
     ]
    }
   ],
   "source": [
    "location_data = makeLocationDataStruct()\n",
    "print(location_data['State full'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Get Client Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def checkIfCity(loc_str, loc_data, isDelta, isContractholder):\n",
    "    cities = []\n",
    "    \n",
    "    if (loc_str in loc_data['City']):\n",
    "        cities.append(loc_str)\n",
    "    return cities\n",
    "\n",
    "def checkIfState(loc_str, loc_data):\n",
    "    states = []\n",
    "    \n",
    "    if(loc_str in loc_data['State full']):\n",
    "        states.append(loc_str.lower())\n",
    "    if(loc_str in location_data['State short']):\n",
    "        try:\n",
    "            states.append(location_data['translate_s2l'][loc_str].lower())\n",
    "        except Exception as e:\n",
    "            return states\n",
    "    return states\n",
    "    \n",
    "def getClientLocation(sents_tokens, bgs, tgs, location_data, filename):\n",
    "    loc_sents = set()\n",
    "    cities = []\n",
    "    states = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for sent in sents_tokens:\n",
    "\n",
    "            sent_cities = []\n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to contractholder\" in sent.lower()\n",
    "            DCtext = [\"washington , d.c.\",\"washington , dc\", \"washington dc\",\"district of columbia\",\"washington d.c.\"]\n",
    "\n",
    "            #Used to filter out sentences in ALL caps. They interfere with the Abbreviated States lists\n",
    "            if sent.isupper():\n",
    "                sent = sent.lower()\n",
    "\n",
    "\n",
    "            text = nltk.word_tokenize(sent)\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            while (i < len(text)) and (len(text) > 2):\n",
    "                if(i < len(text)-2):\n",
    "                    text_bg = \" \".join([text[i], text[i+1]])\n",
    "                else:\n",
    "                    text_bg = \"\"\n",
    "                if(i < len(text)-2):\n",
    "                    text_tg = \" \".join([text[i], text[i+1], text[i+2]])\n",
    "                else:\n",
    "                    text_tg = \"\"\n",
    "\n",
    "                sent_cities.extend(checkIfCity(text[i], location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_bg, location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_tg, location_data, isDelta, isContractholder))\n",
    "\n",
    "                sent_states.extend(checkIfState(text[i], location_data))\n",
    "                sent_states.extend(checkIfState(text_bg, location_data))\n",
    "                sent_states.extend(checkIfState(text_tg, location_data))\n",
    "\n",
    "                i+=1\n",
    "\n",
    "            if (len(sent_states)>0) and not isDelta:\n",
    "                \n",
    "                if bool(set(sent_states).intersection(DCtext)):\n",
    "                    \n",
    "                    for st in sent_states:\n",
    "                        if st in DCtext:\n",
    "                            states.append(st)\n",
    "                            if(isNotice):\n",
    "                                states.append(st)\n",
    "                           \n",
    "                short_sent_states = []\n",
    "                for st in sent_states:\n",
    "                    try:\n",
    "                        short_sent_states.append(location_data['translate_l2s'][st.title()].lower())\n",
    "                    except Exception as e:\n",
    "                        e=e\n",
    "\n",
    "                for city in sent_cities:\n",
    "                    for state in sent_states+short_sent_states:\n",
    "                        \n",
    "                        if (len(checkIfState(city + \" \" + state.title(), location_data))==0) and not (city is \"New York\"):\n",
    "                                \n",
    "                                add_regex_str = str(city) + \"[-,\\s]+\" + str(state) + \"[-,\\s]\"\n",
    "                                add_regex = re.compile(add_regex_str, re.IGNORECASE)\n",
    "                                matches = add_regex.findall(sent)\n",
    "                                if matches:\n",
    "                                    cities.append(city)\n",
    "                                    states.append(state)\n",
    "                                    if(isNotice):\n",
    "                                        states.append(state)\n",
    "                                    \n",
    "            \n",
    "        FN_chunks = re.findall(r\"[\\w]+|[-\\s_]\", filename)\n",
    "        \n",
    "        for chunk in FN_chunks:\n",
    "            FN_state = checkIfState(chunk, location_data)\n",
    "            if len(FN_state)>0:\n",
    "                for i in range(0,5):\n",
    "                    states.append(FN_state[0])\n",
    "                    i+=1\n",
    "                \n",
    "              \n",
    "        try:\n",
    "            print(states)  \n",
    "            final_state = mode(states)\n",
    "            print(final_state)\n",
    "            if(len(final_state)<3):\n",
    "                try:\n",
    "                    final_state = location_data['translate_s2l'][final_state.upper()]\n",
    "                except Exception as e:\n",
    "                    #print(str(e))\n",
    "                    final_state = final_state\n",
    "        except ValueError as ve:\n",
    "            #print(str(ve))\n",
    "            final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(str(e))\n",
    "            final_state = None\n",
    "        try:\n",
    "            final_city = mode(cities)\n",
    "        except ValueError as ve:\n",
    "            #print(str(ve))\n",
    "            final_city = \"not_a_city\"\n",
    "            #print(final_city)\n",
    "        except Exception as e:\n",
    "           # print(str(e))\n",
    "            final_city = None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None #error in function execution\n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### To keep: address regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#    isDelta = \"deltadental\" in sent.lower()\n",
    "            #    isContractholder = \"contractholder\" in sent.lower()\n",
    "            #    if not isDelta and isContractholder:\n",
    "                   # print(sent)\n",
    "                   # print(\"\\n\")\n",
    "                    #[aA]ddress[-: ]\n",
    "                    #\"\\s+\\d+\\D+\" + \"[\"+\"|\".join(cities) + \"]\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "            #        add_regex_str = \"[aA]ddress[-: ]\\s+\\d+[\\D\\d]+\" + \"[\"+\"|\".join(sent_cities) + \"]*\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]*\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "                   # print(add_regex_str)\n",
    "            #        add_regex = re.compile(add_regex_str)\n",
    "                   # print(str(sent_cities))\n",
    "                  #  print(sent_states)\n",
    "            #        matches = add_regex.findall(sent)\n",
    "                    #print(matches)\n",
    "                    #if matches:\n",
    "                     #   for m in matches:\n",
    "                   #         print(m)\n",
    "                   # else:\n",
    "                   #     add_regex_str = \"\\s+\\d+[\\D\\d]+\" + \"[\"+\"|\".join(sent_cities) + \"]*\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]*\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "                   #     add_regex = re.compile(add_regex_str)\n",
    "                   #     matches = add_regex.findall(sent)\n",
    "                        #loc_sents.add(matches.group())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Delta Office Involved -- Not Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getDeltaOffice(sents_tokens, bgs, tgs, location_data):\n",
    "    loc_sents = set()\n",
    "    cities = []\n",
    "    states = []\n",
    "    \n",
    "    try:\n",
    "        for sent in sents_tokens:\n",
    "           \n",
    "            sent_cities = []\n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to delta dental\" in sent.lower()\n",
    "            DCtext = [\"washington , d.c.\",\"washington , dc\", \"washington dc\",\"district of columbia\",\"washington d.c.\"]\n",
    "\n",
    "\n",
    "\n",
    "            if isDelta:\n",
    "                \n",
    "                if sent.isupper():\n",
    "                    sent = sent.lower()\n",
    "                    \n",
    "                text = nltk.word_tokenize(sent)\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                while (i < len(text)) and (len(text) > 2):\n",
    "                    if(i < len(text)-2):\n",
    "                        text_bg = \" \".join([text[i], text[i+1]])\n",
    "                    else:\n",
    "                        text_bg = \"\"\n",
    "                    if(i < len(text)-2):\n",
    "                        text_tg = \" \".join([text[i], text[i+1], text[i+2]])\n",
    "                    else:\n",
    "                        text_tg = \"\"\n",
    "\n",
    "                    sent_cities.extend(checkIfCity(text[i], location_data, isDelta, isContractholder))\n",
    "                    sent_cities.extend(checkIfCity(text_bg, location_data, isDelta, isContractholder))\n",
    "                    sent_cities.extend(checkIfCity(text_tg, location_data, isDelta, isContractholder))\n",
    "\n",
    "                    sent_states.extend(checkIfState(text[i], location_data))\n",
    "                    sent_states.extend(checkIfState(text_bg, location_data))\n",
    "                    sent_states.extend(checkIfState(text_tg, location_data))\n",
    "\n",
    "                    i+=1\n",
    "\n",
    "                if (len(sent_states)>0):\n",
    "                    if bool(set(sent_states).intersection(DCtext)):\n",
    "                        \n",
    "                        for st in sent_states:\n",
    "                            if st in DCtext:\n",
    "                                states.append(st)\n",
    "                                if(isNotice):\n",
    "                                    states.append(st)\n",
    "                                \n",
    "                    short_sent_states = []\n",
    "                    for st in sent_states:\n",
    "                        try:\n",
    "                            short_sent_states.append(location_data['translate_l2s'][st.title()].lower())\n",
    "                        except Exception as e:\n",
    "                            e=e\n",
    "                           \n",
    "                    for city in sent_cities:\n",
    "                        for state in sent_states+short_sent_states:\n",
    "                            if (len(checkIfState(city + \" \" + state.title(), location_data))==0) and not (city is \"New York\"):\n",
    "\n",
    "                                add_regex_str = str(city) + \"[-,\\s]+\" + str(state) + \"[-,\\s]\"\n",
    "                                add_regex = re.compile(add_regex_str, re.IGNORECASE)\n",
    "                                matches = add_regex.findall(sent)\n",
    "                                if matches:\n",
    "                                    cities.append(city)\n",
    "                                    states.append(state)\n",
    "                                    if(isNotice):\n",
    "                                        states.append(state)\n",
    "                                   \n",
    "        try:\n",
    "            final_state = mode(states)\n",
    "            if(len(final_state)<3):\n",
    "                try:\n",
    "                    final_state = location_data['translate_s2l'][final_state.upper()]\n",
    "                except:\n",
    "                    final_state = final_state\n",
    "        except ValueError as ve:\n",
    "            final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            final_state = None\n",
    "        try:\n",
    "            final_city = mode(cities)\n",
    "            #print(final_city)\n",
    "        except ValueError as ve:\n",
    "            final_city = \"not_a_city\"\n",
    "        except Exception as e:\n",
    "            #print(str(e))\n",
    "            final_city = None\n",
    "\n",
    "    except Exception as e:\n",
    "        final_state = None\n",
    "        \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch Run to get attributes\n",
    "#### Functions to process multiple files and their attributes at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: batch pre process: fill output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchPreProcess(errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    ##print(cwd)\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    dataPath = os.path.join(cwd, \"processed\")\n",
    "    if(os.path.isdir(dataPath)):\n",
    "\n",
    "        for file in os.listdir(dataPath):\n",
    "            filepath = os.path.join(dataPath, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                print(\"pre-processing: \" + file)\n",
    "                try:\n",
    "                    \n",
    "                    if(checkFileType(filepath) == 0):\n",
    "                        processedTextPath = processDocxFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" +  filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 1):\n",
    "                        processedTextPath = processPDFfile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 2):\n",
    "                        processedTextPath = processDocFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    else:\n",
    "                        errorFile.write(filepath + \", pre-processing: invalid filetype\\n\")\n",
    "                        raise TypeError('This path does not lead to a valid file type!')                     \n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    errorFile.write(filepath + \", pre-processing,\" + str(e) + \"\\n\")\n",
    "                    print(\"Error pre-processing file: \" + filepath)\n",
    "\n",
    "    else:\n",
    "        print(\"Folder data/raw doesn't exist\")\n",
    "        return None\n",
    "    return \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Batch return token and bigram sets for all output files\n",
    "Returns file information as an array of objects containing key:value information about the file: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[ \n",
    "\n",
    "    {\n",
    "    \n",
    "        'filepath':'users/sydneyknox...', \n",
    "        \n",
    "        'wordTokens':[*tokens*], \n",
    "        \n",
    "        ...\n",
    "        \n",
    "    }, \n",
    "    \n",
    "    {  \n",
    "    \n",
    "        'sentenceTokens':[*tokens*],\n",
    "        \n",
    "        'cleanText':\"string containing the original text from the processed file...\"\n",
    "        \n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchGetTokens(errorFile):\n",
    "    all_tokens = []\n",
    "    cwd = os.getcwd()\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    dataPath = os.path.join(cwd, \"output\")\n",
    "    \n",
    "    if(os.path.isdir(dataPath)):\n",
    "\n",
    "        for file in os.listdir(dataPath):\n",
    "            filepath = os.path.join(dataPath, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                try:\n",
    "\n",
    "                    temp_obj = {}\n",
    "\n",
    "                    with open(filepath, 'r', encoding='utf-8') as txtFile:\n",
    "                        text = txtFile.read()\n",
    "\n",
    "                    temp_obj['filepath'] = filepath\n",
    "\n",
    "                    text = ddCleanText(text)\n",
    "                    temp_obj['cleanText'] = text\n",
    "\n",
    "                    wordTokens = getTokens(text)\n",
    "                    sentTokens = getSents(text)\n",
    "                    temp_obj['wordTokens'] = wordTokens\n",
    "                    temp_obj['sentTokens'] = sentTokens\n",
    "\n",
    "                    bgs = getBigrams(wordTokens)\n",
    "                    tgs = getTrigrams(wordTokens)\n",
    "                    temp_obj['bgs'] = bgs\n",
    "                    temp_obj['tgs'] = tgs\n",
    "\n",
    "                    txtFile.close()\n",
    "                    all_tokens.append(temp_obj)\n",
    "                except Exception as e:\n",
    "                    errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "                    print(\"Error opening and tokenizing \" + file)\n",
    "                    #print(str(e))\n",
    "\n",
    "    else:\n",
    "        print(\"Folder /output doesn't exist. Pre-processing failed.\")\n",
    "        return None\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get metadata attributes\n",
    "This function takes in a single files info -- in this section because it will be used in a batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getMetaDataAtt(file_info):\n",
    "    #print(file_info)\n",
    "    file_attr = {}\n",
    "    file_attr['filepath'] = file_info['filepath']\n",
    "    \n",
    "    fileName = getFileName(file_info['filepath'])\n",
    "    if not fileName:\n",
    "        fileName = file_info['filepath']\n",
    "        file_attr['fileName'] = fileName\n",
    "    else:\n",
    "        print(fileName)\n",
    "        file_attr['fileName'] = fileName\n",
    "    \n",
    "    groupNumber = getGroupNumber(file_info['sentTokens'], file_info['filepath'])\n",
    "    file_attr['groupNumber'] = groupNumber\n",
    "    #print(groupNumber)\n",
    "    #if not (groupNumber):\n",
    "        #Function failed to execute correctly\n",
    "    #    print(\"function getGroupNumber failed to execute.\\n\")\n",
    "    #elif groupNumber==-1:\n",
    "    #     print(\"could not find valid group number in file\")\n",
    "    #else:\n",
    "    #    print(\"group number: \" , groupNumber)\n",
    "    \n",
    "    #contractStartDate = getContractStart(file_info['sentTokens'])\n",
    "    #file_attr['contractStartDate'] = contractStartDate\n",
    "    #print(contractStartDate)\n",
    "    #if not (contractStartDate):\n",
    "        # Function failed to execute\n",
    "    #elif contractStartDate.year == 1066:\n",
    "        #could not find valid start date\n",
    "    #else:\n",
    "    #    print(\"start: \" , contractStartDate)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    #contractEndDate = getContractEnd(file_info['sentTokens'])\n",
    "    #file_attr['contractEndDate'] = contractEndDate\n",
    "    #if not (contractEndDate):\n",
    "        #Function failed to execute\n",
    "    #elif contractEndDate.year == 1066:\n",
    "        #could not find valid start date\n",
    "    #else:\n",
    "    #    print(\"end: \" , contractEndDate)\n",
    "    \n",
    "    #contractDuration = getContractDuration(file_info['sentTokens'])\n",
    "    #file_attr['contractDuration'] = contractDuration\n",
    "    #if not (contractDuration):\n",
    "        #Function failed to execute\n",
    "    #elif contractDuration == -1:\n",
    "        #Could not calculate valid duration\n",
    "    #else:\n",
    "    #    print(\"duration: \" , contractDuration)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    clientLocation = getClientLocation(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data, fileName)\n",
    "    file_attr['clientLocation'] = clientLocation\n",
    "    #if not (clientLocation):\n",
    "        #Failed to execute\n",
    "    #elif clientLocation == \"not_a_state\":\n",
    "        #Failed to find a valid location\n",
    "    #else:\n",
    "    #    print(\"Client Office in: \" + clientLocation)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    deltaOfficeLocation = getDeltaOffice(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data)\n",
    "    file_attr['deltaOfficeLocation'] = deltaOfficeLocation\n",
    "    #if not (deltaOfficeLocation):\n",
    "        #failed to execute\n",
    "    #elif deltaOfficeLocation == \"not_a_state\":\n",
    "        #Failed to find valid location for DD office\n",
    "    #else:\n",
    "     #   print(\"Delta Office in: \" + str(deltaOfficeLocation))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    d={'key':'filename', 'value':fileName}\n",
    "    dfMD = pd.DataFrame(d, index=['MetaData'])\n",
    "    \n",
    "   ## df=pd.DataFrame({'key':'group_number','value':groupNumber}, index=['MetaData'])\n",
    "   ## dfMD = pd.concat([dfMD, df])\n",
    "    \n",
    "   ## df=pd.DataFrame({'key':'contract_start_date','value':contractStartDate}, index=['MetaData'])\n",
    "   ## dfMD = pd.concat([dfMD, df])\n",
    "    \n",
    "    return file_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Workspace Prep Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Initial setup/fill Raw data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Iterate through folder: if pdf/doc/docx move into new folder/make new folder\n",
    "def setupWorkspace():\n",
    "    cwd = os.getcwd()\n",
    "    #print(\"cwd: \" + cwd)\n",
    "    \n",
    "    rawPath = os.path.join(cwd, \"raw\")\n",
    "    processedPath = os.path.join(cwd, \"processed\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    \n",
    "    try:\n",
    "        if(os.path.isdir(rawPath)):\n",
    "            raise Exception(rawPath + \" already exists.\")\n",
    "        else:\n",
    "            os.makedirs(rawPath)\n",
    "        \n",
    "        if(os.path.isdir(processedPath)):\n",
    "            raise Exception(processedPath + \" already exists.\")\n",
    "        else:\n",
    "            os.makedirs(processedPath)\n",
    "            \n",
    "        if(os.path.isdir(outputPath)):\n",
    "            raise Exception(outputPath + \" already exists.\")\n",
    "        else:\n",
    "            os.makedirs(outputPath)\n",
    "            \n",
    "        if(os.path.isdir(dataPath)):\n",
    "            raise Exception(dataPath + \" already exists.\")\n",
    "        else:\n",
    "            os.makedirs(dataPath)\n",
    "            \n",
    "        \n",
    "        for file in os.listdir(cwd):\n",
    "            if(os.path.isfile(file)):\n",
    "                if not(checkFileType(file) == -1):\n",
    "                    #print(\"moving: \" + file)\n",
    "                    shutil.move(os.path.join(cwd,file), os.path.join(rawPath, file))\n",
    "        \n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "setupWorkspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Move to processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processedFolder():\n",
    "    cwd = os.getcwd()\n",
    "    #print(\"cwd: \" + cwd)\n",
    "    \n",
    "    rawPath = os.path.join(cwd, \"raw\")\n",
    "    processedPath = os.path.join(cwd, \"processed\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    \n",
    "    try:\n",
    "        if(os.path.isdir(rawPath) and os.path.isdir(processedPath)):\n",
    "            #is it empty?\n",
    "            if not os.listdir(rawPath):\n",
    "                raise Exception(\"raw data folder is empty.\")\n",
    "            else:\n",
    "                for file in os.listdir(rawPath):\n",
    "                    #move a copy to processed Path\n",
    "                    try:\n",
    "                        shutil.copy(os.path.join(rawPath, file), os.path.join(processedPath, file))\n",
    "                    #catch copy exception here so it doesn't stop all files (?)\n",
    "                    except Exception as e:\n",
    "                        print(str(e))\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Expected file structure doesn't exist.\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "processedFolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Full run through of batch processing with error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error opening and tokenizing .DS_Store\n",
      "19168 EOC.txt\n",
      "['ca', 'pa', 'washington , d.c.', 'pa', 'wv', 'wv']\n",
      "10041 Appendix C EOCc Meridian Union (Janb2014)-pdf.txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "TX 19015 Attachment A ENT (7.2.18).txt\n",
      "['texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "19223 Attach B (01-01-18).txt\n",
      "[]\n",
      "TX 17404 EOC Regional (7.2.18).txt\n",
      "['texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "01248 (SI) Eff. 10-1-14.txt\n",
      "[]\n",
      "1036 - SI (Eff. 7-1-09).txt\n",
      "[]\n",
      "19168 Contract-pdf.txt\n",
      "['ca', 'washington , d.c.', 'wv', 'wv', 'wv']\n",
      "wv\n",
      "1036 Sched I 7-1-09-pdf.txt\n",
      "[]\n",
      "TX 17404 EOC (Regional (7.2.18)-pdf.txt\n",
      "['ca', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "01094 - SI - Eff 7-1-17 to 6-30-19-pdf.txt\n",
      "[]\n",
      "19223 PPO EBB (01-01-18)-pdf.txt\n",
      "['ca', 'washington , d.c.', 'ca']\n",
      "ca\n",
      "01255 (Schedule I) January 1 2014.txt\n",
      "[]\n",
      "TX-19278 ASC-ENT (7.2.18).txt\n",
      "['tx', 'ga', 'ga', 'ga', 'washington , d.c.', 'ga', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "10041 Appendix D EOCc Hampton Union (Jan2012)-pdf.txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "01248 (FULL CONTRACT) Eff. 10-1-14-pdf.txt\n",
      "['district of columbia', 'washington , dc', 'district of columbia', 'district of columbia', 'washington , dc', 'district of columbia', 'ne', 'ne', 'district of columbia', 'district of columbia', 'district of columbia', 'district of columbia', 'district of columbia', 'washington , dc', 'district of columbia', 'district of columbia', 'washington , dc', 'district of columbia']\n",
      "district of columbia\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - SCHEDULE A - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "1036 - R15 (Eff. 7-1-09).txt\n",
      "[]\n",
      "01255 (Schedule I) January 1 2013.txt\n",
      "[]\n",
      "test2.txt\n",
      "[]\n",
      "19194 Contract (Eff. 1-1-18).txt\n",
      "['pa', 'washington , d.c.', 'pa']\n",
      "pa\n",
      "45-3310 ASC Signed-pdf.txt\n",
      "[]\n",
      "01248 (R7 FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.txt\n",
      "[]\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL EOC - Eff 7-1-18-pdf.txt\n",
      "['ca']\n",
      "ca\n",
      "19223 Attach A (01-01-18).txt\n",
      "[]\n",
      "TX 19015 Contract ENT (7.2.18).txt\n",
      "['ga', 'ga', 'ga', 'ga', 'washington , d.c.', 'tx', 'tx', 'tx', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "19168 Contract.txt\n",
      "['ca', 'washington , d.c.', 'pa', 'wv']\n",
      "test2-pdf.txt\n",
      "[]\n",
      "1036- sch 1 7-1-09-pdf.txt\n",
      "[]\n",
      "01094 EOC  7-1-16.txt\n",
      "[]\n",
      "10041 Appendix E EOC Daingerfield Union (Jan2018).txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "1255 master 2 1 07.txt\n",
      "['district of columbia']\n",
      "district of columbia\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - SCHEDULE A - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "01094 (Schedule I) July 1 2013.txt\n",
      "[]\n",
      "01248 (R8 FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.txt\n",
      "[]\n",
      "01094 _SBC Modification_-pdf.txt\n",
      "[]\n",
      "01255 - EOC (January 1 2011).txt\n",
      "[]\n",
      "19223 PPO Contract (01-01-18).txt\n",
      "[]\n",
      "TX 17404 Contract Regional (7.2.18)-pdf.txt\n",
      "['ca', 'ca', 'tx', 'tx', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "01255 (Schedule I and Tax Modification) January 1  2013-pdf.txt\n",
      "[]\n",
      "01036 (EOC) 7-1-14.txt\n",
      "[]\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - CONTRACT - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "10041 Appendix B EOC Non-Union (Jan2018)-pdf.txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "3310 EOC(Jan2013)-pdf.txt\n",
      "[]\n",
      "3310 EOC(Jan2013).txt\n",
      "[]\n",
      "01094 EOC  7-1-16-pdf.txt\n",
      "[]\n",
      "01094 (Tax Modfication).txt\n",
      "[]\n",
      "01094 (SBC Modification).txt\n",
      "[]\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - FULL EOC - Eff 7-1-18-pdf.txt\n",
      "['ca']\n",
      "ca\n",
      "1036 - CONTRACT (Eff. 7-1-09).txt\n",
      "[]\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - R46 - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - EOC - Eff 7-1-18.txt\n",
      "[]\n",
      "10041 Appendix F EOC Ardmore Union (Jan2012).txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - EOC - Eff 7-1-18.txt\n",
      "[]\n",
      "TX 19015 Attachment  C ENT (7.2.18).txt\n",
      "['ca', 'ca', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - PREM AGMT - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - R46 - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "TX 19015 Contract ENT (7.2.18)-pdf.txt\n",
      "['ga', 'ga', 'washington , d.c.', 'tx', 'tx', 'tx', 'ca', 'ca', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "19223 PPO EBB (01-01-18).txt\n",
      "['ca', 'ca', 'washington , d.c.', 'ca']\n",
      "ca\n",
      "1036- contract 7-1-09-pdf.txt\n",
      "[]\n",
      "18635 ASC Contract-pdf.txt\n",
      "['pa', 'washington , d.c.', 'pa']\n",
      "pa\n",
      "1094 Full Contract (Eff. 7-1-07)-pdf.txt\n",
      "[]\n",
      "25-1149 ASC Agreement.txt\n",
      "['georgia', 'ga', 'ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "1036 (Schedule I) July 1 2009.txt\n",
      "[]\n",
      "TX-18745-ASC Contract (7.2.18).txt\n",
      "['ga', 'ga', 'ga', 'washington , d.c.', 'ga', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "01094 - SI - Eff 7-1-17 to 6-30-19.txt\n",
      "[]\n",
      "TX 17404 Contract Regional (7.2.18).txt\n",
      "['ca', 'ca', 'tx', 'tx', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "01255 _Schedule I_ January 1  2014-pdf.txt\n",
      "[]\n",
      "01094 (SI) Eff. 7-1-15.txt\n",
      "[]\n",
      "3468 EOC (RevSept2016)-pdf.txt\n",
      "[]\n",
      "10294-01002 & 02001 PPO EOC (07-01-12)-pdf.txt\n",
      "['ca', 'ca', 'ca', 'washington , d.c.', 'ca']\n",
      "ca\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - R60 - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "18635 ASC Contract.txt\n",
      "['pa', 'washington , d.c.', 'pa']\n",
      "pa\n",
      "01255 (Tax Modification).txt\n",
      "[]\n",
      "01248 - SII (Eff. 10-1-14).txt\n",
      "[]\n",
      "19168 Attachment B Low.txt\n",
      "[]\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - PREM AGMT - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - R60 - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n",
      "01248 (EOC FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.txt\n",
      "[]\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19-pdf.txt\n",
      "[]\n",
      "test.txt\n",
      "[]\n",
      "3310 ASC.txt\n",
      "[]\n",
      "TX-18745 ASC Contract (7.2.18)-pdf.txt\n",
      "['ga', 'tx', 'tx', 'tx', 'tx', 'tx', 'tx', 'tx', 'tx', 'ga', 'ga', 'ga', 'washington , d.c.', 'ga', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "tx\n",
      "19194 Contract Eff. 1-1-18-pdf.txt\n",
      "['pa', 'washington , d.c.', 'pa']\n",
      "pa\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19-pdf.txt\n",
      "[]\n",
      "10041 Appendix E EOC Daingerfield Union (Jan2018)-pdf.txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "19168 Attachment C.txt\n",
      "[]\n",
      "19168 Attachment A.txt\n",
      "[]\n",
      "TX 19015 Attachment B ENT (7.2.18).txt\n",
      "['texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "3468 EOC (RevSept2016).txt\n",
      "[]\n",
      "TX 19015 EOC ENT (7.2.18).txt\n",
      "['ga', 'ga', 'ga', 'ga', 'ga', 'georgia', 'ga', 'washington , d.c.', 'georgia', 'tx', 'tx', 'tx', 'texas', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "01094 (Schedule I and Tax Modification) July 1  2013-pdf.txt\n",
      "[]\n",
      "1036 - SII (Eff. 7-1-09).txt\n",
      "[]\n",
      "01036 (EOC) 7-1-14-pdf.txt\n",
      "['pa']\n",
      "pa\n",
      "01248 (CONTRACT) Eff. 10-1-14.txt\n",
      "['district of columbia', 'district of columbia', 'washington , dc', 'district of columbia', 'washington , dc', 'district of columbia', 'ne', 'ne', 'district of columbia', 'district of columbia', 'district of columbia', 'district of columbia', 'district of columbia', 'district of columbia', 'washington , dc', 'district of columbia', 'washington , dc', 'district of columbia']\n",
      "district of columbia\n",
      "01248 (FULL EOC FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14-pdf.txt\n",
      "['washington , dc', 'district of columbia', 'district of columbia']\n",
      "district of columbia\n",
      "10041 Appendix G EOC Franklin Union (Jan2012).txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "19223 PPO Contract (01-01-18)-pdf.txt\n",
      "[]\n",
      "test-pdf.txt\n",
      "[]\n",
      "10041 Appendix C EOC Meridian Union (Jan2014).txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "TX-19278 ASC-ENT (7.2.18)-pdf.txt\n",
      "['ga', 'tx', 'ga', 'ga', 'ga', 'washington , d.c.', 'ga', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "19168 Attachment B High.txt\n",
      "[]\n",
      "25-1149 ASC Agreement Signed-pdf.txt\n",
      "[]\n",
      "TX 19015 EOC ENT(7.2.18)-pdf.txt\n",
      "['ga', 'ga', 'ga', 'georgia', 'ga', 'washington , d.c.', 'georgia', 'tx', 'tx', 'tx', 'ca', 'texas', 'texas', 'texas', 'texas', 'texas']\n",
      "texas\n",
      "3468 ASC.txt\n",
      "[]\n",
      "10041 Appendix G EOC Franklin Union (Jan2012)-pdf.txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "1036 - R29 (Eff. 7-1-09).txt\n",
      "[]\n",
      "10041 Appendix D EOC Hampton Union (Janb2012).txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "01255 (Full Contract) Eff. 2-1-07-pdf.txt\n",
      "[]\n",
      "10041 Appendix F EOC Ardmore Union (Jan2012)-pdf.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "19168 EOC-pdf.txt\n",
      "['ca', 'washington , d.c.', 'wv', 'wv', 'ca']\n",
      "1036 - R30 (Eff. 7-1-09).txt\n",
      "[]\n",
      "10041 Appendix B EOC Non-Union (Jan2018).txt\n",
      "['ms', 'ms', 'ms', 'ms', 'ms', 'ms', 'washington , d.c.']\n",
      "ms\n",
      "10294-01002 & 02001 PPO EOC (07-01-12).txt\n",
      "['ca', 'ca', 'ca', 'washington , d.c.', 'ca']\n",
      "ca\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - CONTRACT - Eff 7-1-18 to 6-30-19.txt\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "base_info = None\n",
    "pp = batchPreProcess(errorFile)\n",
    "if pp == None:\n",
    "    print(\"Error in pp\")\n",
    "else:\n",
    "    base_info = batchGetTokens(errorFile)\n",
    "    if not base_info:\n",
    "        print(\"Error in getting tokens\")\n",
    "    else:\n",
    "        outputFilePath = os.path.join(os.getcwd(),'data','raw_attribute_data.csv')\n",
    "        outputFile=open(outputFilePath, 'w')    \n",
    "        first_row = 1\n",
    "\n",
    "        for file in base_info:\n",
    "            file_attr = getMetaDataAtt(file)\n",
    "            if first_row:\n",
    "                for key in file_attr:\n",
    "                    outputFile.write(key + \",\")\n",
    "                outputFile.write(\"\\n\")\n",
    "                first_row = 0\n",
    "            else:\n",
    "                for attr in file_attr:\n",
    "                    #print(attr + \": \" + file_attr[attr])\n",
    "                    outputFile.write(str(file_attr[attr])+ \",\")\n",
    "                outputFile.write(\"\\n\")\n",
    "\n",
    "        outputFile.close()\n",
    "errorFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Not done attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Group Information\n",
    "\n",
    "dfGI=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Diagnostic and Preventative (D&P) [Appendix A]\n",
    "\n",
    "d={'key':'D&P Services_PPO','value':'100'}\n",
    "##Need to pass index since we're only doing string values\n",
    "dfDP=pd.DataFrame(d, index=['D%P Services'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Basic Service\n",
    "\n",
    "dfBS=pd.DataFrame(d, index=['Basic Service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Endo Perio (Endodontics (Periodontal(?)))\n",
    "\n",
    "dfEP=pd.DataFrame(d, index=['Endo Perio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Oral (Oral Surgery)\n",
    "\n",
    "dfOa=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Perio (Periodontal)\n",
    "\n",
    "dfPe=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Major (Major Benefits)\n",
    "\n",
    "dfMj=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Prostho (Prosthodontics)\n",
    "\n",
    "dfPr=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Ortho (Orthodontics)\n",
    "\n",
    "dfOt=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Concatenate all frames created by the above dataset\n",
    "\n",
    "frames = [df, dfGI, dfDP, dfBS, dfEP, dfOa, dfPe, dfMj, dfPr, dfOt]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "transpose=result.transpose()\n",
    "print(transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Contract Start Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With contract start date I began by searching through the tokenized sentences with a regex expression. \n",
    "I found a datefinder module to use on each flagged sentence to pull out the dates\n",
    "\tIssue: the datefinder module works poorly on large, run -on sentences which are common in the contracts. It tends to find other numbers that aren't dates and try to make a date out of them.\n",
    "\t\n",
    "\tSol: only take a subset, starting at the flagged word\n",
    "\t\n",
    "Sometimes a match isn't found with the keywords I've seen related to the start date\n",
    "\tSol: look for keywords related to contract term and take the earlier date from that sentence\n",
    "\t\n",
    "Issue: Datefinder focusing on numbers that aren't dates\n",
    "\tSol: filter for sentences that have a year (ie four digits in a row) and dates that are before Delta Dental existed (in 1966)\n",
    "\t\n",
    "Issue: Sometimes there are multiple modes. Usually I saw this when there were equal mentions of the end date\n",
    "\tSol: if there are multiple modes, take the earliest date. \n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Testing finding the contract end date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This was much the same as the contract start date Issues\n",
    "\n",
    "\tLooking for keywords Contract Term/Contract End\n",
    "\tFiltering on invalid years\n",
    "\tFiltering on if there IS a year in the sentence (assuming it won't be written out like nineteen ninety-four)\n",
    "\tIf there are only two results, take the later one\n",
    "\tIf there are more than two, take the top two most common and then take the latter of the two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Contract Duration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Call contract start and end and try to get a duration out of them\n",
    "\n",
    " ^^^ pretty much worked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comments so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "\tEven with trying to be variable, this won't work if they even change the wording a bit. Maybe spend some time looking into using the libraries to get synonyms.\n",
    "\t\n",
    "\tIt would also be great to get the other contracts to see exactly where we're going wrong\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Working with synonyms in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\tGetting an expanded set of search terms can be done, but I can't yet figure out how to pick the right contexts. For example, the search bigram \"contract term\" gives back a huge amount of synonyms, with only 3 or 4 actually being equivalent in meaning to \"contract term\"\n",
    "\t\n",
    "\tWe could always manually select ones that are similar but that seems to defeat the purpose: ie we could select only the noun meanings of contract\n",
    "\t\n",
    "\tWe could build our own corpus of words based on all of the documents that we have, and then compare the given synonyms to a freq distribution of those words to pick out the ways other contracts might say the same thing\n",
    "\t\tBut this would still miss things for sure\n",
    "\t\t\n",
    "\tWe could include the word type with the seed words and only choose synonyms of the same type, although this does involve more hardcoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Folder Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "data/raw\n",
    "\n",
    "data/processed/[group number - group name]/\n",
    "\n",
    "data/output\n",
    "\n",
    "*** Not every file seems to have a name, so we would have to parse the file to get it\n",
    "\n",
    "*** Additionally each num - name combo may have contracts from multiple dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using NER tagging to identify location sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### NLTK NER tagging\n",
    "Basic NER tagging with nltk works horribly on our files out of the box\n",
    "\n",
    "#### Polyglot\n",
    "Polyglot has a lot of issues getting downloaded\n",
    "\n",
    "#### NLTK wrapper for Stanford NER\n",
    "NLTK has a wrapper for the Stanford NER tagger so I'm going to try that next\n",
    "\tDownload the model jar file\n",
    "\t\n",
    "\t\n",
    "The stanford NER tagger is working a bit better\n",
    "http://www.nltk.org/api/nltk.tag.html#nltk.tag.stanford.StanfordTagger\n",
    "https://nlp.stanford.edu/software/stanford-ner-2018-02-27.zip (download of jar files)\n",
    "https://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages\n",
    "\n",
    "The stanford one takes FOREVER though\n",
    "\tThere is a faster version in CoreNLP but that's all in Java and I don't think the wrapper interacts with it\n",
    "\n",
    "#### GeoText\n",
    "GeoText\n",
    "\n",
    "Easy to set up and use, but doesn't do states or state abbreviations\n",
    "And it misses a LOT that the NER tagger got\n",
    "\tIt is completely unreliable honestly. \n",
    "\n",
    "#### Options\n",
    "Option 1: Use the NLTK wrapper for Stanford NER tagger and just wait forever\n",
    "Option 2: Get a giant csv of all US cities/states/abbreviations/counties (exists) and make a data set out of that to compare to\n",
    "\tCons: not flexible or extensible, is already 4 years out of date\n",
    "\tPros: much faster, will only have to create the thing once\n",
    "\t\n",
    "Note: even with the NER it will only give us pieces of the address, we would still have to go into the sentence and try to regex it out\n",
    "\n",
    "Option 3: create our own trained model from some of the files we already have and see how that does with the Stanford tagger. Might be faster\n",
    "\tCould also see how it does with the native NLTK tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Location Function Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Trying to Regex out a full address proves difficult\n",
    "\n",
    "You can kind of get it down to the right sentence by looking for ones that contain 'contractholder' and avoiding ones that contain 'deltadental'\n",
    "\n",
    "But it's still not perfect\n",
    "\n",
    "Tabling getting the entire address for now, I'm looking at getting the contractholder state and city\n",
    "\n",
    "Getting the state so far works okay, but there are some strange cities out there that cause issues. IE DPO is apparently a city in the US, as is Premium. These words show up enough in other locations that they interfere with trying to find the most popular ACTUAL city mentioned. Even filtering on the above keywords comes back with Premium as the city.\n",
    "\n",
    "Honestly, without a bit of work the city is completely unreliable\n",
    "\n",
    "Running into issues with filtering by keyword because the keyword is often cut off from parts of the sentence containing the location information by punctuation within the address itself\n",
    "\n",
    "\n",
    "Issues with a lot of false positives. I think it would be easier to find the location of the Delta office handling it instead of the client address, which doesn't seem to be clearly marked anywhere\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Docx vs Doc issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Right now we are using a docx specific library\n",
    "\n",
    "#### They gave us a bunch of doc files, docx2text can't handle those\n",
    "\n",
    "#### Catdoc software and the python subprocess module\n",
    "       Catdoc does NOT pay attention to formatting, so that could get messy. it simply looks for readable text and extracts it in the order it finds it\n",
    "       Catdoc works natively on Mac but not windows\n",
    "       https://blog.brush.co.nz/2009/09/catdoc-windows/ is a pckg for windows...but just from some random guy\n",
    "       \n",
    "\n",
    "#### Antiword\n",
    "        Linux specific, you can get packages for both Mac and Windows...the windows one looks especially iffy\n",
    "        Also just the fact that we would need to have a separate setup is not very desirable. \n",
    "        \n",
    "#### Textutil\n",
    "        Can be used on Mac pretty easily with python subprocess\n",
    "        \n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Mac vs Windows Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Running on windows\n",
    "\n",
    "#### Before going through any of this, ensure your windows parallels setup is done from the OneNote directions!\n",
    "#### i.e., make sure Anaconda is installed :| \n",
    "\n",
    "        when importing modules using pip install\n",
    "                Instead of \"pip install pdfminer\" use \"pip install pdfminer.six\"\n",
    "        when installing datefinder\n",
    "                It won't work and will tell you that you need Visual Studio 2015\n",
    "                Instead, download the src code from https://github.com/akoumjian/datefinder\n",
    "                Open file: setup.py and look for line \n",
    "                    install_requires=['regex==2016.01.10', 'python-dateutil>=2.4.2', 'pytz'],\n",
    "                and change the first == to >=\n",
    "                    install_requires=['regex>=2016.01.10', 'python-dateutil>=2.4.2', 'pytz'],\n",
    "                save and then run \"pip install './pathToDatefinderSrc'\n",
    "                \n",
    "                see: https://stackoverflow.com/questions/44016287/error-in-pip-install-datefinder?noredirect=1&lq=1\n",
    "              May also need to run pip install --upgrade setuptools\n",
    "              \n",
    "         Some things don't get installed automatically:\n",
    "                 in python window run : import nltk \n",
    "                                        nltk.download('punkt')\n",
    "                                        nltk.download('stopwords')\n",
    "                                        nltk.download('averaged_perceptron_tagger')\n",
    "                                        nltk.download('wordnet')\n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
