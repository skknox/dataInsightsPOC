{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import random as rng\n",
    "import nltk\n",
    "import fnmatch\n",
    "\n",
    "import docx2txt\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "from nltk.book import *\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "import sys as sys\n",
    "from sys import platform\n",
    "import re as re\n",
    "from statistics import mode\n",
    "import datefinder\n",
    "from nltk import ne_chunk\n",
    "from geotext import GeoText\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import csv\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Psuedocode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Analysis Function\n",
    "\n",
    "## Function which calculates the number of combinations (Not Permutations) of distinct values in a dataset. Would \n",
    "## take in a Pandas DataFrame ideally and multiply all distinct value counts for all available columns.\n",
    "\n",
    "# Expected input: Pandas DataFrame\n",
    "# Expected output: Int, the number of possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function to collect mentions of States/Districts/Territories in the United States (DC and Puerto Rico are included)\n",
    "## and rank their relevence, frequency should be a good factor. Note: California, Pennsylvania, and Georgia could \n",
    "## appear multiple times from addresses.\n",
    "\n",
    "# Expected input: A single file, text.\n",
    "# Expected output: A list of locations mentioned, ranked by most used to least used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function for searching a tokenized sentence dictionary for words or phrases. There's probably \n",
    "## already something for this.\n",
    "\n",
    "# Expected input: A tokenized list, and a phrase to search for.\n",
    "# Expected output: A list of sentences containing the phrase searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function to take a file and produce the Acronyms from them.\n",
    "\n",
    "# Expected Input: Text File to be parsed, the Acronym to search\n",
    "# Expected output: A list of possible phrases, most likely first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Parsing Function\n",
    "\n",
    "## Function to take Acronyms and match them to the best possible N-gram for them from a document. Not all will be \n",
    "## possible, but a list of options will help.\n",
    "\n",
    "# Expected Input: Text File to be parsed, the Acronym to search\n",
    "# Expected output: A list of possible phrases, sorted most likely first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Generally important\n",
    "\n",
    "## Need to expand stopwords to include States and Districts, also Delta Dental adjacent names as another available set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Function to load in two different text extracted contracts and return a comparison metric between \n",
    "## the two (Similarity, possibly as a percent?)\n",
    "\n",
    "# Expected Input: Two contracts for Comparison\n",
    "# Expected Output: A measure, some kind of decimal to represent similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## File Prep Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Is it docx or pdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def checkFileType(filename):\n",
    "        if(filename.lower().endswith(('.doc','.docx'))):\n",
    "            return 0\n",
    "        elif(filename.lower().endswith(('.pdf'))):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def makeFilePath(docName):\n",
    "    raw_data_path = os.path.join(os.getcwd(), 'data', 'raw')\n",
    "    return os.path.join(raw_data_path, docName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    text = text.replace(\"\\t\", ' ')\n",
    "    #text = text.replace(\",\", ' ')\n",
    "    \n",
    "    dblSpacesRemaining = True\n",
    "    while(dblSpacesRemaining):\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        if not \"  \" in text:\n",
    "            dblSpacesRemaining = False\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: process dataFrame and group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDF(txtFile):\n",
    "    df = pd.read_csv(txtFile, sep=\" \", header=None) #this doesn't work for me bc of variable number of cols\n",
    "    df = df.T \n",
    "    df = df.dropna()\n",
    "\n",
    "    df['SingleRow']=1\n",
    "\n",
    "    df=df.rename(columns={0 : 'Words'})\n",
    "    print(\"in processDF \" + txtFile)\n",
    "    df.describe(include=\"all\")\n",
    "    #print(df.groupby('Words').SingleRow.sum().sort_values())\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Process a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDocFile(filePath):\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processTextFile(filePath):\n",
    "    #print(filePath)\n",
    "    try:\n",
    "        docxText = docx2txt.process(filePath)\n",
    "        #print(docxText)\n",
    "        replacedText = cleanText(docxText)\n",
    "        #print(replacedText)\n",
    "        fileName = filePath.split('/')[-1]\n",
    "        fileName = fileName.replace(\",\",\"\")\n",
    "        #print(fileName)\n",
    "        baseFileName = fileName[0:-5]\n",
    "        #print(baseFileName)\n",
    "        newFilePath = './data/output/' + baseFileName + \".txt\"\n",
    "        #print(newFilePath)\n",
    "        singleFileDocx=open(newFilePath, 'wb+')    \n",
    "        singleFileDocx.write(replacedText.encode(\"utf-8\"))\n",
    "        singleFileDocx.close()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "    #temp_df = processDF('singleTextDocx.txt')\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Process pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processPDFfile(filePath):\n",
    "    password = \"\"\n",
    "    extracted_text = \"\"\n",
    "    try:\n",
    "        fileName = filePath.split('/')[-1]\n",
    "        fileName = fileName.replace(\",\", \" \")\n",
    "        baseFileName = fileName[0:-4]\n",
    "    \n",
    "        fp = open(filePath, \"rb\")\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser, password)\n",
    "        \n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "            \n",
    "        # Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "\n",
    "        # set parameters for analysis\n",
    "        laparams = LAParams()\n",
    "\n",
    "        # Create a PDFDevice object which translates interpreted information into desired format\n",
    "        # Device needs to be connected to resource manager to store shared resources\n",
    "        # device = PDFDevice(rsrcmgr)\n",
    "        # Extract the decive to page aggregator to get LT object elements\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "        # Create interpreter object to process page content from PDFDocument\n",
    "        # Interpreter needs to be connected to resource manager for shared resources and device \n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            # As the interpreter processes the page stored in PDFDocument object\n",
    "            interpreter.process_page(page)\n",
    "            # The device renders the layout from interpreter\n",
    "            layout = device.get_result()\n",
    "            # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "            for lt_obj in layout:\n",
    "                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    newText = lt_obj.get_text()\n",
    "                    newText = newText.replace('\\n', ' ')\n",
    "                    extracted_text += newText\n",
    "\n",
    "        #close the pdf file\n",
    "        fp.close()\n",
    "        \n",
    "        extracted_text = cleanText(extracted_text)#extracted_text.replace(\"\\n\", ' ')\n",
    "        \n",
    "        newFilePath = './data/output/' + baseFileName + '-pdf' + \".txt\"\n",
    "        with open(newFilePath, 'wb+') as singleFilePDF:\n",
    "            singleFilePDF.write(extracted_text.encode(\"utf-8\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "        #temp_df = processDF('./data/output/' + baseFileName + \".txt\")\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Docx Extraction -- Currently not using this: see File Prep Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "DocX extractor for data purposes. Requires customization to each purpose but is important for pulling data out of DocX files. Does not understand tables or bullet points, however is visually consistent with what's on the page.\n",
    "\n",
    "Strongest values: Order, consistentcy, noise reduction\n",
    "\n",
    "Weakest values: Completeness, flexibility, whitespace characters, formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Docx Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "\n",
    "def cleanText(text):\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    text = text.replace(\"\\t\", ' ')\n",
    "    text = text.replace(\",\", ' ')\n",
    "    \n",
    "    dblSpacesRemaining = True\n",
    "    while(dblSpacesRemaining):\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        if not \"  \" in text:\n",
    "            dblSpacesRemaining = False\n",
    "        \n",
    "    return text\n",
    "fileName = \"TX 17404 Contract Regional (7.2.18).docx\"\n",
    "baseFileName = \"\"\n",
    "if(fileName.lower().endswith(('.docx'))):\n",
    "    baseFileName = fileName[0:-5]\n",
    "    #print(baseFileName)\n",
    "elif(fileName.lower().endswith(('.pdf'))):\n",
    "    baseFileName = fileName[0:-4]\n",
    "else:\n",
    "    print(\"ending error\")\n",
    "\n",
    "docText = docx2txt.process(\"./data/raw/\" + fileName)\n",
    "singleFileDocx=open('./data/output/' + baseFileName + \".txt\", 'wb+')\n",
    "replacedText = cleanText(docText)\n",
    "#print(replacedText)\n",
    "singleFileDocx.write(docText.encode(\"utf-8\"))\n",
    "singleFileDocx.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PDF Extraction -- Currently not using this: see File Prep Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### PDF practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C://data/test.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d313aef8c3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Open and read the pdf file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Create parser object to parse the pdf content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C://data/test.pdf'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "# pdfTextMiner.py\n",
    "# Python 2.7.6\n",
    "# For Python 3.x use pdfminer3k module\n",
    "# This link has useful information on components of the program\n",
    "# https://euske.github.io/pdfminer/programming.html\n",
    "# http://denis.papathanasiou.org/posts/2010.08.04.post.html\n",
    "\n",
    "\n",
    "''' Important classes to remember\n",
    "PDFParser - fetches data from pdf file\n",
    "PDFDocument - stores data parsed by PDFParser\n",
    "PDFPageInterpreter - processes page contents from PDFDocument\n",
    "PDFDevice - translates processed information from PDFPageInterpreter to whatever you need\n",
    "PDFResourceManager - Stores shared resources such as fonts or images used by both PDFPageInterpreter and PDFDevice\n",
    "LAParams - A layout analyzer returns a LTPage object for each page in the PDF document\n",
    "PDFPageAggregator - Extract the decive to page aggregator to get LT object elements\n",
    "'''\n",
    "\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "# From PDFInterpreter import both PDFResourceManager and PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "# Import this to raise exception whenever text extraction from PDF is not allowed\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "''' This is what we are trying to do:\n",
    "1) Transfer information from PDF file to PDF document object. This is done using parser\n",
    "2) Open the PDF file\n",
    "3) Parse the file using PDFParser object\n",
    "4) Assign the parsed content to PDFDocument object\n",
    "5) Now the information in this PDFDocumet object has to be processed. For this we need\n",
    "   PDFPageInterpreter, PDFDevice and PDFResourceManager\n",
    " 6) Finally process the file page by page \n",
    "'''\n",
    "\n",
    "base_path = \"C://data\"\n",
    "\n",
    "my_file = os.path.join(base_path + \"/\" + \"test.pdf\")\n",
    "log_file = os.path.join(base_path + \"/\" + \"pdf_log.txt\")\n",
    "\n",
    "password = \"\"\n",
    "extracted_text = \"\"\n",
    "\n",
    "# Open and read the pdf file in binary mode\n",
    "fp = open(my_file, \"rb\")\n",
    "\n",
    "# Create parser object to parse the pdf content\n",
    "parser = PDFParser(fp)\n",
    "\n",
    "# Store the parsed content in PDFDocument object\n",
    "document = PDFDocument(parser, password)\n",
    "\n",
    "# Check if document is extractable, if not abort\n",
    "if not document.is_extractable:\n",
    "    raise PDFTextExtractionNotAllowed\n",
    "    \n",
    "# Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "rsrcmgr = PDFResourceManager()\n",
    "\n",
    "# set parameters for analysis\n",
    "laparams = LAParams()\n",
    "\n",
    "# Create a PDFDevice object which translates interpreted information into desired format\n",
    "# Device needs to be connected to resource manager to store shared resources\n",
    "# device = PDFDevice(rsrcmgr)\n",
    "# Extract the decive to page aggregator to get LT object elements\n",
    "device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "# Create interpreter object to process page content from PDFDocument\n",
    "# Interpreter needs to be connected to resource manager for shared resources and device \n",
    "interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "# Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "for page in PDFPage.create_pages(document):\n",
    "    # As the interpreter processes the page stored in PDFDocument object\n",
    "    interpreter.process_page(page)\n",
    "    # The device renders the layout from interpreter\n",
    "    layout = device.get_result()\n",
    "    # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "    for lt_obj in layout:\n",
    "        if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "            extracted_text += lt_obj.get_text()\n",
    "            \n",
    "#close the pdf file\n",
    "fp.close()\n",
    "\n",
    "# print (extracted_text.encode(\"utf-8\"))\n",
    "            \n",
    "with open(log_file, \"wb\") as my_log:\n",
    "    my_log.write(extracted_text.encode(\"utf-8\"))\n",
    "print(\"Done !!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NLTK Tokenizing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: DD specific text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ddCleanText(text):\n",
    "    newText = text.replace('Delta Dental', 'DeltaDental')\n",
    "    newText = newText.replace('DELTA DENTAL', 'DELTADENTAL')\n",
    "    newText = newText.replace('DeltaDental Insurance Company', 'DeltaDentalInsuranceCompany')\n",
    "    return newText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    customStopWords = set(stopwords.words('english')+list(punctuation))\n",
    "    wordsWOStop=[word for word in words if word not in customStopWords]\n",
    "    \n",
    "    return wordsWOStop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized Sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSents(text):\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBigrams(tokens):\n",
    "    bigram_measures=nltk.collocations.BigramAssocMeasures();\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    sorted_bgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "    \n",
    "    return sorted_bgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTrigrams(tokens):\n",
    "    trigram_measures =nltk.collocations.TrigramAssocMeasures();\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    sorted_tgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "\n",
    "    return sorted_tgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MetaData and Attribute Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Filename\n",
    "Contract Start\n",
    "Contract End\n",
    "Contract Duration\n",
    "State\n",
    "Delta Office Involved\n",
    "\n",
    "#### Group Information\n",
    "(Group Number)\n",
    "\n",
    "#### Numeric attributes Only\n",
    "Basics\n",
    "Diagnostics\n",
    "Major\n",
    "Endo\n",
    "Oral\n",
    "Perio\n",
    "Prostho\n",
    "Ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##Establish a dataframe to capture the attributes\n",
    "#d = {'key': 'file','value':fileName}\n",
    "#{'key':'state', 'value':state}\n",
    "#df = pd.DataFrame(d, index=['uid'])\n",
    "d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### NLTK synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### I kept this to processing single words, bigrams or trigrams so as to keep the complexity down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: translate from syn POS to nltk POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Use POS to cull wordnet synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonyms_usingPOS(word_tuple):\n",
    "    #print(word_tuple)\n",
    "    word_tagged = word_tuple[0]\n",
    "    word_pos = get_wordnet_pos(word_tuple[1])\n",
    "    syns = wn.synsets(word_tagged, pos=word_pos)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    #print(syns)\n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get synonyms of a single word. Helper function to Bigram and Trigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## You can't cull this one down with the POS b/c you can't tag a single word\n",
    "def getSyns(word):\n",
    "    syns1 = wn.synsets(word)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns1:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    \n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get a similar bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarBigrams(word1, word2):\n",
    "    #print([word1, word2])\n",
    "    #print(word_tokenize(\" \".join([word1,word2])))\n",
    "    \n",
    "    tagged_words = nltk.pos_tag([word1,word2])\n",
    "    #print(tagged_words)\n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    #print(set1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    #print(set2)\n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            word_set.add(\" \".join([word1, word2]))\n",
    "    #print(word_set)\n",
    "    \n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get a similar trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarTrigrams(word1, word2, word3):\n",
    "    tagged_words = nltk.pos_tag([word1,word2,word3])\n",
    "    \n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    set3 = getSynonyms_usingPOS(tagged_words[2])\n",
    "    if not len(set3):\n",
    "        set3.add(word3)\n",
    "    \n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            for word3 in set3:\n",
    "                word_set.add(\" \".join([word1, word2, word3]))\n",
    "    #print(word_set)\n",
    "    \n",
    "    return word_set\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get synonyms from a list of key words. Returns more keywords/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonymsFromList(keywords):\n",
    "    matches = []\n",
    "\n",
    "    for kw in keywords:\n",
    "        try:\n",
    "            words = word_tokenize(kw)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        #print(words)\n",
    "        if(len(words) == 1):\n",
    "            #print(\"is 1\")\n",
    "            syns = getSyns(words[0])\n",
    "            for syn in syns:\n",
    "                matches.append(syn)\n",
    "            #keywords.append(list(syns))\n",
    "        elif(len(words) == 2):\n",
    "            #print(\"is 2\")\n",
    "            syns = getSimilarBigrams(words[0],words[1])\n",
    "            #print(syns)\n",
    "            matches.extend(getSimilarBigrams(words[0],words[1]))\n",
    "        elif(len(words) == 3):\n",
    "            #print(\"is 3\")\n",
    "            matches.extend(getSimilarTrigrams(words[0],words[1],words[2]))\n",
    "        else:\n",
    "            print(\"keyword string too long\")\n",
    "        #print(matches)\n",
    "    keywords.extend(matches)\n",
    "    keywords = set(keywords)\n",
    "\n",
    "    #print(start_keywords)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### POS tagging key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ADJ\tadjective\tnew, good, high, special, big, local\n",
    "ADV\tadverb\treally, already, still, early, now\n",
    "CNJ\tconjunction\tand, or, but, if, while, although\n",
    "DET\tdeterminer\tthe, a, some, most, every, no\n",
    "EX\texistential\tthere, there's\n",
    "FW\tforeign word\tdolce, ersatz, esprit, quo, maitre\n",
    "MOD\tmodal verb\twill, can, would, may, must, should\n",
    "N\tnoun\tyear, home, costs, time, education\n",
    "NP\tproper noun\tAlison, Africa, April, Washington\n",
    "NUM\tnumber\ttwenty-four, fourth, 1991, 14:24\n",
    "PRO\tpronoun\the, their, her, its, my, I, us\n",
    "P\tpreposition\ton, of, at, with, by, into, under\n",
    "TO\tthe word to\tto\n",
    "UH\tinterjection\tah, bang, ha, whee, hmpf, oops\n",
    "V\tverb\tis, has, get, do, make, see, run\n",
    "VD\tpast tense\tsaid, took, told, made, asked\n",
    "VG\tpresent participle\tmaking, going, playing, working\n",
    "VN\tpast participle\tgiven, taken, begun, sung\n",
    "WH\twh determiner\twho,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Attempt with using POS tagging in the synonyms to reduce extraneous syns\n",
    "We would then have to manually tag all of the original phrases and words we use to seed the decisions\n",
    "It does seem to reduce though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#start_keywords = [\"effective date\\S\\s*\\S\", \"effective\"]\n",
    "#print(getSynonymsFromList(start_keywords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### get file name\n",
    "Get the filename from a full path. Determines the OS and splits the string correctly based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFileName(fullPath):\n",
    "    try:\n",
    "        if(platform == \"win32\"):\n",
    "            fileName = fullPath.split(\"\\\\\")[-1]\n",
    "        else:\n",
    "            fileName = fullPath.split(\"/\")[-1]\n",
    "        return fileName\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filePath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-a3347399fe6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetFileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'filePath' is not defined"
     ]
    }
   ],
   "source": [
    "print(getFileName(filePath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### get group number\n",
    "Uses regex's made from keywords to attempt to find a group number in the file. Failing that, it searches the filename for the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getGroupNumber(sents_tokens, filePath):\n",
    "    \n",
    "    group_keywords = [\"group number\", \"groupnumber\"]\n",
    "    regex_exps = []\n",
    "    poss_nums = []\n",
    "    finalGN = None\n",
    "    \n",
    "    try:\n",
    "        #Create regex exps out of group number keywords\n",
    "        for kw in group_keywords:\n",
    "                temp_re = kw + \"\\W\\s*(?P<gn>\\d+)\"\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "        #For each sentence, search for the expression, if found add the number to\n",
    "        #list of possible group numbers\n",
    "        for sent in sents_tokens:\n",
    "            #print(sent)\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    temp_gn = result.group('gn')\n",
    "                    poss_nums.append(temp_gn)\n",
    "\n",
    "        #Try and get the number from the file name, looking for list of numeric chars\n",
    "        num_regex = re.compile(\"\\d+\")\n",
    "        fileName = getFileName(filePath)\n",
    "        fileGN = num_regex.search(fileName)\n",
    "\n",
    "        if not fileGN==None:#if they filename has a number sequence\n",
    "            if fileGN.group() in poss_nums:#then if the file group number matches one in the document, choose it\n",
    "                finalGN = fileGN.group()\n",
    "            else:\n",
    "                poss_nums.append(fileGN.group())#otherwise add the filename one to the list and try to get the most co\n",
    "                try:\n",
    "                    finalGN = mode(poss_nums)\n",
    "                except:\n",
    "                    return -1\n",
    "                    #print(\"Unexpected error: Cannot determine group number of file: \" + filePath)\n",
    "        else: #it is none and there was no group number in the filename\n",
    "            try:\n",
    "                finalGN = mode(poss_nums)\n",
    "            except:\n",
    "                #no mode found, couldn't find a group number\n",
    "                return -1\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    return finalGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### get contract start\n",
    "Uses regex and a list of keywords to attempt to find the start date of the contract. It makes multiple passes based on patterns seen in contract samples so far.\n",
    "\n",
    "Some of the passes are necessary to filter out non-date numbers that the datefinder incorrectly parses to dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractStart(sents_tokens):\n",
    "    start_keywords = [\"effective date\\S\\s*\\S\", \"effective\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:20]\n",
    "                    subset = \" \".join(subset)\n",
    "                    poss_dates.append(subset)\n",
    "                    if \"effective date\" in subset.lower():\n",
    "                        poss_dates.append(subset)\n",
    "\n",
    "                \n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\"]\n",
    "    \n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:6])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-6:])\n",
    "                    subset = subset_2 + subset_1\n",
    "\n",
    "                    m = datefinder.find_dates(subset)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "                    \n",
    "    ## Second pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.search(sent)\n",
    "\n",
    "            if not year==None:\n",
    "                #print(sent)\n",
    "                m = datefinder.find_dates(sent)\n",
    "                for match in m:\n",
    "                    if match.year >= 1966:\n",
    "                        matches.append(match)\n",
    "## Last pass: try to find the most common date. If there is more than one mode, choose the earliest date\n",
    "##.           this seems to occur when it is finding the contract start and end in equal quantities\n",
    "        #print(matches)\n",
    "        try:\n",
    "            finalDate = mode(matches)\n",
    "        except ValueError as e:\n",
    "            #print(str(e))\n",
    "            if matches:\n",
    "                earliestMatch = matches[0]\n",
    "                for match in matches:\n",
    "                    if(match < earliestMatch):\n",
    "                        earliestMatch = match\n",
    "                finalDate = earliestMatch\n",
    "            else:\n",
    "                finalDate = datetime.datetime(1066, 1, 1)\n",
    "        except Exception as e:\n",
    "            return None #i.e. not only could they not find a start date, something failed\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "####  get Contract End\n",
    "Similar to get contract start, it uses regex and keywords over multiple passes to attempt and find the contract end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractEnd(sents_tokens):\n",
    "    \n",
    "    start_keywords = [\"contract term\\S\\s*\\S\", \"contract term \", \"contract end\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates based on keywords\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:25]\n",
    "                    subset = \" \".join(subset)\n",
    "                    poss_dates.append(subset)\n",
    "\n",
    "\n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\"]\n",
    "\n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:6])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-6:])\n",
    "                    subset = subset_2 + subset_1\n",
    "\n",
    "                    m = datefinder.find_dates(subset)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "\n",
    "    ## Pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "            #print(sent)\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.search(sent)\n",
    "\n",
    "            if not year==None:\n",
    "                m = datefinder.find_dates(sent)\n",
    "                for match in m:\n",
    "                    if match.year >= 1966:\n",
    "                        matches.append(match)\n",
    "\n",
    "\n",
    "        #print(matches)\n",
    "\n",
    "    ### If there are exactly two matches, try to find a max. If error b/c they're the same, choose one\n",
    "        if(len(matches) == 2):\n",
    "            try:\n",
    "                finalDate = max(matches)\n",
    "            except ValueError as e:\n",
    "                finalDate = matches[0]\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "    ## If there are more, try and find the top two most mentioned and take the later. else just take the latest            \n",
    "        elif(len(matches) > 2):\n",
    "\n",
    "            try:\n",
    "                date1 = mode(matches)\n",
    "                matches.remove(date1)\n",
    "\n",
    "                date2 = mode(matches)\n",
    "                matches.remove(date2)\n",
    "\n",
    "                finalDate = max([date1, date2])\n",
    "            except ValueError as e:\n",
    "                #print(str(e))\n",
    "                if matches:\n",
    "                    latestMatch = matches[0]\n",
    "                    for match in matches:\n",
    "                        if(match > latestMatch):\n",
    "                            latestMatch = match\n",
    "                    finalDate = latestMatch\n",
    "            except Exception as e:\n",
    "                return None\n",
    "        else:\n",
    "            return datetime.datetime(1066, 1, 1)\n",
    "            #print(\"could not find contract end for file\")\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    #print(\"\\n\")\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### get Contract Duration\n",
    "Uses the functions getContractStart and getContractEnd to calculate a duration if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractDuration(sents_tokens):\n",
    "    start = None\n",
    "    end = None\n",
    "    duration = None\n",
    "    \n",
    "    try:\n",
    "        start = getContractStart(sents_tokens)\n",
    "        end = getContractEnd(sents_tokens)\n",
    "        \n",
    "        if(start and end):\n",
    "            if (start.year==1066) or (end.year == 1066):\n",
    "                return -1\n",
    "            else:\n",
    "                duration = (end - start).days\n",
    "                if duration <= 0:\n",
    "                    return -1\n",
    "        else:\n",
    "            return -1\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get State/Location: -- Not Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Helper function to create location data set from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def makeLocationDataStruct():\n",
    "    categories = []\n",
    "    location_data = {}\n",
    "    \n",
    "    us_filename = 'us_cities_states_counties.csv'\n",
    "    cwd = os.getcwd()\n",
    "    filepath = os.path.join(cwd, us_filename)\n",
    "    \n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='|')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                for cat in row:\n",
    "                    categories.append(cat)\n",
    "                    location_data[cat]=set()\n",
    "                category_row = 0\n",
    "            else:\n",
    "                #print(len(row))\n",
    "                for item in range(len(row)):\n",
    "                    location_data[categories[item]].add(row[item])\n",
    "    csvfile.close()\n",
    "    \n",
    "    states_filename = 'state_abbrv_to_name.csv'\n",
    "    filepath = os.path.join(cwd, states_filename)\n",
    "    \n",
    "    location_data['state_translations'] = {}\n",
    "\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                category_row = 0\n",
    "            else:\n",
    "                location_data['state_translations'][row[0]] = row[1]\n",
    "    \n",
    "    csvfile.close()\n",
    "    #print(location_data)\n",
    "    return location_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Call function to set up location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "location_data = makeLocationDataStruct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Get Client Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#arg1 = '/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "#arg2 = '/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/stanford-ner-2018-02-27/stanford-ner.jar'\n",
    "\n",
    "#st = StanfordNERTagger(arg1, arg2)\n",
    "\n",
    "def checkIfCity(loc_str, loc_data, isDelta, isContractholder):\n",
    "    hasCity = 0\n",
    "    cities = []\n",
    "    \n",
    "    if (loc_str in loc_data['City']):\n",
    "        hasCity=1\n",
    "        cities.append(loc_str)\n",
    "    return cities\n",
    "\n",
    "def checkIfState(loc_str, loc_data):\n",
    "    hasState = 0\n",
    "    states = []\n",
    "    \n",
    "    if(loc_str in loc_data['State full']):\n",
    "        hasState=1\n",
    "        states.append(loc_str.lower())\n",
    "    if(loc_str in location_data['State short']):\n",
    "        hasState = 1\n",
    "        try:\n",
    "            states.append(location_data['state_translations'][loc_str].lower())\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    return states\n",
    "    \n",
    "def getClientLocation(sents_tokens, bgs, tgs, location_data, filename):\n",
    "    loc_sents = set()\n",
    "    cities = []\n",
    "    states = []\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for sent in sents_tokens:\n",
    "\n",
    "            hasCity = 0\n",
    "            hasState = 0\n",
    "\n",
    "            sent_cities = []\n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to contractholder\" in sent.lower()\n",
    "\n",
    "            if sent.isupper():\n",
    "                sent = sent.lower()\n",
    "\n",
    "\n",
    "            text = nltk.word_tokenize(sent)\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            while (i < len(text)) and (len(text) > 2):\n",
    "                if(i < len(text)-2):\n",
    "                    text_bg = \" \".join([text[i], text[i+1]])\n",
    "                else:\n",
    "                    text_bg = \"\"\n",
    "                if(i < len(text)-2):\n",
    "                    text_tg = \" \".join([text[i], text[i+1], text[i+2]])\n",
    "                else:\n",
    "                    text_tg = \"\"\n",
    "\n",
    "                sent_cities.extend(checkIfCity(text[i], location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_bg, location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_tg, location_data, isDelta, isContractholder))\n",
    "\n",
    "                sent_states.extend(checkIfState(text[i], location_data))\n",
    "                sent_states.extend(checkIfState(text_bg, location_data))\n",
    "                sent_states.extend(checkIfState(text_tg, location_data))\n",
    "\n",
    "                i+=1\n",
    "\n",
    "            if (len(sent_states)>0):\n",
    "                #print(sent)\n",
    "                cities.extend(sent_cities)\n",
    "                states.extend(sent_states)\n",
    "            \n",
    "        FN_chunks = re.findall(r\"[\\w]+|[-\\s_]\", filename)\n",
    "        \n",
    "        for chunk in FN_chunks:\n",
    "            FN_state = checkIfState(chunk, location_data)\n",
    "            if len(FN_state)>0:\n",
    "                states.append(FN_state[0])\n",
    "                \n",
    "                \n",
    "        try:\n",
    "            final_state = mode(states)\n",
    "            #print(final_state)\n",
    "        except ValueError as ve:\n",
    "            final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            final_state = None\n",
    "        try:\n",
    "            final_city = mode(cities)\n",
    "        except ValueError as ve:\n",
    "            final_city = \"not_a_city\"\n",
    "            #print(final_city)\n",
    "        except Exception as e:\n",
    "            #print(str(e))\n",
    "            final_city = None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None #error in function execution\n",
    "    \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#    isDelta = \"deltadental\" in sent.lower()\n",
    "            #    isContractholder = \"contractholder\" in sent.lower()\n",
    "            #    if not isDelta and isContractholder:\n",
    "                   # print(sent)\n",
    "                   # print(\"\\n\")\n",
    "                    #[aA]ddress[-: ]\n",
    "                    #\"\\s+\\d+\\D+\" + \"[\"+\"|\".join(cities) + \"]\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "            #        add_regex_str = \"[aA]ddress[-: ]\\s+\\d+[\\D\\d]+\" + \"[\"+\"|\".join(sent_cities) + \"]*\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]*\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "                   # print(add_regex_str)\n",
    "            #        add_regex = re.compile(add_regex_str)\n",
    "                   # print(str(sent_cities))\n",
    "                  #  print(sent_states)\n",
    "            #        matches = add_regex.findall(sent)\n",
    "                    #print(matches)\n",
    "                    #if matches:\n",
    "                     #   for m in matches:\n",
    "                   #         print(m)\n",
    "                   # else:\n",
    "                   #     add_regex_str = \"\\s+\\d+[\\D\\d]+\" + \"[\"+\"|\".join(sent_cities) + \"]*\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]*\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "                   #     add_regex = re.compile(add_regex_str)\n",
    "                   #     matches = add_regex.findall(sent)\n",
    "                        #loc_sents.add(matches.group())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Office Involved -- Not Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDeltaOffice(sents_tokens, bgs, tgs, location_data):\n",
    "    loc_sents = set()\n",
    "    cities = []\n",
    "    states = []\n",
    "    \n",
    "    try:\n",
    "        for sent in sents_tokens:\n",
    "            hasCity = 0\n",
    "            hasState = 0\n",
    "\n",
    "            sent_cities = []\n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to contractholder\" in sent.lower()\n",
    "\n",
    "            if isDelta:\n",
    "                if sent.isupper():\n",
    "                    sent = sent.lower()\n",
    "\n",
    "                text = nltk.word_tokenize(sent)\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                while (i < len(text)) and (len(text) > 2):\n",
    "                    if(i < len(text)-2):\n",
    "                        text_bg = \" \".join([text[i], text[i+1]])\n",
    "                    else:\n",
    "                        text_bg = \"\"\n",
    "                    if(i < len(text)-2):\n",
    "                        text_tg = \" \".join([text[i], text[i+1], text[i+2]])\n",
    "                    else:\n",
    "                        text_tg = \"\"\n",
    "\n",
    "                    sent_cities.extend(checkIfCity(text[i], location_data, isDelta, isContractholder))\n",
    "                    sent_cities.extend(checkIfCity(text_bg, location_data, isDelta, isContractholder))\n",
    "                    sent_cities.extend(checkIfCity(text_tg, location_data, isDelta, isContractholder))\n",
    "\n",
    "                    sent_states.extend(checkIfState(text[i], location_data))\n",
    "                    sent_states.extend(checkIfState(text_bg, location_data))\n",
    "                    sent_states.extend(checkIfState(text_tg, location_data))\n",
    "\n",
    "                    i+=1\n",
    "\n",
    "                if (len(sent_states)>0):\n",
    "\n",
    "                    for city in sent_cities:\n",
    "                        for state in sent_states:\n",
    "\n",
    "                            add_regex_str = \"\\s+\\d+[\\w\\s]+\" + \"\\s+\" + str(city) + \"\\s+\" + str(state) + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "                            add_regex = re.compile(add_regex_str, re.IGNORECASE)\n",
    "                            matches = add_regex.findall(sent)\n",
    "                            if matches:\n",
    "                                cities.append(city)\n",
    "                                states.append(state)\n",
    "                                #for m in matches:\n",
    "                                #    print(m)\n",
    "\n",
    "\n",
    "        try:\n",
    "            final_state = mode(states)\n",
    "            #print(final_state)\n",
    "        except ValueError as ve:\n",
    "            final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            final_state = None\n",
    "        try:\n",
    "            final_city = mode(cities)\n",
    "            #print(final_city)\n",
    "        except ValueError as ve:\n",
    "            final_city = \"not_a_city\"\n",
    "        except Exception as e:\n",
    "            #print(str(e))\n",
    "            final_city = None\n",
    "\n",
    "    except Exception as e:\n",
    "        final_state = None\n",
    "        \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Run to get attributes\n",
    "#### Functions to process multiple files and their attributes at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Function: batch pre process: fill output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchPreProcess(errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    print(cwd)\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    dataPath = os.path.join(cwd, \"data/raw\")\n",
    "\n",
    "    if(os.path.isdir(dataPath)):\n",
    "\n",
    "        for file in os.listdir(dataPath):\n",
    "            filepath = os.path.join(dataPath, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                print(\"pre-processing: \" + file)\n",
    "                try:\n",
    "                    if(checkFileType(filepath) == 0):\n",
    "                        processedTextPath = processTextFile(filepath)\n",
    "                        if not processedTextPath:\n",
    "                            errorFile.write(filepath + \", pre-processing txt\\n\")\n",
    "                            print(\"Error pre-processing file: \" +  filepath)\n",
    "                    elif(checkFileType(filepath) == 1):\n",
    "                        processedTextPath = processPDFfile(filepath)\n",
    "                        if not processedTextPath:\n",
    "                            errorFile.write(filepath + \", pre-processing pdf\\n\")\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                    else:\n",
    "                        errorFile.write(filepath + \", pre-processing: invalid filetype\\n\")\n",
    "                        raise TypeError('This path does not lead to a valid file type!')                     \n",
    "                except Exception as e:\n",
    "                    errorFile.write(filepath + \", pre-processing\\n\")\n",
    "                    print(\"Error pre-processing file: \" + filepath)\n",
    "\n",
    "    else:\n",
    "        print(\"Folder data/raw doesn't exist\")\n",
    "        return None\n",
    "    return \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Function: Batch return token and bigram sets for all output files\n",
    "Returns file information as an array of objects containing key:value information about the file: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[ \n",
    "\n",
    "    {\n",
    "    \n",
    "        'filepath':'users/sydneyknox...', \n",
    "        \n",
    "        'wordTokens':[*tokens*], \n",
    "        \n",
    "        ...\n",
    "        \n",
    "    }, \n",
    "    \n",
    "    {  \n",
    "    \n",
    "        'sentenceTokens':[*tokens*],\n",
    "        \n",
    "        'cleanText':\"string containing the original text from the processed file...\"\n",
    "        \n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchGetTokens():\n",
    "    all_tokens = []\n",
    "    cwd = os.getcwd()\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    dataPath = os.path.join(cwd, \"data/output\")\n",
    "    print(dataPath)\n",
    "    \n",
    "    if(os.path.isdir(dataPath)):\n",
    "\n",
    "        for file in os.listdir(dataPath):\n",
    "            filepath = os.path.join(dataPath, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                try:\n",
    "                    print(\"getting tokens: \" + file)\n",
    "\n",
    "                    temp_obj = {}\n",
    "\n",
    "                    with open(filepath, 'r') as txtFile:\n",
    "                        text = txtFile.read()\n",
    "\n",
    "                    temp_obj['filepath'] = filepath\n",
    "\n",
    "                    text = ddCleanText(text)\n",
    "                    temp_obj['cleanText'] = text\n",
    "\n",
    "                    wordTokens = getTokens(text)\n",
    "                    sentTokens = getSents(text)\n",
    "                    temp_obj['wordTokens'] = wordTokens\n",
    "                    temp_obj['sentTokens'] = sentTokens\n",
    "\n",
    "                    bgs = getBigrams(wordTokens)\n",
    "                    tgs = getTrigrams(wordTokens)\n",
    "                    temp_obj['bgs'] = bgs\n",
    "                    temp_obj['tgs'] = tgs\n",
    "\n",
    "                    txtFile.close()\n",
    "                    all_tokens.append(temp_obj)\n",
    "                except Exception as e:\n",
    "                    print(\"Error opening and tokenizing \" + file)\n",
    "                    #print(str(e))\n",
    "\n",
    "    else:\n",
    "        print(\"Folder data/output doesn't exist. Pre-processing failed.\")\n",
    "        return None\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function: get metadata attributes\n",
    "This function takes in a single files info -- in this section because it will be used in a batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getMetaDataAtt(file_info):\n",
    "    #print(file_info)\n",
    "    file_attr = {}\n",
    "    file_attr['filepath'] = file_info['filepath']\n",
    "    \n",
    "    fileName = getFileName(file_info['filepath'])\n",
    "    if not fileName:\n",
    "        fileName = file_info['filepath']\n",
    "        file_attr['fileName'] = fileName\n",
    "    else:\n",
    "        print(fileName)\n",
    "        file_attr['fileName'] = fileName\n",
    "    \n",
    "    groupNumber = getGroupNumber(file_info['sentTokens'], file_info['filepath'])\n",
    "    file_attr['groupNumber'] = groupNumber\n",
    "    #if not (groupNumber):\n",
    "        #Function failed to execute correctly\n",
    "    #    print(\"function getGroupNumber failed to execute.\\n\")\n",
    "    #elif groupNumber==-1:\n",
    "    #     print(\"could not find valid group number in file\")\n",
    "    #else:\n",
    "    #    print(\"group number: \" , groupNumber)\n",
    "    \n",
    "    contractStartDate = getContractStart(file_info['sentTokens'])\n",
    "    file_attr['contractStartDate'] = contractStartDate\n",
    "    #if not (contractStartDate):\n",
    "        # Function failed to execute\n",
    "    #elif contractStartDate.year == 1066:\n",
    "        #could not find valid start date\n",
    "    #else:\n",
    "    #    print(\"start: \" , contractStartDate)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    contractEndDate = getContractEnd(file_info['sentTokens'])\n",
    "    file_attr['contractEndDate'] = contractEndDate\n",
    "    #if not (contractEndDate):\n",
    "        #Function failed to execute\n",
    "    #elif contractEndDate.year == 1066:\n",
    "        #could not find valid start date\n",
    "    #else:\n",
    "    #    print(\"end: \" , contractEndDate)\n",
    "    \n",
    "    contractDuration = getContractDuration(file_info['sentTokens'])\n",
    "    file_attr['contractDuration'] = contractDuration\n",
    "    #if not (contractDuration):\n",
    "        #Function failed to execute\n",
    "    #elif contractDuration == -1:\n",
    "        #Could not calculate valid duration\n",
    "    #else:\n",
    "    #    print(\"duration: \" , contractDuration)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    clientLocation = getClientLocation(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data, fileName)\n",
    "    file_attr['clientLocation'] = clientLocation\n",
    "    #if not (clientLocation):\n",
    "        #Failed to execute\n",
    "    #elif clientLocation == \"not_a_state\":\n",
    "        #Failed to find a valid location\n",
    "    #else:\n",
    "    #    print(\"Client Office in: \" + clientLocation)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    deltaOfficeLocation = getDeltaOffice(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data)\n",
    "    file_attr['deltaOfficeLocation'] = deltaOfficeLocation\n",
    "    #if not (deltaOfficeLocation):\n",
    "        #failed to execute\n",
    "    #elif deltaOfficeLocation == \"not_a_state\":\n",
    "        #Failed to find valid location for DD office\n",
    "    #else:\n",
    "     #   print(\"Delta Office in: \" + str(deltaOfficeLocation))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    d={'key':'filename', 'value':fileName}\n",
    "    dfMD = pd.DataFrame(d, index=['MetaData'])\n",
    "    \n",
    "   ## df=pd.DataFrame({'key':'group_number','value':groupNumber}, index=['MetaData'])\n",
    "   ## dfMD = pd.concat([dfMD, df])\n",
    "    \n",
    "   ## df=pd.DataFrame({'key':'contract_start_date','value':contractStartDate}, index=['MetaData'])\n",
    "   ## dfMD = pd.concat([dfMD, df])\n",
    "    \n",
    "    return file_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Chunking/ POS practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser('CHUNK: {<V.* to>}')\n",
    "brown = nltk.corpus.brown\n",
    "for sent in brown.tagged_sents():\n",
    "    tree = cp.parse(sent)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CHUNK': print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Practice with synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def testSyns(sentTokens, word1, word2):\n",
    "    syns1 = wn.synsets(word1)\n",
    "    syns2 = wn.synsets(word2)\n",
    "    #print(syns)\n",
    "    #print(syns[0].lemmas()[1].name())\n",
    "    set1 = set()\n",
    "    for syn in syns1:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    #print(set1)\n",
    "    set2 = set()\n",
    "    for syn in syns2:\n",
    "        for lem in syn.lemmas():\n",
    "            set2.add(lem.name())\n",
    "    #print(set2)\n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            word_set.add(\" \".join([word1, word2]).lower())\n",
    "    #print(word_set)\n",
    "    for sent in sentTokens:\n",
    "        words = word_tokenize(sent)\n",
    "        start = 0\n",
    "        end = 1\n",
    "        #while(end < len(words)):\n",
    "            #print(\" \".join([words[start], words[end]]))\n",
    "            #if(\" \".join([words[start], words[end]]).lower() in word_set):\n",
    "                #print(\" \".join([words[start], words[end]]))\n",
    "                #print(sent)\n",
    "            #start += 1\n",
    "            #end += 1\n",
    "    return\n",
    "            \n",
    "#testSyns(sentTokens, \"effective\", \"date\")            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## workspace: batch process files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### testArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     3,
     5,
     7,
     11
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filePath = \"./data/raw/TX 17404 Contract Regional (7.2.18).docx\"\n",
    "processedTextPath = \"\"\n",
    "\n",
    "if(checkFileType(filePath) == 0):\n",
    "    processedTextPath = processTextFile(filePath)\n",
    "elif(checkFileType(filePath) == 1):\n",
    "    processedTextPath = processPDFfile(filePath)\n",
    "else:\n",
    "    raise TypeError(\"File type incorrect\") \n",
    "\n",
    "#print(processedTextPath)\n",
    "with open(processedTextPath, 'r') as txtFile:\n",
    "            text = txtFile.read()\n",
    "\n",
    "\n",
    "text = ddCleanText(text)\n",
    "wordTokens = getTokens(text)\n",
    "sentTokens = getSents(text)\n",
    "#print(sentTokens)\n",
    "bgs = getBigrams(wordTokens)\n",
    "tgs = getTrigrams(wordTokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Call PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "batchPreProcess() missing 1 required positional argument: 'errorFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-502470092db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatchPreProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: batchPreProcess() missing 1 required positional argument: 'errorFile'"
     ]
    }
   ],
   "source": [
    "batchPreProcess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Call the tokenizing functions to get organized data for all the files in the pre-processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output\n",
      "Error opening and tokenizing .DS_Store\n"
     ]
    }
   ],
   "source": [
    "base_info = batchGetTokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Call the functions to begin extracting and storing the data from all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TX 19015 Attachment A ENT (7.2.18).txt\n",
      "filepath,\n",
      "fileName,\n",
      "groupNumber,\n",
      "contractStartDate,\n",
      "contractEndDate,\n",
      "contractDuration,\n",
      "clientLocation,\n",
      "deltaOfficeLocation,\n",
      "\n",
      "\n",
      "TX 17404 EOC Regional (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 17404 EOC Regional (7.2.18).txt,\n",
      "TX 17404 EOC Regional (7.2.18).txt,\n",
      "17404,\n",
      "2017-10-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "texas,\n",
      "georgia,\n",
      "\n",
      "\n",
      "TX 17404 EOC (Regional (7.2.18)-pdf.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 17404 EOC (Regional (7.2.18)-pdf.txt,\n",
      "TX 17404 EOC (Regional (7.2.18)-pdf.txt,\n",
      "17404,\n",
      "2013-07-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "texas,\n",
      "georgia,\n",
      "\n",
      "\n",
      "TX-19278 ASC-ENT (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX-19278 ASC-ENT (7.2.18).txt,\n",
      "TX-19278 ASC-ENT (7.2.18).txt,\n",
      "19278,\n",
      "2018-03-01 00:00:00,\n",
      "2019-12-31 00:00:00,\n",
      "670,\n",
      "georgia,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "test2.txt\n",
      "'VI'\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/test2.txt,\n",
      "test2.txt,\n",
      "2,\n",
      "1066-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "illinois,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX 19015 Contract ENT (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 19015 Contract ENT (7.2.18).txt,\n",
      "TX 19015 Contract ENT (7.2.18).txt,\n",
      "19015,\n",
      "1066-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "texas,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "test2-pdf.txt\n",
      "'VI'\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/test2-pdf.txt,\n",
      "test2-pdf.txt,\n",
      "2,\n",
      "1066-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "illinois,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX 17404 Contract Regional (7.2.18)-pdf.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 17404 Contract Regional (7.2.18)-pdf.txt,\n",
      "TX 17404 Contract Regional (7.2.18)-pdf.txt,\n",
      "17404,\n",
      "1066-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "texas,\n",
      "georgia,\n",
      "\n",
      "\n",
      "TX 19015 Attachment  C ENT (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 19015 Attachment  C ENT (7.2.18).txt,\n",
      "TX 19015 Attachment  C ENT (7.2.18).txt,\n",
      "19015,\n",
      "2018-01-01 00:00:00,\n",
      "2019-12-31 00:00:00,\n",
      "729,\n",
      "california,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX 19015 Contract ENT (7.2.18)-pdf.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 19015 Contract ENT (7.2.18)-pdf.txt,\n",
      "TX 19015 Contract ENT (7.2.18)-pdf.txt,\n",
      "19015,\n",
      "2018-01-01 00:00:00,\n",
      "2019-12-31 00:00:00,\n",
      "729,\n",
      "texas,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX-18745-ASC Contract (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX-18745-ASC Contract (7.2.18).txt,\n",
      "TX-18745-ASC Contract (7.2.18).txt,\n",
      "18745,\n",
      "2017-04-01 00:00:00,\n",
      "2019-12-31 00:00:00,\n",
      "1004,\n",
      "not_a_state,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX 17404 Contract Regional (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 17404 Contract Regional (7.2.18).txt,\n",
      "TX 17404 Contract Regional (7.2.18).txt,\n",
      "17404,\n",
      "2014-10-01 00:00:00,\n",
      "2017-09-30 00:00:00,\n",
      "1095,\n",
      "texas,\n",
      "georgia,\n",
      "\n",
      "\n",
      "test.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/test.txt,\n",
      "test.txt,\n",
      "-1,\n",
      "1066-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "california,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX-18745 ASC Contract (7.2.18)-pdf.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX-18745 ASC Contract (7.2.18)-pdf.txt,\n",
      "TX-18745 ASC Contract (7.2.18)-pdf.txt,\n",
      "18745,\n",
      "2017-04-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "not_a_state,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX 19015 Attachment B ENT (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 19015 Attachment B ENT (7.2.18).txt,\n",
      "TX 19015 Attachment B ENT (7.2.18).txt,\n",
      "19015,\n",
      "2018-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "texas,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX 19015 EOC ENT (7.2.18).txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 19015 EOC ENT (7.2.18).txt,\n",
      "TX 19015 EOC ENT (7.2.18).txt,\n",
      "19015,\n",
      "2018-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "texas,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "test-pdf.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/test-pdf.txt,\n",
      "test-pdf.txt,\n",
      "-1,\n",
      "1066-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "california,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX-19278 ASC-ENT (7.2.18)-pdf.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX-19278 ASC-ENT (7.2.18)-pdf.txt,\n",
      "TX-19278 ASC-ENT (7.2.18)-pdf.txt,\n",
      "19278,\n",
      "2018-03-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "georgia,\n",
      "not_a_state,\n",
      "\n",
      "\n",
      "TX 19015 EOC ENT(7.2.18)-pdf.txt\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output/TX 19015 EOC ENT(7.2.18)-pdf.txt,\n",
      "TX 19015 EOC ENT(7.2.18)-pdf.txt,\n",
      "19015,\n",
      "2018-01-01 00:00:00,\n",
      "1066-01-01 00:00:00,\n",
      "-1,\n",
      "texas,\n",
      "texas,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputFilePath = './data/processed/raw_attribute_data.csv'\n",
    "#print(newFilePath)\n",
    "outputFile=open(outputFilePath, 'w')    \n",
    "first_row = 1\n",
    "\n",
    "for file in base_info:\n",
    "    file_attr = getMetaDataAtt(file)\n",
    "    if first_row:\n",
    "        for key in file_attr:\n",
    "            print(key + \",\")\n",
    "            outputFile.write(key + \",\")\n",
    "        print(\"\\n\")\n",
    "        outputFile.write(\"\\n\")\n",
    "        first_row = 0\n",
    "    else:\n",
    "        for attr in file_attr:\n",
    "            print(str(file_attr[attr]) + \",\")\n",
    "            outputFile.write(str(file_attr[attr])+ \",\")\n",
    "        print(\"\\n\")\n",
    "        outputFile.write(\"\\n\")\n",
    "        \n",
    "outputFile.close()\n",
    "    #testSyns(file['sentTokens'], \"group\",\"number\")\n",
    "    \n",
    "    #transpose=dfMD.transpose()\n",
    "    #print(transpose)\n",
    "    #print(dfMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full run through of batch processing with error checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode\n",
      "pre-processing: 19168 Contract.pdf\n",
      "pre-processing: 01255 _Schedule I_ January 1, 2014.pdf\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - EOC - Eff 7-1-18.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILD DEVELOPMENT CENTER INC - EOC - Eff 7-1-18.doc\n",
      "pre-processing: 10041 Appendix C EOCc Meridian Union (Janb2014).pdf\n",
      "pre-processing: 10041 Appendix E EOC Daingerfield Union (Jan2018).pdf\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - PREM AGMT - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILD DEVELOPMENT CENTER INC - PREM AGMT - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 19168 EOC.docx\n",
      "pre-processing: 01094 EOC  7-1-16.pdf\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - R46 - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILD DEVELOPMENT CENTER INC - R46 - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19.pdf\n",
      "pre-processing: 1036 - CONTRACT (Eff. 7-1-09).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1036 - CONTRACT (Eff. 7-1-09).doc\n",
      "pre-processing: 10041 Appendix F EOC Ardmore Union (Jan2012).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/10041 Appendix F EOC Ardmore Union (Jan2012).doc\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - R46 - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILDREN'S SERVICES INC - R46 - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - EOC - Eff 7-1-18.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILDREN'S SERVICES INC - EOC - Eff 7-1-18.doc\n",
      "pre-processing: 18635 ASC Contract.docx\n",
      "pre-processing: 01248 (FULL CONTRACT) Eff. 10-1-14.pdf\n",
      "pre-processing: TX 19015 EOC ENT(7.2.18).pdf\n",
      "pre-processing: 25-1149 ASC Agreement.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/25-1149 ASC Agreement.doc\n",
      "pre-processing: test.docx\n",
      "pre-processing: 01094 - SI - Eff 7-1-17 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01094 - SI - Eff 7-1-17 to 6-30-19.doc\n",
      "pre-processing: 19168 Attachment B High.docx\n",
      "pre-processing: 01248 (FULL EOC FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.pdf\n",
      "pre-processing: TX 19015 Contract ENT (7.2.18).pdf\n",
      "pre-processing: 01094 (SI) Eff. 7-1-15.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01094 (SI) Eff. 7-1-15.doc\n",
      "pre-processing: 01094 (Tax Modfication).docx\n",
      "pre-processing: .DS_Store\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/.DS_Store\n",
      "pre-processing: 3310 EOC(Jan2013).pdf\n",
      "pre-processing: TX 17404 EOC (Regional (7.2.18).pdf\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - R60 - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILDREN'S SERVICES INC - R60 - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 19223 PPO Contract (01-01-18).pdf\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - PREM AGMT - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILDREN'S SERVICES INC - PREM AGMT - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 01255 (Tax Modification).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01255 (Tax Modification).doc\n",
      "pre-processing: 19223 Attach B (01-01-18).docx\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - R60 - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILD DEVELOPMENT CENTER INC - R60 - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 01036 (EOC) 7-1-14.pdf\n",
      "pre-processing: 01094 _SBC Modification_.pdf\n",
      "pre-processing: 01248 - SII (Eff. 10-1-14).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01248 - SII (Eff. 10-1-14).doc\n",
      "pre-processing: TX-19278 ASC-ENT (7.2.18).docx\n",
      "pre-processing: TX-18745-ASC Contract (7.2.18).docx\n",
      "pre-processing: 1094 Full Contract (Eff. 7-1-07).pdf\n",
      "pre-processing: 3310 ASC.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/3310 ASC.doc\n",
      "pre-processing: 19223 PPO EBB (01-01-18).docx\n",
      "pre-processing: 01248 (EOC FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01248 (EOC FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.doc\n",
      "pre-processing: TX-19278 ASC-ENT (7.2.18).pdf\n",
      "pre-processing: 01255 - EOC (January 1, 2011).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01255 - EOC (January 1, 2011).doc\n",
      "pre-processing: 01255 (Schedule I) January 1, 2013.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01255 (Schedule I) January 1, 2013.doc\n",
      "pre-processing: 10041 Appendix G EOC Franklin Union (Jan2012).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/10041 Appendix G EOC Franklin Union (Jan2012).doc\n",
      "pre-processing: 01255 (Full Contract) Eff. 2-1-07.pdf\n",
      "pre-processing: 01248 (CONTRACT) Eff. 10-1-14.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01248 (CONTRACT) Eff. 10-1-14.doc\n",
      "pre-processing: 19168 EOC.pdf\n",
      "pre-processing: 10041 Appendix C EOC Meridian Union (Jan2014).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/10041 Appendix C EOC Meridian Union (Jan2014).doc\n",
      "pre-processing: 19194 Contract (Eff. 1-1-18).docx\n",
      "pre-processing: 19168 Attachment A.docx\n",
      "pre-processing: 1036 - SII (Eff. 7-1-09).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1036 - SII (Eff. 7-1-09).doc\n",
      "pre-processing: 19223 PPO Contract (01-01-18).docx\n",
      "pre-processing: 3468 EOC (RevSept2016).docx\n",
      "pre-processing: 01255 (Schedule I) January 1, 2014.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01255 (Schedule I) January 1, 2014.doc\n",
      "pre-processing: 10041 Appendix D EOC Hampton Union (Janb2012).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/10041 Appendix D EOC Hampton Union (Janb2012).doc\n",
      "pre-processing: 3468 ASC.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/3468 ASC.doc\n",
      "pre-processing: 1036 - R29 (Eff. 7-1-09).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1036 - R29 (Eff. 7-1-09).doc\n",
      "pre-processing: test2.pdf\n",
      "pre-processing: 10294-01002 & 02001 PPO EOC (07-01-12).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/10294-01002 & 02001 PPO EOC (07-01-12).doc\n",
      "pre-processing: 10041 Appendix B EOC Non-Union (Jan2018).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/10041 Appendix B EOC Non-Union (Jan2018).doc\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - CONTRACT - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILDREN'S SERVICES INC - CONTRACT - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 1036 - R30 (Eff. 7-1-09).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1036 - R30 (Eff. 7-1-09).doc\n",
      "pre-processing: 25-1149 ASC Agreement Signed.pdf\n",
      "pre-processing: 01248 (SI) Eff. 10-1-14.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01248 (SI) Eff. 10-1-14.doc\n",
      "pre-processing: 01094 (Schedule I) July 1, 2013.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01094 (Schedule I) July 1, 2013.doc\n",
      "pre-processing: 01094 (Schedule I and Tax Modification) July 1, 2013.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing: 1036 - SI (Eff. 7-1-09).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1036 - SI (Eff. 7-1-09).doc\n",
      "pre-processing: 3468 EOC (RevSept2016).pdf\n",
      "pre-processing: 10041 Appendix G EOC Franklin Union (Jan2012).pdf\n",
      "pre-processing: TX 19015 Attachment B ENT (7.2.18).docx\n",
      "pre-processing: test.pdf\n",
      "pre-processing: 19223 Attach A (01-01-18).docx\n",
      "pre-processing: 19168 Attachment C.docx\n",
      "pre-processing: 19194 Contract Eff. 1-1-18.pdf\n",
      "pre-processing: 10294-01002 & 02001 PPO EOC (07-01-12).pdf\n",
      "pre-processing: 10041 Appendix B EOC Non-Union (Jan2018).pdf\n",
      "pre-processing: 01248 (R7 FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01248 (R7 FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.doc\n",
      "pre-processing: test2.docx\n",
      "pre-processing: 10041 Appendix D EOCc Hampton Union (Jan2012).pdf\n",
      "pre-processing: 45-3310 ASC Signed.pdf\n",
      "pre-processing: TX 19015 EOC ENT (7.2.18).docx\n",
      "pre-processing: TX-18745 ASC Contract (7.2.18).pdf\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - SCHEDULE A - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILDREN'S SERVICES INC - SCHEDULE A - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 1036 - R15 (Eff. 7-1-09).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1036 - R15 (Eff. 7-1-09).doc\n",
      "pre-processing: TX 17404 EOC Regional (7.2.18).docx\n",
      "pre-processing: 19223 PPO EBB (01-01-18).pdf\n",
      "pre-processing: TX 17404 Contract Regional (7.2.18).pdf\n",
      "pre-processing: 01094 - SI - Eff 7-1-17 to 6-30-19.pdf\n",
      "pre-processing: 1036- sch 1 7-1-09.pdf\n",
      "pre-processing: 01248 (R8 FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01248 (R8 FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14.doc\n",
      "pre-processing: 1036 Sched I 7-1-09.pdf\n",
      "pre-processing: 1255 master 2 1 07.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1255 master 2 1 07.doc\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - SCHEDULE A - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILD DEVELOPMENT CENTER INC - SCHEDULE A - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 01255 - EOC ( 1-1-11).pdf\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01255 - EOC ( 1-1-11).pdf\n",
      "pre-processing: TX 17404 Contract Regional (7.2.18).docx\n",
      "pre-processing: 19168 Attachment B Low.docx\n",
      "pre-processing: 10041 Appendix F EOC Ardmore Union (Jan2012).pdf\n",
      "pre-processing: 19168 Contract.docx\n",
      "pre-processing: 10041 Appendix E EOC Daingerfield Union (Jan2018).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/10041 Appendix E EOC Daingerfield Union (Jan2018).doc\n",
      "pre-processing: 01094 EOC  7-1-16.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01094 EOC  7-1-16.doc\n",
      "pre-processing: 18635 ASC Contract.pdf\n",
      "pre-processing: 01036 (EOC) 7-1-14.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01036 (EOC) 7-1-14.doc\n",
      "pre-processing: TX 19015 Attachment  C ENT (7.2.18).docx\n",
      "pre-processing: TX 19015 Contract ENT (7.2.18).docx\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - FULL EOC - Eff 7-1-18.pdf\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - CONTRACT - Eff 7-1-18 to 6-30-19.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/19380 - JACKSON CHILD DEVELOPMENT CENTER INC - CONTRACT - Eff 7-1-18 to 6-30-19.doc\n",
      "pre-processing: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL EOC - Eff 7-1-18.pdf\n",
      "pre-processing: 19380 - JACKSON CHILDREN'S SERVICES INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19.pdf\n",
      "pre-processing: 1036 (Schedule I) July 1, 2009.doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/1036 (Schedule I) July 1, 2009.doc\n",
      "pre-processing: TX 19015 Attachment A ENT (7.2.18).docx\n",
      "pre-processing: 01094 (SBC Modification).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/01094 (SBC Modification).doc\n",
      "pre-processing: 01255 (Schedule I and Tax Modification) January 1, 2013.pdf\n",
      "pre-processing: 3310 EOC(Jan2013).doc\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/3310 EOC(Jan2013).doc\n",
      "pre-processing: 1036- contract 7-1-09.pdf\n",
      "pre-processing: ASO 2010 Contract.pdf\n",
      "Error pre-processing file: /Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/raw/ASO 2010 Contract.pdf\n",
      "/Users/sydneyknox/Documents/data-insights/jupyter-pseudocode/data/output\n",
      "getting tokens: 19168 EOC.txt\n",
      "getting tokens: 10041 Appendix C EOCc Meridian Union (Janb2014)-pdf.txt\n",
      "getting tokens: TX 19015 Attachment A ENT (7.2.18).txt\n",
      "getting tokens: 19223 Attach B (01-01-18).txt\n",
      "getting tokens: TX 17404 EOC Regional (7.2.18).txt\n",
      "getting tokens: 19168 Contract-pdf.txt\n",
      "getting tokens: 1036 Sched I 7-1-09-pdf.txt\n",
      "getting tokens: TX 17404 EOC (Regional (7.2.18)-pdf.txt\n",
      "getting tokens: 01094 - SI - Eff 7-1-17 to 6-30-19-pdf.txt\n",
      "getting tokens: 19223 PPO EBB (01-01-18)-pdf.txt\n",
      "getting tokens: TX-19278 ASC-ENT (7.2.18).txt\n",
      "getting tokens: .DS_Store\n",
      "Error opening and tokenizing .DS_Store\n",
      "getting tokens: 10041 Appendix D EOCc Hampton Union (Jan2012)-pdf.txt\n",
      "getting tokens: 01248 (FULL CONTRACT) Eff. 10-1-14-pdf.txt\n",
      "getting tokens: test2.txt\n",
      "getting tokens: 19194 Contract (Eff. 1-1-18).txt\n",
      "getting tokens: 45-3310 ASC Signed-pdf.txt\n",
      "getting tokens: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL EOC - Eff 7-1-18-pdf.txt\n",
      "getting tokens: 19223 Attach A (01-01-18).txt\n",
      "getting tokens: TX 19015 Contract ENT (7.2.18).txt\n",
      "getting tokens: 19168 Contract.txt\n",
      "getting tokens: test2-pdf.txt\n",
      "getting tokens: 1036- sch 1 7-1-09-pdf.txt\n",
      "getting tokens: 01094 _SBC Modification_-pdf.txt\n",
      "getting tokens: 19223 PPO Contract (01-01-18).txt\n",
      "getting tokens: TX 17404 Contract Regional (7.2.18)-pdf.txt\n",
      "getting tokens: 01255 (Schedule I and Tax Modification) January 1  2013-pdf.txt\n",
      "getting tokens: 10041 Appendix B EOC Non-Union (Jan2018)-pdf.txt\n",
      "getting tokens: 3310 EOC(Jan2013)-pdf.txt\n",
      "getting tokens: 01094 EOC  7-1-16-pdf.txt\n",
      "getting tokens: 01094 (Tax Modfication).txt\n",
      "getting tokens: 19380 - JACKSON CHILDREN'S SERVICES INC - FULL EOC - Eff 7-1-18-pdf.txt\n",
      "getting tokens: TX 19015 Attachment  C ENT (7.2.18).txt\n",
      "getting tokens: TX 19015 Contract ENT (7.2.18)-pdf.txt\n",
      "getting tokens: 19223 PPO EBB (01-01-18).txt\n",
      "getting tokens: 1036- contract 7-1-09-pdf.txt\n",
      "getting tokens: 18635 ASC Contract-pdf.txt\n",
      "getting tokens: 1094 Full Contract (Eff. 7-1-07)-pdf.txt\n",
      "getting tokens: TX-18745-ASC Contract (7.2.18).txt\n",
      "getting tokens: TX 17404 Contract Regional (7.2.18).txt\n",
      "getting tokens: 01255 _Schedule I_ January 1  2014-pdf.txt\n",
      "getting tokens: 3468 EOC (RevSept2016)-pdf.txt\n",
      "getting tokens: 10294-01002 & 02001 PPO EOC (07-01-12)-pdf.txt\n",
      "getting tokens: 18635 ASC Contract.txt\n",
      "getting tokens: 19168 Attachment B Low.txt\n",
      "getting tokens: 19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19-pdf.txt\n",
      "getting tokens: test.txt\n",
      "getting tokens: TX-18745 ASC Contract (7.2.18)-pdf.txt\n",
      "getting tokens: 19194 Contract Eff. 1-1-18-pdf.txt\n",
      "getting tokens: 19380 - JACKSON CHILDREN'S SERVICES INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19-pdf.txt\n",
      "getting tokens: 10041 Appendix E EOC Daingerfield Union (Jan2018)-pdf.txt\n",
      "getting tokens: 19168 Attachment C.txt\n",
      "getting tokens: 19168 Attachment A.txt\n",
      "getting tokens: TX 19015 Attachment B ENT (7.2.18).txt\n",
      "getting tokens: 3468 EOC (RevSept2016).txt\n",
      "getting tokens: TX 19015 EOC ENT (7.2.18).txt\n",
      "getting tokens: 01094 (Schedule I and Tax Modification) July 1  2013-pdf.txt\n",
      "getting tokens: 01036 (EOC) 7-1-14-pdf.txt\n",
      "getting tokens: 01248 (FULL EOC FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14-pdf.txt\n",
      "getting tokens: 19223 PPO Contract (01-01-18)-pdf.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tokens: test-pdf.txt\n",
      "getting tokens: TX-19278 ASC-ENT (7.2.18)-pdf.txt\n",
      "getting tokens: 19168 Attachment B High.txt\n",
      "getting tokens: 25-1149 ASC Agreement Signed-pdf.txt\n",
      "getting tokens: TX 19015 EOC ENT(7.2.18)-pdf.txt\n",
      "getting tokens: 10041 Appendix G EOC Franklin Union (Jan2012)-pdf.txt\n",
      "getting tokens: 01255 (Full Contract) Eff. 2-1-07-pdf.txt\n",
      "getting tokens: 10041 Appendix F EOC Ardmore Union (Jan2012)-pdf.txt\n",
      "getting tokens: 19168 EOC-pdf.txt\n",
      "19168 EOC.txt\n",
      "10041 Appendix C EOCc Meridian Union (Janb2014)-pdf.txt\n",
      "TX 19015 Attachment A ENT (7.2.18).txt\n",
      "19223 Attach B (01-01-18).txt\n",
      "TX 17404 EOC Regional (7.2.18).txt\n",
      "19168 Contract-pdf.txt\n",
      "1036 Sched I 7-1-09-pdf.txt\n",
      "TX 17404 EOC (Regional (7.2.18)-pdf.txt\n",
      "01094 - SI - Eff 7-1-17 to 6-30-19-pdf.txt\n",
      "19223 PPO EBB (01-01-18)-pdf.txt\n",
      "TX-19278 ASC-ENT (7.2.18).txt\n",
      "10041 Appendix D EOCc Hampton Union (Jan2012)-pdf.txt\n",
      "01248 (FULL CONTRACT) Eff. 10-1-14-pdf.txt\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "test2.txt\n",
      "'VI'\n",
      "19194 Contract (Eff. 1-1-18).txt\n",
      "45-3310 ASC Signed-pdf.txt\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL EOC - Eff 7-1-18-pdf.txt\n",
      "19223 Attach A (01-01-18).txt\n",
      "TX 19015 Contract ENT (7.2.18).txt\n",
      "19168 Contract.txt\n",
      "test2-pdf.txt\n",
      "'VI'\n",
      "1036- sch 1 7-1-09-pdf.txt\n",
      "01094 _SBC Modification_-pdf.txt\n",
      "19223 PPO Contract (01-01-18).txt\n",
      "TX 17404 Contract Regional (7.2.18)-pdf.txt\n",
      "01255 (Schedule I and Tax Modification) January 1  2013-pdf.txt\n",
      "10041 Appendix B EOC Non-Union (Jan2018)-pdf.txt\n",
      "3310 EOC(Jan2013)-pdf.txt\n",
      "01094 EOC  7-1-16-pdf.txt\n",
      "01094 (Tax Modfication).txt\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - FULL EOC - Eff 7-1-18-pdf.txt\n",
      "TX 19015 Attachment  C ENT (7.2.18).txt\n",
      "TX 19015 Contract ENT (7.2.18)-pdf.txt\n",
      "19223 PPO EBB (01-01-18).txt\n",
      "1036- contract 7-1-09-pdf.txt\n",
      "18635 ASC Contract-pdf.txt\n",
      "1094 Full Contract (Eff. 7-1-07)-pdf.txt\n",
      "TX-18745-ASC Contract (7.2.18).txt\n",
      "TX 17404 Contract Regional (7.2.18).txt\n",
      "01255 _Schedule I_ January 1  2014-pdf.txt\n",
      "3468 EOC (RevSept2016)-pdf.txt\n",
      "10294-01002 & 02001 PPO EOC (07-01-12)-pdf.txt\n",
      "18635 ASC Contract.txt\n",
      "19168 Attachment B Low.txt\n",
      "19380 - JACKSON CHILD DEVELOPMENT CENTER INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19-pdf.txt\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "test.txt\n",
      "TX-18745 ASC Contract (7.2.18)-pdf.txt\n",
      "19194 Contract Eff. 1-1-18-pdf.txt\n",
      "19380 - JACKSON CHILDREN'S SERVICES INC - FULL CONTRACT - Eff 7-1-18 to 6-30-19-pdf.txt\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'AS'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "'VI'\n",
      "'AS'\n",
      "10041 Appendix E EOC Daingerfield Union (Jan2018)-pdf.txt\n",
      "19168 Attachment C.txt\n",
      "19168 Attachment A.txt\n",
      "TX 19015 Attachment B ENT (7.2.18).txt\n",
      "3468 EOC (RevSept2016).txt\n",
      "TX 19015 EOC ENT (7.2.18).txt\n",
      "01094 (Schedule I and Tax Modification) July 1  2013-pdf.txt\n",
      "01036 (EOC) 7-1-14-pdf.txt\n",
      "01248 (FULL EOC FOR SMITHSONIAN INSTITUTION) Eff. 10-1-14-pdf.txt\n",
      "19223 PPO Contract (01-01-18)-pdf.txt\n",
      "test-pdf.txt\n",
      "TX-19278 ASC-ENT (7.2.18)-pdf.txt\n",
      "19168 Attachment B High.txt\n",
      "25-1149 ASC Agreement Signed-pdf.txt\n",
      "TX 19015 EOC ENT(7.2.18)-pdf.txt\n",
      "10041 Appendix G EOC Franklin Union (Jan2012)-pdf.txt\n",
      "01255 (Full Contract) Eff. 2-1-07-pdf.txt\n",
      "10041 Appendix F EOC Ardmore Union (Jan2012)-pdf.txt\n",
      "19168 EOC-pdf.txt\n"
     ]
    }
   ],
   "source": [
    "errorFilePath = './data/processed/cannot_process.csv'\n",
    "errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "#base_info = None\n",
    "#pp = batchPreProcess(errorFile)\n",
    "#if pp == None:\n",
    "#    print(\"Error in pp\")\n",
    "#else:\n",
    "#    base_info = batchGetTokens()\n",
    "#    if not base_info:\n",
    "#        print(\"Error in getting tokens\")\n",
    "#    else:\n",
    "outputFilePath = './data/processed/raw_attribute_data.csv'\n",
    "outputFile=open(outputFilePath, 'w')    \n",
    "first_row = 1\n",
    "\n",
    "for file in base_info:\n",
    "    file_attr = getMetaDataAtt(file)\n",
    "    if first_row:\n",
    "        for key in file_attr:\n",
    "            outputFile.write(key + \",\")\n",
    "        outputFile.write(\"\\n\")\n",
    "        first_row = 0\n",
    "    else:\n",
    "        for attr in file_attr:\n",
    "            outputFile.write(str(file_attr[attr])+ \",\")\n",
    "        outputFile.write(\"\\n\")\n",
    "\n",
    "outputFile.close()\n",
    "errorFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Not done attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Group Information\n",
    "\n",
    "dfGI=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Diagnostic and Preventative (D&P) [Appendix A]\n",
    "\n",
    "d={'key':'D&P Services_PPO','value':'100'}\n",
    "##Need to pass index since we're only doing string values\n",
    "dfDP=pd.DataFrame(d, index=['D%P Services'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Basic Service\n",
    "\n",
    "dfBS=pd.DataFrame(d, index=['Basic Service'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Endo Perio (Endodontics (Periodontal(?)))\n",
    "\n",
    "dfEP=pd.DataFrame(d, index=['Endo Perio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Oral (Oral Surgery)\n",
    "\n",
    "dfOa=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Perio (Periodontal)\n",
    "\n",
    "dfPe=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Major (Major Benefits)\n",
    "\n",
    "dfMj=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Prostho (Prosthodontics)\n",
    "\n",
    "dfPr=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Ortho (Orthodontics)\n",
    "\n",
    "dfOt=pd.DataFrame(d, index=['uid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Concatenate all frames created by the above dataset\n",
    "\n",
    "frames = [df, dfGI, dfDP, dfBS, dfEP, dfOa, dfPe, dfMj, dfPr, dfOt]\n",
    "\n",
    "result = pd.concat(frames)\n",
    "transpose=result.transpose()\n",
    "print(transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Contract Start Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With contract start date I began by searching through the tokenized sentences with a regex expression. \n",
    "I found a datefinder module to use on each flagged sentence to pull out the dates\n",
    "\tIssue: the datefinder module works poorly on large, run -on sentences which are common in the contracts. It tends to find other numbers that aren't dates and try to make a date out of them.\n",
    "\t\n",
    "\tSol: only take a subset, starting at the flagged word\n",
    "\t\n",
    "Sometimes a match isn't found with the keywords I've seen related to the start date\n",
    "\tSol: look for keywords related to contract term and take the earlier date from that sentence\n",
    "\t\n",
    "Issue: Datefinder focusing on numbers that aren't dates\n",
    "\tSol: filter for sentences that have a year (ie four digits in a row) and dates that are before Delta Dental existed (in 1966)\n",
    "\t\n",
    "Issue: Sometimes there are multiple modes. Usually I saw this when there were equal mentions of the end date\n",
    "\tSol: if there are multiple modes, take the earliest date. \n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Testing finding the contract end date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This was much the same as the contract start date Issues\n",
    "\n",
    "\tLooking for keywords Contract Term/Contract End\n",
    "\tFiltering on invalid years\n",
    "\tFiltering on if there IS a year in the sentence (assuming it won't be written out like nineteen ninety-four)\n",
    "\tIf there are only two results, take the later one\n",
    "\tIf there are more than two, take the top two most common and then take the latter of the two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Contract Duration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Call contract start and end and try to get a duration out of them\n",
    "\n",
    " ^^^ pretty much worked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Comments so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "\tEven with trying to be variable, this won't work if they even change the wording a bit. Maybe spend some time looking into using the libraries to get synonyms.\n",
    "\t\n",
    "\tIt would also be great to get the other contracts to see exactly where we're going wrong\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Working with synonyms in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\tGetting an expanded set of search terms can be done, but I can't yet figure out how to pick the right contexts. For example, the search bigram \"contract term\" gives back a huge amount of synonyms, with only 3 or 4 actually being equivalent in meaning to \"contract term\"\n",
    "\t\n",
    "\tWe could always manually select ones that are similar but that seems to defeat the purpose: ie we could select only the noun meanings of contract\n",
    "\t\n",
    "\tWe could build our own corpus of words based on all of the documents that we have, and then compare the given synonyms to a freq distribution of those words to pick out the ways other contracts might say the same thing\n",
    "\t\tBut this would still miss things for sure\n",
    "\t\t\n",
    "\tWe could include the word type with the seed words and only choose synonyms of the same type, although this does involve more hardcoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Folder Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "data/raw\n",
    "\n",
    "data/processed/[group number - group name]/\n",
    "\n",
    "data/output\n",
    "\n",
    "*** Not every file seems to have a name, so we would have to parse the file to get it\n",
    "\n",
    "*** Additionally each num - name combo may have contracts from multiple dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Using NER tagging to identify location sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### NLTK NER tagging\n",
    "Basic NER tagging with nltk works horribly on our files out of the box\n",
    "\n",
    "#### Polyglot\n",
    "Polyglot has a lot of issues getting downloaded\n",
    "\n",
    "#### NLTK wrapper for Stanford NER\n",
    "NLTK has a wrapper for the Stanford NER tagger so I'm going to try that next\n",
    "\tDownload the model jar file\n",
    "\t\n",
    "\t\n",
    "The stanford NER tagger is working a bit better\n",
    "http://www.nltk.org/api/nltk.tag.html#nltk.tag.stanford.StanfordTagger\n",
    "https://nlp.stanford.edu/software/stanford-ner-2018-02-27.zip (download of jar files)\n",
    "https://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages\n",
    "\n",
    "The stanford one takes FOREVER though\n",
    "\tThere is a faster version in CoreNLP but that's all in Java and I don't think the wrapper interacts with it\n",
    "\n",
    "#### GeoText\n",
    "GeoText\n",
    "\n",
    "Easy to set up and use, but doesn't do states or state abbreviations\n",
    "And it misses a LOT that the NER tagger got\n",
    "\tIt is completely unreliable honestly. \n",
    "\n",
    "#### Options\n",
    "Option 1: Use the NLTK wrapper for Stanford NER tagger and just wait forever\n",
    "Option 2: Get a giant csv of all US cities/states/abbreviations/counties (exists) and make a data set out of that to compare to\n",
    "\tCons: not flexible or extensible, is already 4 years out of date\n",
    "\tPros: much faster, will only have to create the thing once\n",
    "\t\n",
    "Note: even with the NER it will only give us pieces of the address, we would still have to go into the sentence and try to regex it out\n",
    "\n",
    "Option 3: create our own trained model from some of the files we already have and see how that does with the Stanford tagger. Might be faster\n",
    "\tCould also see how it does with the native NLTK tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Function Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to Regex out a full address proves difficult\n",
    "\n",
    "You can kind of get it down to the right sentence by looking for ones that contain 'contractholder' and avoiding ones that contain 'deltadental'\n",
    "\n",
    "But it's still not perfect\n",
    "\n",
    "Tabling getting the entire address for now, I'm looking at getting the contractholder state and city\n",
    "\n",
    "Getting the state so far works okay, but there are some strange cities out there that cause issues. IE DPO is apparently a city in the US, as is Premium. These words show up enough in other locations that they interfere with trying to find the most popular ACTUAL city mentioned. Even filtering on the above keywords comes back with Premium as the city.\n",
    "\n",
    "Honestly, without a bit of work the city is completely unreliable\n",
    "\n",
    "Running into issues with filtering by keyword because the keyword is often cut off from parts of the sentence containing the location information by punctuation within the address itself\n",
    "\n",
    "\n",
    "Issues with a lot of false positives. I think it would be easier to find the location of the Delta office handling it instead of the client address, which doesn't seem to be clearly marked anywhere\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docx vs Doc issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we are using a docx specific library\n",
    "\n",
    "They gave us a bunch of doc files, docx2text can't handle those\n",
    "\n",
    "one option is to use the catdoc software and the python subprocess module\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
