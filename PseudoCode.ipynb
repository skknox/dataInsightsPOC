{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import random as rng\n",
    "import nltk\n",
    "import fnmatch\n",
    "import docx\n",
    "from lxml import etree\n",
    "\n",
    "import docx2txt\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfdevice import PDFDevice\n",
    "from pdfminer.pdfpage import PDFTextExtractionNotAllowed\n",
    "from pdfminer.layout import LAParams, LTTextBox, LTTextLine\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.collocations import *\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from string import punctuation\n",
    "import sys as sys\n",
    "from sys import platform\n",
    "import re as re\n",
    "from statistics import mode\n",
    "import datefinder\n",
    "from nltk import ne_chunk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "try:\n",
    "    if(platform == \"win32\"):\n",
    "        import win32com.client as win32\n",
    "        from win32com.client import constants\n",
    "except Exception as e:\n",
    "    print(str(e))\n",
    "\n",
    "import subprocess\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-acd307ae542a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_Path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\fileValidation\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdoc_Path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TestDocConversion.doc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwin32\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word.application\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "### Test Cell for Validating Document Conversion\n",
    "\n",
    "test_Path = \"C:\\\\fileValidation\"\n",
    "doc_Path = os.path.join(test_path, 'TestDocConversion.doc')\n",
    "try:\n",
    "    word = win32.Dispatch(\"Word.application\")\n",
    "    #word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "    doc = word.Documents.Open(doc_Path)\n",
    "    doc.Activate()\n",
    "\n",
    "    # Rename path with .docx\n",
    "    new_file_abs = os.path.abspath(doc_Path)\n",
    "    new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "\n",
    "    # Save and Close\n",
    "    word.ActiveDocument.SaveAs(\n",
    "        new_file_abs, FileFormat=16\n",
    "    )\n",
    "    doc.Close(False)\n",
    "        \n",
    "except Exception as e:\n",
    "    #doc.Close(False)\n",
    "    errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "    print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Cell for Validating Document Extraction\n",
    "test_Path = \"C:\\\\fileValidation\"\n",
    "doc_Path = os.path.join(test_path, 'TestDocXExtraction.docx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Test Cell for Validating Path Creation\n",
    "test_Path = \"C:\\\\fileValidation\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Prep Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Is it docx or pdf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFileType(filename):\n",
    "        if(filename.lower().endswith(('.docx'))):\n",
    "            return 0\n",
    "        elif(filename.lower().endswith(('.pdf'))):\n",
    "            return 1\n",
    "        elif(filename.lower().endswith(('.doc'))):\n",
    "            return 2\n",
    "        else:\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: read in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFilePath(docName):\n",
    "    raw_data_path = os.path.join(os.getcwd(), 'data', 'raw')\n",
    "    return os.path.join(raw_data_path, docName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    text = text.replace(\"\\n\", ' ')\n",
    "    text = text.replace(\"\\t\", ' ')\n",
    "    #text = text.replace(\",\", ' ')\n",
    "    \n",
    "    dblSpacesRemaining = True\n",
    "    while(dblSpacesRemaining):\n",
    "        text = text.replace(\"  \", \" \")\n",
    "        if not \"  \" in text:\n",
    "            dblSpacesRemaining = False\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: process dataFrame and group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processDF(txtFile):\n",
    "    df = pd.read_csv(txtFile, sep=\" \", header=None) #this doesn't work for me bc of variable number of cols\n",
    "    df = df.T \n",
    "    df = df.dropna()\n",
    "\n",
    "    df['SingleRow']=1\n",
    "\n",
    "    df=df.rename(columns={0 : 'Words'})\n",
    "    print(\"in processDF \" + txtFile)\n",
    "    df.describe(include=\"all\")\n",
    "    #print(df.groupby('Words').SingleRow.sum().sort_values())\n",
    "    #print(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Process a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def processDocFileWindows(filepath, errorFile):\n",
    "    try:\n",
    "        word = win32.Dispatch(\"Word.application\")\n",
    "        #word = win32.gencache.EnsureDispatch('Word.Application')\n",
    "        doc = word.Documents.Open(filepath)\n",
    "        doc.Activate()\n",
    "\n",
    "        # Rename path with .docx\n",
    "        new_file_abs = os.path.abspath(filepath)\n",
    "        new_file_abs = re.sub(r'\\.\\w+$', '.docx', new_file_abs)\n",
    "\n",
    "        # Save and Close\n",
    "        word.ActiveDocument.SaveAs(\n",
    "            new_file_abs, FileFormat=16\n",
    "        )\n",
    "        doc.Close(False)\n",
    "\n",
    "        return new_file_abs\n",
    "        \n",
    "    except Exception as e:\n",
    "        #doc.Close(False)\n",
    "        errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    \n",
    "def processDocFileMac(filepath, errorFile):\n",
    "    #textutil -convert docx ~/Desktop/mypage.webarchive\n",
    "    try:\n",
    "        subprocess.run([\"textutil\", \"-convert\", \"docx\", filepath])\n",
    "        newFilePath = re.sub(r'\\.\\w+$', '.docx', filepath)\n",
    "        #print(\"in Mac doc file process: \" + newFilePath)\n",
    "        return newFilePath\n",
    "    except Exception as e:\n",
    "        #errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "        print(\"Error processDocFileMac:  \" + str(e))\n",
    "        return None\n",
    "    \n",
    "\n",
    "def processDocFile(filePath, errorFile):\n",
    "    \n",
    "    try:\n",
    "        if(platform == \"win32\"):\n",
    "            newFilePath = processDocFileWindows(filePath, errorFile)\n",
    "        else:\n",
    "            newFilePath = processDocFileMac(filePath, errorFile)\n",
    "        #print(\"return from mac vs windows: \" + newFilePath)\n",
    "        processedFilePath = processDocxFile(newFilePath, errorFile)\n",
    "        if(processedFilePath):\n",
    "            #rmv old .doc file\n",
    "            os.remove(filePath)\n",
    "    except Exception as e:\n",
    "        #errorFile.write(filePath + \", \" + str(e) + \"\\n\")\n",
    "        print(\"Error processDocFile:  \" + str(e))\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    return processedFilePath\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def processDocxFile(filePath, errorFile):\n",
    "    #print(filePath)\n",
    "    try:\n",
    "    ######below is the footer extraction and concatenation code.\n",
    "    ######it is concatenated into a single list even when there are multiple footers.\n",
    "        docxText = docx2txt.process(filePath)\n",
    "        #print(docxText)\n",
    "        ######Pre-appending ReplacedText with Footer information\n",
    "\n",
    "        #replacedText = footer + \"\\n\" + cleanText(docxText)\n",
    "        replacedText = cleanText(docxText)\n",
    "        #print(replacedText)\n",
    "        #print(filePath)\n",
    "        fileName = os.path.split(filePath)[1]#.split('/')[-1]\n",
    "        #print(fileName)\n",
    "        fileName = fileName.replace(\",\",\"\")\n",
    "        #print(fileName)\n",
    "        baseFileName = fileName[0:-5]\n",
    "        #print(baseFileName)\n",
    "        cwd = os.getcwd()\n",
    "\n",
    "        newFilePath = os.path.join(cwd,'output',  baseFileName + \".txt\")\n",
    "\n",
    "        #print(newFilePath)\n",
    "        singleFileDocx=open(newFilePath, 'wb+')\n",
    "        singleFileDocx.write(replacedText.encode(\"utf-8\"))\n",
    "        singleFileDocx.close()\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        #errorFile.write(filePath + \", \" + str(e) + \"\\n\")\n",
    "        return None\n",
    "\n",
    "    #temp_df = processDF('singleTextDocx.txt')\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Function: Process pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def processPDFfile(filePath, errorFile):\n",
    "    password = \"\"\n",
    "    extracted_text = \"\"\n",
    "    try:\n",
    "        #print(filePath)\n",
    "        fileName = os.path.split(filePath)[1]#.split('/')[-1]\n",
    "        #print(fileName)\n",
    "        fileName = fileName.replace(\",\", \" \")\n",
    "        baseFileName = fileName[0:-4]\n",
    "    \n",
    "        fp = open(filePath, \"rb\")\n",
    "        parser = PDFParser(fp)\n",
    "        document = PDFDocument(parser, password)\n",
    "        \n",
    "        if not document.is_extractable:\n",
    "            raise PDFTextExtractionNotAllowed\n",
    "            \n",
    "        # Create PDFResourceManager object that stores shared resources such as fonts or images\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "\n",
    "        # set parameters for analysis\n",
    "        laparams = LAParams()\n",
    "\n",
    "        # Create a PDFDevice object which translates interpreted information into desired format\n",
    "        # Device needs to be connected to resource manager to store shared resources\n",
    "        # device = PDFDevice(rsrcmgr)\n",
    "        # Extract the decive to page aggregator to get LT object elements\n",
    "        device = PDFPageAggregator(rsrcmgr, laparams=laparams)\n",
    "\n",
    "        # Create interpreter object to process page content from PDFDocument\n",
    "        # Interpreter needs to be connected to resource manager for shared resources and device \n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "        # Ok now that we have everything to process a pdf document, lets process it page by page\n",
    "        for page in PDFPage.create_pages(document):\n",
    "            # As the interpreter processes the page stored in PDFDocument object\n",
    "            interpreter.process_page(page)\n",
    "            # The device renders the layout from interpreter\n",
    "            layout = device.get_result()\n",
    "            # Out of the many LT objects within layout, we are interested in LTTextBox and LTTextLine\n",
    "            for lt_obj in layout:\n",
    "                if isinstance(lt_obj, LTTextBox) or isinstance(lt_obj, LTTextLine):\n",
    "                    newText = lt_obj.get_text()\n",
    "                    newText = newText.replace('\\n', ' ')\n",
    "                    extracted_text += newText\n",
    "\n",
    "        #close the pdf file\n",
    "        fp.close()\n",
    "        \n",
    "        extracted_text = cleanText(extracted_text)#extracted_text.replace(\"\\n\", ' ')\n",
    "        cwd = os.getcwd()\n",
    "        newFilePath = os.path.join(cwd,'output', baseFileName + '-pdf' + \".txt\")\n",
    "        #print(newFilePath)\n",
    "        with open(newFilePath, 'wb+') as singleFilePDF:\n",
    "            singleFilePDF.write(extracted_text.encode(\"utf-8\"))\n",
    "        \n",
    "    except Exception as e:\n",
    "        errorFile.write(filePath + \", \" + str(e) + \"\\n\")\n",
    "        return None\n",
    "        #temp_df = processDF('./data/output/' + baseFileName + \".txt\")\n",
    "    return newFilePath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NLTK Tokenizing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: DD specific text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def ddCleanText(text):\n",
    "    newText = text.replace('Delta Dental', 'DeltaDental')\n",
    "    newText = newText.replace('DELTA DENTAL', 'DELTADENTAL')\n",
    "    newText = newText.replace('DeltaDental Insurance Company', 'DeltaDentalInsuranceCompany')\n",
    "    return newText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    customStopWords = set(stopwords.words('english')+list(punctuation))\n",
    "    wordsWOStop=[word for word in words if word not in customStopWords]\n",
    "    \n",
    "    return wordsWOStop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Make tokenized Sentence list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSents(text):\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getBigrams(tokens):\n",
    "    bigram_measures=nltk.collocations.BigramAssocMeasures();\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    sorted_bgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "    \n",
    "    return sorted_bgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getTrigrams(tokens):\n",
    "    trigram_measures =nltk.collocations.TrigramAssocMeasures();\n",
    "    finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    sorted_tgs = (sorted(list(finder.ngram_fd.items()), key=lambda item: item[-1], reverse=True))\n",
    "\n",
    "    return sorted_tgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Get Footer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFooter(txtFile):\n",
    "    if not(\"-pdf.\" in txtFile):\n",
    "        docxFileName = (os.path.split(txtFile)[1]).replace(\".txt\",\".docx\")\n",
    "        #print(docxFileName)\n",
    "        filePath = os.path.join(os.getcwd(),\"processed\",docxFileName)\n",
    "        #print(filePath)\n",
    "        doc = docx.Document(filePath)\n",
    "        footerXML = [x.blob.decode() for x in doc.part.package.parts if x.partname.find('footer')>0]\n",
    "        #print(len(footerXML))\n",
    "        #print(type(footerXML))\n",
    "        #print(footerXML)\n",
    "        footer = []\n",
    "        for i in range(0,len(footerXML)):\n",
    "            root = etree.XML(footerXML[i].split(\"\\n\",1)[1].replace(\"w:\", \"\"))\n",
    "            footer.append('')\n",
    "            for p in root:\n",
    "                for r in p:\n",
    "                #print(r.get(\"t\"))\n",
    "                    for t in r:\n",
    "                        if(t.tag == \"t\"):\n",
    "                            footer[i] = footer[i] + t.text\n",
    "\n",
    "        return footer\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## MetaData and Attribute Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### NLTK synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: translate from syn POS to nltk POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Use POS to cull wordnet synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonyms_usingPOS(word_tuple):\n",
    "    #print(word_tuple)\n",
    "    word_tagged = word_tuple[0]\n",
    "    word_pos = get_wordnet_pos(word_tuple[1])\n",
    "    syns = wn.synsets(word_tagged, pos=word_pos)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    #print(syns)\n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get synonyms of a single word. Helper function to Bigram and Trigram function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## You can't cull this one down with the POS b/c you can't tag a single word\n",
    "def getSyns(word):\n",
    "    syns1 = wn.synsets(word)\n",
    "    \n",
    "    set1 = set()\n",
    "    for syn in syns1:\n",
    "        for lem in syn.lemmas():\n",
    "            set1.add(lem.name())\n",
    "    \n",
    "    return set1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get a similar bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarBigrams(word1, word2):\n",
    "    \n",
    "    tagged_words = nltk.pos_tag([word1,word2])\n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            word_set.add(\" \".join([word1, word2]))\n",
    "    \n",
    "    return word_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get a similar trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSimilarTrigrams(word1, word2, word3):\n",
    "    tagged_words = nltk.pos_tag([word1,word2,word3])\n",
    "    \n",
    "    set1 = getSynonyms_usingPOS(tagged_words[0])\n",
    "    if not len(set1):\n",
    "        set1.add(word1)\n",
    "    set2 = getSynonyms_usingPOS(tagged_words[1])\n",
    "    if not len(set2):\n",
    "        set2.add(word2)\n",
    "    set3 = getSynonyms_usingPOS(tagged_words[2])\n",
    "    if not len(set3):\n",
    "        set3.add(word3)\n",
    "    \n",
    "    word_set = set()\n",
    "    for word1 in set1:\n",
    "        for word2 in set2:\n",
    "            for word3 in set3:\n",
    "                word_set.add(\" \".join([word1, word2, word3]))\n",
    "    #print(word_set)\n",
    "    \n",
    "    return word_set\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Get synonyms from a list of key words. Returns more keywords/phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getSynonymsFromList(keywords):\n",
    "    matches = []\n",
    "\n",
    "    for kw in keywords:\n",
    "        try:\n",
    "            words = word_tokenize(kw)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        #print(words)\n",
    "        if(len(words) == 1):\n",
    "            #print(\"is 1\")\n",
    "            syns = getSyns(words[0])\n",
    "            for syn in syns:\n",
    "                matches.append(syn)\n",
    "            #keywords.append(list(syns))\n",
    "        elif(len(words) == 2):\n",
    "            #print(\"is 2\")\n",
    "            syns = getSimilarBigrams(words[0],words[1])\n",
    "            #print(syns)\n",
    "            matches.extend(getSimilarBigrams(words[0],words[1]))\n",
    "        elif(len(words) == 3):\n",
    "            #print(\"is 3\")\n",
    "            matches.extend(getSimilarTrigrams(words[0],words[1],words[2]))\n",
    "        else:\n",
    "            print(\"keyword string too long\")\n",
    "        #print(matches)\n",
    "    keywords.extend(matches)\n",
    "    keywords = set(keywords)\n",
    "\n",
    "    #print(start_keywords)\n",
    "    \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get file name\n",
    "Get the filename from a full path. Determines the OS and splits the string correctly based on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFileName(fullPath):\n",
    "    try:\n",
    "        if(platform == \"win32\"):\n",
    "            fileName = fullPath.split(\"\\\\\")[-1]\n",
    "        else:\n",
    "            fileName = fullPath.split(\"/\")[-1]\n",
    "        return fileName\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get group number\n",
    "Uses regex's made from keywords to attempt to find a group number in the file. Failing that, it searches the filename for the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getGroupNumber(sents_tokens, filePath):\n",
    "    \n",
    "    group_keywords = [\"group number\", \"groupnumber\"]\n",
    "    regex_exps = []\n",
    "    poss_nums = []\n",
    "    finalGN = None\n",
    "    \n",
    "    try:\n",
    "        #Create regex exps out of group number keywords\n",
    "        for kw in group_keywords:\n",
    "                temp_re = kw + \"\\W\\s*(?P<gn>\\d+)(?P<gn2>[-\\s]\\d+)\"\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "        #For each sentence, search for the expression, if found add the number to\n",
    "        #list of possible group numbers\n",
    "        for sent in sents_tokens:\n",
    "            #print(sent)\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    temp_gn = result.group('gn')\n",
    "                    temp_gn2 = result.group('gn2')\n",
    "                    if(temp_gn2):\n",
    "                        if(len(temp_gn2[1:])>len(temp_gn)):\n",
    "                            poss_nums.append(temp_gn2[1:])\n",
    "                        else:\n",
    "                            poss_nums.append(temp_gn)\n",
    "                    else:\n",
    "                        #temp_gn2 = temp_gn2[1:]\n",
    "                        #print(temp_gn2)\n",
    "                        \n",
    "                        poss_nums.append(temp_gn)\n",
    "\n",
    "        #Try and get the number from the file name, looking for list of numeric chars\n",
    "        num_regex = re.compile(\"(?P<gn>\\d+)(?P<gn2>[-\\s]+\\d+)?\")\n",
    "        fileName = getFileName(filePath)\n",
    "        fileGN = num_regex.search(fileName)\n",
    "\n",
    "        if not fileGN==None:#if they filename has a number sequence\n",
    "            temp_gn = None\n",
    "            if(fileGN.group('gn2')):\n",
    "                if(len(fileGN.group('gn'))>=len(fileGN.group('gn2')[1:])):\n",
    "                    temp_gn = fileGN.group('gn')\n",
    "                else:\n",
    "                    temp_gn = fileGN.group('gn2')[1:]\n",
    "\n",
    "            else:\n",
    "                temp_gn = fileGN.group()\n",
    "            if (temp_gn in poss_nums):#then if the file group number matches one in the document, choose it\n",
    "                finalGN = temp_gn\n",
    "            else:\n",
    "                poss_nums.append(temp_gn)#otherwise add the filename one to the list and try to get the most co\n",
    "                try:\n",
    "                    finalGN = mode(poss_nums)\n",
    "                except Exception as e:\n",
    "                    return -1\n",
    "        else: #it is none and there was no group number in the filename\n",
    "            try:\n",
    "                finalGN = mode(poss_nums)\n",
    "            except Exception as e:\n",
    "                #no mode found, couldn't find a group number\n",
    "                return -1\n",
    "    except Exception as e:\n",
    "        #print(str(e))\n",
    "        return None\n",
    "    \n",
    "    return finalGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get contract start\n",
    "Uses regex and a list of keywords to attempt to find the start date of the contract. It makes multiple passes based on patterns seen in contract samples so far.\n",
    "\n",
    "Some of the passes are necessary to filter out non-date numbers that the datefinder incorrectly parses to dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractStart(sents_tokens):\n",
    "    start_keywords = [\"effective date\\S\\s*\\S\", \"effective\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:20]\n",
    "                    subset = \" \".join(subset)\n",
    "                    poss_dates.append(subset)\n",
    "                    if \"effective date\" in subset.lower():\n",
    "                        poss_dates.append(subset)\n",
    "\n",
    "                \n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\"]\n",
    "    \n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:6])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-6:])\n",
    "                    subset = subset_2 + subset_1\n",
    "\n",
    "                    m = datefinder.find_dates(subset, strict=True)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "                    \n",
    "    ## Second pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "            #print(sent)\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.search(sent)\n",
    "\n",
    "            if not year==None:\n",
    "                #print(sent)\n",
    "                #print(year.group())\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                for match in m:\n",
    "                    if match.year >= 1966:\n",
    "                        matches.append(match)\n",
    "## Last pass: try to find the most common date. If there is more than one mode, choose the earliest date\n",
    "##.           this seems to occur when it is finding the contract start and end in equal quantities\n",
    "        #print(matches)\n",
    "        try:\n",
    "            finalDate = mode(matches)\n",
    "        except ValueError as e:\n",
    "            #print(str(e))\n",
    "            if matches:\n",
    "                earliestMatch = matches[0]\n",
    "                for match in matches:\n",
    "                    if(match < earliestMatch):\n",
    "                        earliestMatch = match\n",
    "                finalDate = earliestMatch\n",
    "            else:\n",
    "                finalDate = datetime.datetime(1066, 1, 1)\n",
    "        except Exception as e:\n",
    "            return None #i.e. not only could they not find a start date, something failed\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "####  get Contract End\n",
    "Similar to get contract start, it uses regex and keywords over multiple passes to attempt and find the contract end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractEnd(sents_tokens):\n",
    "    \n",
    "    start_keywords = [\"contract term\\S\\s*\\S\", \"contract term \", \"contract end\", \"termination date\"]\n",
    "    #start_keywords = [\"termination date\"]\n",
    "    regex_exps = []\n",
    "    poss_dates = []\n",
    "    finalDate = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "    ## Original pass through sentence tokens to find possible dates based on keywords\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    subset = word_tokenize(sent[sent.lower().find(result.group()):])[:30]\n",
    "                    #print(subset[2])\n",
    "                    if not (subset[2]=='beginning'):\n",
    "                        subset = \" \".join(subset)\n",
    "                       # print(subset)\n",
    "                        poss_dates.append(subset)\n",
    "\n",
    "\n",
    "    ## Second pass through sentence tokens to find possible dates based on a date range format\n",
    "        regex_exps = []\n",
    "        backup_kw = [\"\\S\\sthrough\\s\\S\",\"\\S\\sthru\\s\\S\", \"\\d\\d\\d\\d\\sto\\s\\S\"]\n",
    "\n",
    "        for kw in backup_kw:\n",
    "            temp_re = kw\n",
    "            regex = re.compile(temp_re)\n",
    "            regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    half_1 = sent[sent.lower().find(result.group()):]\n",
    "                    half_2 = sent[:sent.lower().find(result.group())]\n",
    "\n",
    "                    subset_1 = \" \".join(word_tokenize(half_1)[:12])\n",
    "                    subset_2 = \" \".join(word_tokenize(half_2)[-12:])\n",
    "                    subset = subset_2 + subset_1\n",
    "                    #print(subset)\n",
    "                    m = datefinder.find_dates(subset, strict=True)\n",
    "                    temp_matches = []\n",
    "                    for match in m:\n",
    "                        #print(match)\n",
    "                        if match.year >= 1966:\n",
    "                            temp_matches.append(subset)\n",
    "                    if len(temp_matches)>=2:\n",
    "                        \n",
    "                        poss_dates.append(subset)\n",
    "                        #print(subset)\n",
    "\n",
    "\n",
    "    ## Pass through sentences with possible dates to eliminate ones without a year or with an invalid year\n",
    "    ## These are likely other values flagged incorrectly as dates by the datefinder\n",
    "    ## 1966 is the year Delta Dental was created\n",
    "        for sent in poss_dates:\n",
    "            #print(sent)\n",
    "            find_year_re = re.compile(\"\\d\\d\\d\\d\")\n",
    "            year = find_year_re.findall(sent)\n",
    "            #print(year)\n",
    "            if len(year)>=2:\n",
    "                #print(year)\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                maxMatch = datetime.datetime(1066,1,1)\n",
    "                if not m == None:\n",
    "                    #print(m)\n",
    "                    for match in m:\n",
    "                        if match > maxMatch:\n",
    "                            maxMatch = match\n",
    "                    if maxMatch.year >= 1966:\n",
    "                        matches.append(match)\n",
    "                        #print(matches)\n",
    "            elif len(year)==1:\n",
    "                find_year_re = re.compile(\"termination date\")\n",
    "                valid_sent = find_year_re.findall(sent.lower())\n",
    "           #     print(sent)\n",
    "                m = datefinder.find_dates(sent, strict=True)\n",
    "                if not m == None:\n",
    "          #          print(m)\n",
    "                    for match in m:\n",
    "                        if match.year >= 1966:\n",
    "                          #  print(matches)\n",
    "                            matches.append(match)\n",
    "\n",
    "        #if(len(matches)):\n",
    "        #    print(matches)\n",
    "\n",
    "    ### If there are exactly two matches, try to find a max. If error b/c they're the same, choose one\n",
    "        if(len(matches) == 2):\n",
    "            try:\n",
    "                finalDate = max(matches)\n",
    "            except ValueError as e:\n",
    "                finalDate = matches[0]\n",
    "            except Exception as e:\n",
    "                return None\n",
    "\n",
    "    ## If there are more, try and find the top two most mentioned and take the later. else just take the latest            \n",
    "        elif(len(matches) > 2):\n",
    "\n",
    "            try:\n",
    "                date1 = mode(matches)\n",
    "                matches.remove(date1)\n",
    "\n",
    "                date2 = mode(matches)\n",
    "                matches.remove(date2)\n",
    "\n",
    "                finalDate = max([date1, date2])\n",
    "            except ValueError as e:\n",
    "                #print(str(e))\n",
    "                if matches:\n",
    "                    latestMatch = matches[0]\n",
    "                    for match in matches:\n",
    "                        if(match > latestMatch):\n",
    "                            latestMatch = match\n",
    "                    finalDate = latestMatch\n",
    "            except Exception as e:\n",
    "                return None\n",
    "        else:\n",
    "            finalDate = mode(matches)\n",
    "            #return datetime.datetime(1066, 1, 1)\n",
    "            #print(\"could not find contract end for file\")\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    #print(\"\\n\")\n",
    "    return finalDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get Contract Duration\n",
    "Uses the functions getContractStart and getContractEnd to calculate a duration if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getContractDuration(sents_tokens):\n",
    "    start = None\n",
    "    end = None\n",
    "    duration = None\n",
    "    \n",
    "    try:\n",
    "        start = getContractStart(sents_tokens)\n",
    "        end = getContractEnd(sents_tokens)\n",
    "        \n",
    "        if(start and end):\n",
    "            if (start.year==1066) or (end.year == 1066):\n",
    "                return -1\n",
    "            else:\n",
    "                duration = (end - start).days\n",
    "                if duration <= 0:\n",
    "                    return -1\n",
    "        else:\n",
    "            return -1\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### get State/Location: -- Not Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Helper function to create location data set from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def makeLocationDataStruct():\n",
    "    categories = []\n",
    "    location_data = {}\n",
    "    \n",
    "    us_filename = 'us_cities_states_counties.csv'\n",
    "    cwd = os.getcwd()\n",
    "    filepath = os.path.join(cwd, us_filename)\n",
    "    #print(filepath)\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter='|',escapechar=',')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                for cat in row:\n",
    "                    categories.append(cat)\n",
    "                    location_data[cat]=set()\n",
    "                category_row = 0\n",
    "            else:\n",
    "                \n",
    "                for item in range(len(row)):\n",
    "                    #print(row[item])\n",
    "                    if len(row[item]):\n",
    "                        location_data[categories[item]].add(row[item])\n",
    "        location_data['State full'].add(\"Washington , DC\")\n",
    "        location_data['State full'].add(\"Washington , D.C.\")\n",
    "    csvfile.close()\n",
    "    \n",
    "    states_filename = 'state_abbrv_to_name.csv'\n",
    "    filepath = os.path.join(cwd, states_filename)\n",
    "    \n",
    "    location_data['translate_s2l'] = {}\n",
    "    location_data['translate_l2s'] = {}\n",
    "    \n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        category_row = 1\n",
    "        for row in spamreader:\n",
    "            if category_row:\n",
    "                category_row = 0\n",
    "            else:\n",
    "                location_data['translate_s2l'][row[0]] = row[1]\n",
    "                location_data['translate_l2s'][row[1]] = row[0]\n",
    "    csvfile.close()\n",
    "    #print(location_data)\n",
    "    return location_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Call function to set up location data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "location_data = makeLocationDataStruct()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Get Client Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def checkIfCity(loc_str, loc_data, isDelta, isContractholder):\n",
    "    cities = []\n",
    "    \n",
    "    if (loc_str in loc_data['City']):\n",
    "        cities.append(loc_str)\n",
    "    return cities\n",
    "\n",
    "def checkIfState(loc_str, loc_data):\n",
    "    states = []\n",
    "    \n",
    "    if(loc_str in loc_data['State full']):\n",
    "        states.append(loc_str.lower())\n",
    "    if(loc_str in location_data['State short']):\n",
    "        try:\n",
    "            states.append(location_data['translate_s2l'][loc_str].lower())\n",
    "        except Exception as e:\n",
    "            return states\n",
    "    return states\n",
    "    \n",
    "def getClientLocation(sents_tokens, bgs, tgs, location_data, filename):\n",
    "    loc_sents = set()\n",
    "    cities = []\n",
    "    states = []\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        for sent in sents_tokens:\n",
    "\n",
    "            sent_cities = []\n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to contractholder\" in sent.lower()\n",
    "            DCtext = [\"washington , d.c.\",\"washington , dc\", \"washington dc\",\"district of columbia\",\"washington d.c.\"]\n",
    "\n",
    "            #Used to filter out sentences in ALL caps. They interfere with the Abbreviated States lists\n",
    "            if sent.isupper():\n",
    "                sent = sent.lower()\n",
    "\n",
    "\n",
    "            text = nltk.word_tokenize(sent)\n",
    "\n",
    "            i = 0\n",
    "\n",
    "            while (i < len(text)) and (len(text) > 2):\n",
    "                if(i < len(text)-2):\n",
    "                    text_bg = \" \".join([text[i], text[i+1]])\n",
    "                else:\n",
    "                    text_bg = \"\"\n",
    "                if(i < len(text)-2):\n",
    "                    text_tg = \" \".join([text[i], text[i+1], text[i+2]])\n",
    "                else:\n",
    "                    text_tg = \"\"\n",
    "\n",
    "                sent_cities.extend(checkIfCity(text[i], location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_bg, location_data, isDelta, isContractholder))\n",
    "                sent_cities.extend(checkIfCity(text_tg, location_data, isDelta, isContractholder))\n",
    "\n",
    "                sent_states.extend(checkIfState(text[i], location_data))\n",
    "                sent_states.extend(checkIfState(text_bg, location_data))\n",
    "                sent_states.extend(checkIfState(text_tg, location_data))\n",
    "\n",
    "                i+=1\n",
    "\n",
    "            if (len(sent_states)>0) and not isDelta:\n",
    "                \n",
    "                if bool(set(sent_states).intersection(DCtext)):\n",
    "                    \n",
    "                    for st in sent_states:\n",
    "                        if st in DCtext:\n",
    "                            states.append(st.lower())\n",
    "                            if(isNotice):\n",
    "                                states.append(st.lower())\n",
    "                           \n",
    "                short_sent_states = []\n",
    "                for st in sent_states:\n",
    "                    try:\n",
    "                        short_sent_states.append(location_data['translate_l2s'][st.title()].lower())\n",
    "                    except Exception as e:\n",
    "                        e=e\n",
    "\n",
    "                for city in sent_cities:\n",
    "                    for state in sent_states+short_sent_states:\n",
    "                        \n",
    "                        if (len(checkIfState(city + \" \" + state.title(), location_data))==0) and not (city is \"New York\"):\n",
    "                                \n",
    "                                add_regex_str = str(city) + \"[-,\\s]+\" + str(state) + \"[-,\\s]\"\n",
    "                                add_regex = re.compile(add_regex_str, re.IGNORECASE)\n",
    "                                matches = add_regex.findall(sent)\n",
    "                                if matches:\n",
    "                                    cities.append(city)\n",
    "                                    states.append(state.lower())\n",
    "                                    if(isNotice):\n",
    "                                        states.append(state.lower())\n",
    "                                    \n",
    "            \n",
    "        FN_chunks = re.findall(r\"[\\w]+|[-\\s_]\", filename)\n",
    "        \n",
    "        for chunk in FN_chunks:\n",
    "            FN_state = checkIfState(chunk, location_data)\n",
    "            if len(FN_state)>0:\n",
    "                for i in range(0,5):\n",
    "                    states.append(FN_state[0].lower())\n",
    "                    i+=1\n",
    "                \n",
    "              \n",
    "        try:\n",
    "            for st in states:\n",
    "                st = st.lower()\n",
    "            final_state = mode(states)\n",
    "            #print(final_state)\n",
    "            if(len(final_state)<3):\n",
    "                try:\n",
    "                    final_state = location_data['translate_s2l'][final_state.upper()].lower()\n",
    "                except Exception as e:\n",
    "                    #print(str(e))\n",
    "                    final_state = final_state\n",
    "        except ValueError as ve:\n",
    "            #print(str(ve))\n",
    "            if(len(states)>0):\n",
    "                for st in states:\n",
    "                    st = st.lower()\n",
    "                try:\n",
    "                    final_state = mode(states)\n",
    "                except Exception as e:\n",
    "                    final_state = \"not_a_state\"\n",
    "            else:\n",
    "                final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(str(e))\n",
    "            final_state = None\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None #error in function execution\n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### To keep: address regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#    isDelta = \"deltadental\" in sent.lower()\n",
    "            #    isContractholder = \"contractholder\" in sent.lower()\n",
    "            #    if not isDelta and isContractholder:\n",
    "                   # print(sent)\n",
    "                   # print(\"\\n\")\n",
    "                    #[aA]ddress[-: ]\n",
    "                    #\"\\s+\\d+\\D+\" + \"[\"+\"|\".join(cities) + \"]\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "            #        add_regex_str = \"[aA]ddress[-: ]\\s+\\d+[\\D\\d]+\" + \"[\"+\"|\".join(sent_cities) + \"]*\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]*\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "                   # print(add_regex_str)\n",
    "            #        add_regex = re.compile(add_regex_str)\n",
    "                   # print(str(sent_cities))\n",
    "                  #  print(sent_states)\n",
    "            #        matches = add_regex.findall(sent)\n",
    "                    #print(matches)\n",
    "                    #if matches:\n",
    "                     #   for m in matches:\n",
    "                   #         print(m)\n",
    "                   # else:\n",
    "                   #     add_regex_str = \"\\s+\\d+[\\D\\d]+\" + \"[\"+\"|\".join(sent_cities) + \"]*\" \"\\s+\" + \"[\" + \"|\".join(sent_states) + \"]*\" + \"\\s+\\d\\d\\d\\d\\d\"\n",
    "                   #     add_regex = re.compile(add_regex_str)\n",
    "                   #     matches = add_regex.findall(sent)\n",
    "                        #loc_sents.add(matches.group())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Delta Office Involved -- Not Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getDeltaOffice(sents_tokens, bgs, tgs, location_data):\n",
    "    loc_sents = set()\n",
    "    cities = []\n",
    "    states = []\n",
    "    \n",
    "    try:\n",
    "        for sent in sents_tokens:\n",
    "           \n",
    "            sent_cities = []\n",
    "            sent_states = []\n",
    "\n",
    "            isDelta = \"deltadental\" in sent.lower()\n",
    "            isContractholder = \"contractholder\" in sent.lower()\n",
    "            isNotice = \"notice to delta dental\" in sent.lower()\n",
    "            DCtext = [\"washington , d.c.\",\"washington , dc\", \"washington dc\",\"district of columbia\",\"washington d.c.\"]\n",
    "\n",
    "\n",
    "\n",
    "            if isDelta:\n",
    "                \n",
    "                if sent.isupper():\n",
    "                    sent = sent.lower()\n",
    "                    \n",
    "                text = nltk.word_tokenize(sent)\n",
    "\n",
    "                i = 0\n",
    "\n",
    "                while (i < len(text)) and (len(text) > 2):\n",
    "                    if(i < len(text)-2):\n",
    "                        text_bg = \" \".join([text[i], text[i+1]])\n",
    "                    else:\n",
    "                        text_bg = \"\"\n",
    "                    if(i < len(text)-2):\n",
    "                        text_tg = \" \".join([text[i], text[i+1], text[i+2]])\n",
    "                    else:\n",
    "                        text_tg = \"\"\n",
    "\n",
    "                    sent_cities.extend(checkIfCity(text[i], location_data, isDelta, isContractholder))\n",
    "                    sent_cities.extend(checkIfCity(text_bg, location_data, isDelta, isContractholder))\n",
    "                    sent_cities.extend(checkIfCity(text_tg, location_data, isDelta, isContractholder))\n",
    "\n",
    "                    sent_states.extend(checkIfState(text[i], location_data))\n",
    "                    sent_states.extend(checkIfState(text_bg, location_data))\n",
    "                    sent_states.extend(checkIfState(text_tg, location_data))\n",
    "\n",
    "                    i+=1\n",
    "\n",
    "                if (len(sent_states)>0):\n",
    "                    if bool(set(sent_states).intersection(DCtext)):\n",
    "                        \n",
    "                        for st in sent_states:\n",
    "                            if st in DCtext:\n",
    "                                states.append(st)\n",
    "                                if(isNotice):\n",
    "                                    states.append(st)\n",
    "                                \n",
    "                    short_sent_states = []\n",
    "                    for st in sent_states:\n",
    "                        try:\n",
    "                            short_sent_states.append(location_data['translate_l2s'][st.title()].lower())\n",
    "                        except Exception as e:\n",
    "                            e=e\n",
    "                           \n",
    "                    for city in sent_cities:\n",
    "                        for state in sent_states+short_sent_states:\n",
    "                            if (len(checkIfState(city + \" \" + state.title(), location_data))==0) and not (city is \"New York\"):\n",
    "\n",
    "                                add_regex_str = str(city) + \"[-,\\s]+\" + str(state) + \"[-,\\s]\"\n",
    "                                add_regex = re.compile(add_regex_str, re.IGNORECASE)\n",
    "                                matches = add_regex.findall(sent)\n",
    "                                if matches:\n",
    "                                    cities.append(city)\n",
    "                                    states.append(state)\n",
    "                                    if(isNotice):\n",
    "                                        states.append(state)\n",
    "                                   \n",
    "        try:\n",
    "            final_state = mode(states)\n",
    "            if(len(final_state)<3):\n",
    "                try:\n",
    "                    final_state = location_data['translate_s2l'][final_state.upper()]\n",
    "                except:\n",
    "                    final_state = final_state\n",
    "        except ValueError as ve:\n",
    "            final_state = \"not_a_state\"\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            final_state = None\n",
    "        try:\n",
    "            final_city = mode(cities)\n",
    "            #print(final_city)\n",
    "        except ValueError as ve:\n",
    "            final_city = \"not_a_city\"\n",
    "        except Exception as e:\n",
    "            #print(str(e))\n",
    "            final_city = None\n",
    "\n",
    "    except Exception as e:\n",
    "        final_state = None\n",
    "        \n",
    "    return final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Get contractholder name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def longestSubstring(filename, sent):\n",
    "    filename = filename.replace(\"docx\", \"\")\n",
    "    filename = filename.replace(\"pdf\", \"\")\n",
    "    filename = filename.replace(\"txt\", \"\")\n",
    "    filename = re.sub(r'[-,()._]', r' ',filename)\n",
    "    filename = re.sub(r'\\d+', r' ', filename)\n",
    "    #print(filename)\n",
    "    fn_words = nltk.word_tokenize(filename.lower())\n",
    "    #print(fn_words)\n",
    "    \n",
    "    sent = sent.replace(\"docx\", \"\")\n",
    "    sent = sent.replace(\"pdf\", \"\")\n",
    "    sent = sent.replace(\"txt\", \"\")\n",
    "    sent = re.sub(r'[-,()._]', r' ',sent)\n",
    "    sent = re.sub(r'\\d+', r' ', sent)\n",
    "    sent_words = nltk.word_tokenize(sent.lower())\n",
    "    matches = []\n",
    "    match = []\n",
    "    i=0\n",
    "    j=0\n",
    "    \n",
    "    while(i < len(fn_words)):\n",
    "        while(j < len(sent_words)):\n",
    "            if fn_words[i] == sent_words[j]:\n",
    "                #match found\n",
    "                #match.append(fn_words[i])\n",
    "                m=i\n",
    "                k=j\n",
    "                while((m<len(fn_words)) and (k<len(sent_words)) and (fn_words[m] == sent_words[k])):\n",
    "                    match.append(fn_words[m])\n",
    "                    m+=1\n",
    "                    k+=1\n",
    "                if(len(match)>1):\n",
    "                    #re.match(r\"hello[0-9]+\", 'hello1')\n",
    "                    file_kw = [r'schedule\\s\\w{1,2}', r'attachment\\s[a-zA-Z]', r'appendix\\s[a-zA-Z]']\n",
    "                    falseMatch = 0\n",
    "                    for fm in file_kw:\n",
    "                        if(re.match(fm, \" \".join(match))):\n",
    "                            falseMatch = 1\n",
    "                    if not falseMatch:\n",
    "                        matches.append(match)\n",
    "                match = []\n",
    "                j+=1\n",
    "            else:\n",
    "                j+=1\n",
    "        j=0\n",
    "        i+=1\n",
    "            \n",
    "        \n",
    "    longest_match = []\n",
    "    if(len(matches)):\n",
    "        #print(matches)\n",
    "        \n",
    "        for m in matches:\n",
    "            m = \" \".join(m)\n",
    "            if(len(m) > len(longest_match)):\n",
    "                longest_match = m\n",
    "        \n",
    "        return longest_match\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def getContractholder(sents_tokens, fileName):\n",
    "    #start_keywords = [\"contractholder\\s?[-:]+\\s?(?P<name>\\w+)\"]\n",
    "    start_keywords = [\"contractholder name\\s?[-:]+\\s+(?P<name>[\\w\\s]+)[gG]roup\\s?[nN]umber\", \"contractholder\\s?[-:]\\s+(?P<name>[\\w\\s]+)[gG]roup\\s?[nN]umber\"]\n",
    "    regex_exps = []\n",
    "    poss_names = []\n",
    "    finalName = \"\"\n",
    "    matches = []\n",
    "    \n",
    "    try:\n",
    "        #start_keywords = getSynonymsFromList(start_keywords)\n",
    "        #print(start_keywords)\n",
    "        for kw in start_keywords:\n",
    "                temp_re = kw\n",
    "                regex = re.compile(temp_re)\n",
    "                regex_exps.append(regex)\n",
    "\n",
    "        for sent in sents_tokens:\n",
    "            for my_regex in regex_exps:\n",
    "                result = my_regex.search(sent.lower())\n",
    "                if not result==None:\n",
    "                    #subset = word_tokenize(sent[sent.lower().find(result.group('name')):])\n",
    "                    subset = result.group('name')\n",
    "                    #subset = \" \".join(subset)\n",
    "                    if not (subset.startswith('the employer')):\n",
    "                        poss_names.append(subset)\n",
    "            \n",
    "            \n",
    "            \n",
    "        for sent in sents_tokens[0:5]:\n",
    "            #print(sent)\n",
    "            substring = longestSubstring(fileName, sent)\n",
    "            if substring:\n",
    "                poss_names.append(substring)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #if(len(poss_names)>0):\n",
    "            #print(poss_names)\n",
    "        try:\n",
    "            finalName = mode(poss_names)\n",
    "            #print(finalName)\n",
    "        except ValueError as e:\n",
    "            e=e\n",
    "            #print(str(e))\n",
    "            #if matches:\n",
    "                #earliestMatch = matches[0]\n",
    "                #for match in matches:\n",
    "                #    if(match < earliestMatch):\n",
    "                #        earliestMatch = match\n",
    "                #finalDate = earliestMatch\n",
    "            #else:\n",
    "            #    finalDate = datetime.datetime(1066, 1, 1)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            return None #i.e. not only could they not find a start date, something failed\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "    return finalName\n",
    "\n",
    "\n",
    "#for file in base_info:\n",
    " #   fileName = os.path.split(file['filepath'])[1]\n",
    "  #  print(fileName)\n",
    "   # contractholderName = getContractholder(file['sentTokens'], fileName)\n",
    "    #if(contractholderName):\n",
    "     #   print(contractholderName)\n",
    "      #  print(\"\\n\")\n",
    "    #for sent in file['sentTokens'][0:5]:\n",
    "        #print(sent)\n",
    "        ##longestSubstring(fileName, sent)\n",
    "    #longestSubstring(fileName, \"test string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### File type functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def isEnterprise(fileName, footer):\n",
    "\n",
    "    isEnterprise = 0\n",
    "\n",
    "    #print(fileName)\n",
    "    enterpriseList = [\"-ENT\",\"ENT-\",\"E-\"]\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            #print(f)\n",
    "            if(any([x in f for x in enterpriseList])):\n",
    "                isEnterprise = 1\n",
    "\n",
    "    #print(\"\\n\")\n",
    "    return isEnterprise\n",
    "\n",
    "def isASC(filePath, footer):\n",
    "    isASC = 0\n",
    "\n",
    "    ascList = [\"ASC-\", \"-ASC\",\"ASO-\",\"-ASO\",\" ASC\", \"ASC \",\" ASO\", \"ASO \"]\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in ascList])):\n",
    "                isASC = 1\n",
    "\n",
    "    return isASC\n",
    "\n",
    "\n",
    "def isEOC(filePath, footer):\n",
    "    isEOC = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "\n",
    "    eocList = [\"EOC-\", \"-EOC\"]\n",
    "    eocRegex = [\"[- (]EOC\"]\n",
    "    for r in eocRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isEOC = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in eocList])):\n",
    "                isEOC = 1\n",
    "\n",
    "    return isEOC\n",
    "\n",
    "def isEBB(filePath, footer):\n",
    "    isEBB = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "\n",
    "    ebbList = [\"EBB-\", \"-EBB\"]\n",
    "    ebbRegex = [\"[- (]EBB\"]\n",
    "    for r in ebbRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isEBB = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in ebbList])):\n",
    "                isEBB = 1\n",
    "\n",
    "    return isEBB\n",
    "\n",
    "def isSchedule(filePath, footer):\n",
    "    isSchedule = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    schRegex = [\"[sS]chedule\\s+I\",\"[ (]S[I]+[) ]\",\"[sS]ch[ed]*[ ]+[I12]+\"]\n",
    "    if \"schedule\" in filename.lower():\n",
    "        isSchedule = 1\n",
    "        #print(\"in filename\")\n",
    "    else:\n",
    "        for r in schRegex:\n",
    "            r = re.compile(r)\n",
    "            results = r.search(filename)\n",
    "            if results:\n",
    "                isSchedule = 1\n",
    "                #print(filename)\n",
    "                #print(results.group())\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    return isSchedule\n",
    "\n",
    "def isContract(filePath, footer):\n",
    "    isContract = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    #schRegex = [\"[cC]ontract\",\"CONTRACT\"]\n",
    "    if \"contract\" in filename.lower():\n",
    "        isContract = 1\n",
    "        #print(\"in filename\")\n",
    "        #print(\"\\n\")\n",
    "\n",
    "    return isContract\n",
    "\n",
    "def isAttachment(filePath, footer):\n",
    "    isAttachment = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    attachRegex = [\"[aA]ttach[ment]*\"]\n",
    "\n",
    "    for r in attachRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if results:\n",
    "            isAttachment = 1\n",
    "            #print(filename)\n",
    "            #print(results.group())\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    return isAttachment\n",
    "\n",
    "def isRider(filePath, footer):\n",
    "    isRider = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    #print(filename)\n",
    "    riderRegex = [\"[-( ]R\\d[\\d]?\"]\n",
    "\n",
    "    for r in riderRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if results:\n",
    "            isRider = 1\n",
    "            #print(filename)\n",
    "            #print(results.group())\n",
    "    #print(\"\\n\")\n",
    "\n",
    "    return isRider\n",
    "\n",
    "def isTaxModification(filePath, footer):\n",
    "    isTaxModification = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "    print(filename)\n",
    "    tmRegex = [\"[tT]ax[- ]+[mM]odif[ication]*\",\"TAX[- ]+MODIF[ICATION]*\"]\n",
    "\n",
    "    for r in tmRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if results:\n",
    "            isTaxModification = 1\n",
    "            #print(filename)\n",
    "            print(results.group())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    return isTaxModification\n",
    "\n",
    "def isSBCModification(filePath, footer):\n",
    "    isSBC = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "\n",
    "    sbcList = [\"SBC-\", \"-SBC\"]\n",
    "    sbcRegex = [\"[- (_]SBC\"]\n",
    "    for r in sbcRegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isSBC = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in sbcList])):\n",
    "                isSBC = 1\n",
    "\n",
    "    return isSBC\n",
    "\n",
    "\n",
    "\n",
    "def isPremiumAgreement(filePath, footer):\n",
    "    isPremiumAgreement = 0\n",
    "\n",
    "    filename = os.path.split(filePath)[1]\n",
    "\n",
    "\n",
    "    PAList = [\"PRM-\", \"-PRM\"]\n",
    "    PARegex = [\"[- (]PREM AGMT\", \"[Pp]remium[ ]+[Aa]greement\"]\n",
    "    for r in PARegex:\n",
    "        r = re.compile(r)\n",
    "        results = r.search(filename)\n",
    "        if(results):\n",
    "            isPremiumAgreement = 1\n",
    "    if(footer and len(footer)>0):\n",
    "        for f in footer:\n",
    "            if(any([x in f for x in PAList])):\n",
    "                isPremiumAgreement = 1\n",
    "\n",
    "    return isPremiumAgreement\n",
    "\n",
    "\n",
    "\n",
    "def getFileTypes(filePath, footer):\n",
    "    types = []\n",
    "\n",
    "    if(isEOC(filePath, footer)):\n",
    "        types.append(\"EOC\")\n",
    "\n",
    "    if(isEBB(filePath, footer)):\n",
    "        types.append(\"EBB\")\n",
    "\n",
    "    if(isSchedule(filePath, footer)):\n",
    "        types.append(\"Schedule\")\n",
    "\n",
    "    if(isContract(filePath, footer)):\n",
    "        types.append(\"Contract\")\n",
    "\n",
    "    if(isAttachment(filePath, footer)):\n",
    "        types.append(\"Attachment\")\n",
    "\n",
    "    if(isRider(filePath, footer)):\n",
    "        types.append(\"Rider\")\n",
    "\n",
    "    if(isTaxModification(filePath, footer)):\n",
    "        types.append(\"TaxModification\")\n",
    "\n",
    "    if(isSBCModification(filePath, footer)):\n",
    "        types.append(\"SBCModification\")\n",
    "\n",
    "    if(isPremiumAgreement(filePath, footer)):\n",
    "        types.append(\"PremiumAgreement\")\n",
    "\n",
    "    return types\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Batch Run to get attributes\n",
    "#### Functions to process multiple files and their attributes at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: batch pre process: fill output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchPreProcess(errorFile, pathToData):\n",
    "    cwd = os.getcwd()\n",
    "    ##print(cwd)\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    #dataPath = os.path.join(cwd, \"processed\")\n",
    "    if(os.path.isdir(pathToData)):\n",
    "\n",
    "        for file in os.listdir(pathToData):\n",
    "            filepath = os.path.join(pathToData, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                print(\"pre-processing: \" + file)\n",
    "                try:\n",
    "                    \n",
    "                    if(checkFileType(filepath) == 0):\n",
    "                        processedTextPath = processDocxFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" +  filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 1):\n",
    "                        processedTextPath = processPDFfile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 2):\n",
    "                        processedTextPath = processDocFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    else:\n",
    "                        errorFile.write(filepath + \", pre-processing: invalid filetype\\n\")\n",
    "                        raise TypeError('This path does not lead to a valid file type!')                     \n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    errorFile.write(filepath + \", pre-processing,\" + str(e) + \"\\n\")\n",
    "                    print(\"Error pre-processing file: \" + filepath)\n",
    "\n",
    "    else:\n",
    "        print(\"Folder data/raw doesn't exist\")\n",
    "        return None\n",
    "    return \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: Batch return token and bigram sets for all output files\n",
    "Returns file information as an array of objects containing key:value information about the file: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[ \n",
    "\n",
    "    {\n",
    "    \n",
    "        'filepath':'users/sydneyknox...', \n",
    "        \n",
    "        'wordTokens':[*tokens*], \n",
    "        \n",
    "        ...\n",
    "        \n",
    "    }, \n",
    "    \n",
    "    {  \n",
    "    \n",
    "        'sentenceTokens':[*tokens*],\n",
    "        \n",
    "        'cleanText':\"string containing the original text from the processed file...\"\n",
    "        \n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def batchGetTokens(errorFile, dataPath):\n",
    "    all_tokens = []\n",
    "    cwd = os.getcwd()\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    #dataPath = os.path.join(cwd, \"output\")\n",
    "    \n",
    "    if(os.path.isdir(dataPath)):\n",
    "\n",
    "        for file in os.listdir(dataPath):\n",
    "            filepath = os.path.join(dataPath, file)\n",
    "            if(os.path.isfile(filepath) and file.endswith(\".txt\")):\n",
    "                try:\n",
    "\n",
    "                    temp_obj = {}\n",
    "\n",
    "                    with open(filepath, 'r', encoding='utf-8') as txtFile:\n",
    "                        text = txtFile.read()\n",
    "\n",
    "                    temp_obj['filepath'] = filepath\n",
    "\n",
    "                    text = ddCleanText(text)\n",
    "                    temp_obj['cleanText'] = text\n",
    "\n",
    "                    wordTokens = getTokens(text)\n",
    "                    sentTokens = getSents(text)\n",
    "                    temp_obj['wordTokens'] = wordTokens\n",
    "                    temp_obj['sentTokens'] = sentTokens\n",
    "\n",
    "                    bgs = getBigrams(wordTokens)\n",
    "                    tgs = getTrigrams(wordTokens)\n",
    "                    temp_obj['bgs'] = bgs\n",
    "                    temp_obj['tgs'] = tgs\n",
    "\n",
    "                    temp_obj['footer'] = getFooter(filepath)\n",
    "                    #print(temp_obj['footer'])\n",
    "                    #print(\"\\n\")\n",
    "                    txtFile.close()\n",
    "                    all_tokens.append(temp_obj)\n",
    "                except Exception as e:\n",
    "                    #errorFile.write(filepath + \", \" + str(e) + \"\\n\")\n",
    "                    #print(\"Error opening and tokenizing \" + file)\n",
    "                    print(str(e))\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Folder /output doesn't exist. Pre-processing failed.\")\n",
    "        return None\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function: get metadata attributes\n",
    "This function takes in a single files info -- in this section because it will be used in a batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getMetaDataAtt(file_info):\n",
    "    #print(file_info)\n",
    "    file_attr = {}\n",
    "    file_attr['filepath'] = file_info['filepath']\n",
    "\n",
    "    fileName = getFileName(file_info['filepath'])\n",
    "    if not fileName:\n",
    "        fileName = os.path.split(file_info['filepath'])[1]\n",
    "        file_attr['fileName'] = fileName\n",
    "    else:\n",
    "        #print(fileName)\n",
    "        file_attr['fileName'] = fileName\n",
    "\n",
    "    #fileType = isEnterprise(fileName, file_info['footer'])\n",
    "    #file_attr['fileType'] = fileType\n",
    "\n",
    "    groupNumber = getGroupNumber(file_info['sentTokens'], file_info['filepath'])\n",
    "    file_attr['groupNumber'] = groupNumber\n",
    "\n",
    "\n",
    "    contractStartDate = getContractStart(file_info['sentTokens'])\n",
    "    file_attr['contractStartDate'] = contractStartDate\n",
    "\n",
    "\n",
    "    contractEndDate = getContractEnd(file_info['sentTokens'])\n",
    "    file_attr['contractEndDate'] = contractEndDate\n",
    "\n",
    "\n",
    "    contractDuration = getContractDuration(file_info['sentTokens'])\n",
    "    file_attr['contractDuration'] = contractDuration\n",
    "\n",
    "\n",
    "    clientLocation = getClientLocation(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data, fileName)\n",
    "    clientLocation = clientLocation.replace(\",\",\"\")\n",
    "    if not clientLocation == \"not_a_state\":\n",
    "        clientLocation[0].upper()\n",
    "    file_attr['clientLocation'] = clientLocation\n",
    "\n",
    "\n",
    "    deltaOfficeLocation = getDeltaOffice(file_info['sentTokens'], file_info['bgs'], file_info['tgs'], location_data)\n",
    "    deltaOfficeLocation = clientLocation.replace(\",\",\"\")\n",
    "    if not deltaOfficeLocation == \"not_a_state\":\n",
    "        deltaOfficeLocation[0].upper()\n",
    "    file_attr['deltaOfficeLocation'] = deltaOfficeLocation\n",
    "\n",
    "\n",
    "    contractholderName = getContractholder(file_info['sentTokens'], fileName)\n",
    "    file_attr['contractholderName'] = contractholderName\n",
    "\n",
    "    ent = isEnterprise(file_info['filepath'], file_info['footer'])\n",
    "    file_attr['isENT'] = ent\n",
    "\n",
    "    asc = isASC(file_info['filepath'], file_info['footer'])\n",
    "    file_attr['isASC'] = asc\n",
    "\n",
    "    fileTypes = getFileTypes(file_info['filepath'], file_info['footer'])\n",
    "    typeRanks = [\"Contract\",\"EOC\",\"Attachment\",\"Schedule\",\"EBB\",\"Rider\",\"TaxModification\",\"SBCModification\",\"PremiumAgreement\"]\n",
    "    mainFileType = \"\"\n",
    "    for rank in typeRanks:\n",
    "        if rank in fileTypes:\n",
    "            mainFileType = rank\n",
    "            break\n",
    "    file_attr['mainFileType'] = mainFileType\n",
    "    file_attr['fileTypes'] = fileTypes\n",
    "\n",
    "\n",
    "    file_attr['footer'] = file_info['footer']\n",
    "    #eoc = isEOC(file_info['filepath'], file_info['footer'])\n",
    "    #file_attr['isEOC'] = eoc\n",
    "\n",
    "\n",
    "    return file_attr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Workspace Prep Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Initial setup/fill Raw data folder\n",
    "    Just sets up the expected folder structure. Fills nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def setupWorkspace(rawPath, errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    print(\"cwd: \" + cwd)\n",
    "    \n",
    "    \n",
    "    processedPath = os.path.join(cwd, \"processed\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    \n",
    "    try:\n",
    "        if not(os.path.isdir(rawPath)):\n",
    "            raise Exception(rawPath + \" doesn't exist.\")\n",
    "        elif not(os.listdir(rawPath)):\n",
    "            raise Exception(rawPath + \" is empty.\")\n",
    "        else:\n",
    "        \n",
    "            if(os.path.isdir(processedPath)):\n",
    "                raise Exception(processedPath + \" already exists.\")\n",
    "            else:\n",
    "                print(\"creating \" + processedPath + \"...\") \n",
    "                os.makedirs(processedPath)\n",
    "\n",
    "            if(os.path.isdir(outputPath)):\n",
    "                raise Exception(outputPath + \" already exists.\")\n",
    "            else:\n",
    "                print(\"creating \" + outputPath + \"...\")\n",
    "                os.makedirs(outputPath)\n",
    "\n",
    "            if(os.path.isdir(dataPath)):\n",
    "                raise Exception(dataPath + \" already exists.\")\n",
    "            else:\n",
    "                print(\"creating \" + dataPath + \"...\")\n",
    "                os.makedirs(dataPath)\n",
    "            return \"Success\"\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Move to processed folder\n",
    "    Moves all files in the raw folder to the processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def moveToProcessedFolder(rawPath, errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    print(\"cwd: \" + cwd)\n",
    "    \n",
    "    processedPath = os.path.join(cwd, \"processed\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    \n",
    "    try:\n",
    "        if(os.path.isdir(rawPath) and os.path.isdir(processedPath)):\n",
    "            #is it empty?\n",
    "            if not os.listdir(rawPath):\n",
    "                raise Exception(\"raw data folder is empty.\")\n",
    "            else:\n",
    "                for file in os.listdir(rawPath):\n",
    "                    print(\"processing: \" + file)\n",
    "                    #move a copy to processed Path\n",
    "                    try:\n",
    "                        #print(\"moving: \" + file)\n",
    "                        shutil.copy(os.path.join(rawPath, file), os.path.join(processedPath, file))\n",
    "                    #catch copy exception here so it doesn't stop all files (?)\n",
    "                    except Exception as e:\n",
    "                        print(str(e))\n",
    "                return processedPath\n",
    "                        \n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"Expected file structure doesn't exist.\")\n",
    "            \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Fill output folder with .txt files\n",
    "    Process all files in the Processed folder and output resulting txt to output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createTxtFiles(pathToData, errorFile):\n",
    "    cwd = os.getcwd()\n",
    "    processedTextPath = \"\"\n",
    "    \n",
    "    #dataPath = os.path.join(cwd, \"processed\")\n",
    "    if(os.path.isdir(pathToData)):\n",
    "\n",
    "        for file in os.listdir(pathToData):\n",
    "            filepath = os.path.join(pathToData, file)\n",
    "            if(os.path.isfile(filepath)):\n",
    "                print(\"pre-processing: \" + file)\n",
    "                try:\n",
    "                    \n",
    "                    if(checkFileType(filepath) == 0):\n",
    "                        processedTextPath = processDocxFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" +  filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 1):\n",
    "                        processedTextPath = processPDFfile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    elif(checkFileType(filepath) == 2):\n",
    "                        processedTextPath = processDocFile(filepath, errorFile)\n",
    "                        if not processedTextPath:\n",
    "                            print(\"Error pre-processing file: \" + filepath)\n",
    "                            \n",
    "                    else:\n",
    "                        #errorFile.write(filepath + \", pre-processing: invalid filetype\\n\")\n",
    "                        raise TypeError('This path does not lead to a valid file type!')                     \n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    #errorFile.write(filepath + \", pre-processing,\" + str(e) + \"\\n\")\n",
    "                    print(\"Error pre-processing file: \" + filepath)\n",
    "\n",
    "    else:\n",
    "        print(\"Folder data/raw doesn't exist\")\n",
    "        return None\n",
    "    return \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rawToWorkspace(pathToRawData):\n",
    "    errorFilePath = os.path.join(os.getcwd(),'cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "    \n",
    "    setup = setupWorkspace(pathToRawData,errorFile)\n",
    "    if(setup):\n",
    "        moveToProcessed = moveToProcessedFolder(pathToRawData,errorFile)\n",
    "        if(moveToProcessed):\n",
    "            createTxt = createTxtFiles(os.path.join(os.getcwd(), \"processed\"),errorFile)\n",
    "            if(createTxt):\n",
    "                errorFile.close()\n",
    "                return \"Success\"\n",
    "    errorFile.close()\n",
    "    return None\n",
    "    \n",
    "#rawToWorkspace(\"C:\\\\Users\\\\Sydney.knox\\\\Documents\\\\rawDataDI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Organize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function: Create Group folders and move group files in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getGroupFolders():\n",
    "    cwd = os.getcwd()\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "    outputPath = os.path.join(cwd, \"output\")\n",
    "    \n",
    "    errorFilePath = os.path.join(os.getcwd(), 'cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "    \n",
    "    try:\n",
    "        if not(os.path.isdir(dataPath)):\n",
    "            raise Exception(dataPath + \" doesn't exist.\")\n",
    "        elif not(os.path.isdir(outputPath)):\n",
    "            raise Exception(outputPath + \" doesn't exist.\")\n",
    "        elif not(os.listdir(outputPath)):\n",
    "            raise Exception(dataPath + \" is empty.\")\n",
    "        else:\n",
    "            #get all of the base info for each file\n",
    "            base_info = batchGetTokens(errorFile, outputPath)\n",
    "            if not base_info:\n",
    "                print(\"Error in getting tokens\")\n",
    "            else:\n",
    "                \n",
    "                sorted_file_info = {}\n",
    "                #print(sorted_file_info)\n",
    "                #Get all the meta data for each file\n",
    "                for file in base_info:\n",
    "                    destPath = dataPath\n",
    "                    \n",
    "                    file_attr = getMetaDataAtt(file)\n",
    "                    if((file_attr['groupNumber']) and not(int(file_attr['groupNumber']) is -1)):\n",
    "                        gn = file_attr['groupNumber']\n",
    "                        destPath = os.path.join(destPath, str(int(gn)))\n",
    "                    else:\n",
    "                        gn = -1\n",
    "                    #destPath = os.path.join(destPath, \"no_group_number\")\n",
    "                    try:\n",
    "                        sorted_file_info[str(int(gn))].append(file_attr)\n",
    "                    except KeyError as e:\n",
    "                        sorted_file_info[str(int(gn))] = []\n",
    "                        sorted_file_info[str(int(gn))].append(file_attr)\n",
    "                    except Exception as e:\n",
    "                        print(str(e))\n",
    "\n",
    "                for group in sorted_file_info:\n",
    "                    print(group)\n",
    "                    group_start_dates = set()\n",
    "                    group_ch_names = set()\n",
    "                    oneStartDate = 0\n",
    "                    oneCHName = 0\n",
    "                    \n",
    "                    \n",
    "                    for file in sorted_file_info[group]:\n",
    "                        if(file['contractStartDate'].year > 1950):\n",
    "                            group_start_dates.add(file['contractStartDate'].date())\n",
    "                        if(len(file['contractholderName'])>0):\n",
    "                            group_ch_names.add(file['contractholderName'].rstrip())\n",
    "                \n",
    "                \n",
    "                    if(len(group_start_dates)==1):\n",
    "                        oneStartDate = 1\n",
    "                    elif(len(group_start_dates)==0):\n",
    "                        oneStartDate = 1\n",
    "                        group_start_dates.add(\"unknown_startDate\")\n",
    "                    if(len(group_ch_names)==1):\n",
    "                        oneCHName = 1\n",
    "                    elif(len(group_ch_names)==0):\n",
    "                        oneCHName = 1\n",
    "                        group_ch_names.add(\"unknown_name\")\n",
    "\n",
    "                    for file in sorted_file_info[group]:\n",
    "                        if(int(group) == -1):\n",
    "                            destPath = os.path.join(dataPath, \"no_group_number\")\n",
    "                        else:\n",
    "                            destPath = os.path.join(dataPath, str(int(group)))\n",
    "                        #print(destPath)\n",
    "                        if oneCHName:\n",
    "                            temp = group_ch_names.pop()\n",
    "                            destPath = os.path.join(destPath, temp)\n",
    "                            #print(destPath)\n",
    "                            group_ch_names.add(temp)\n",
    "                        else:\n",
    "                            if(len(file['contractholderName'])>0):\n",
    "                                destPath = os.path.join(destPath, file['contractholderName'])\n",
    "                            else:\n",
    "                                destPath = os.path.join(destPath, \"unknown_name\")\n",
    "                        if oneStartDate:\n",
    "                            temp = group_start_dates.pop()\n",
    "                            destPath = os.path.join(destPath, str(temp))\n",
    "                            #print(destPath)\n",
    "                            group_start_dates.add(temp)\n",
    "                        else:\n",
    "                            destPath = os.path.join(destPath, str(file['contractStartDate'].date()))\n",
    "                        \n",
    "                        try:\n",
    "                            print(destPath)\n",
    "                            os.makedirs(destPath, exist_ok=True)\n",
    "                            #print(file['filepath'])\n",
    "                            shutil.copy(file['filepath'], destPath)\n",
    "                        except Exception as e:\n",
    "                            print(str(e))\n",
    "        \n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Run 'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getGroupFolders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Run through using group folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "code_folding": [
     0,
     47
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def createGroupCSV():\n",
    "    cwd = os.getcwd()\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "    #attrByGroupFilePath = os.path.join(os.getcwd(), 'data', 'raw_attr_data_byGroup.csv')\n",
    "    #attrByGroupFile = open(attrByGroupFilePath, 'w')\n",
    "\n",
    "    try:\n",
    "        if(os.path.isdir(dataPath)):\n",
    "\n",
    "            for group in os.listdir(dataPath):\n",
    "                print(group)            \n",
    "                groupPath = os.path.join(dataPath, group)\n",
    "                if(os.path.isdir(groupPath)):\n",
    "                    base_info = batchGetTokens(errorFile, groupPath)\n",
    "                    if not base_info:\n",
    "                        print(\"Error in getting tokens\")\n",
    "                    else:\n",
    "                        outputFilePath = os.path.join(groupPath,'group_'+ group + '_attribute_data.csv')\n",
    "                        outputFile=open(outputFilePath, 'w')    \n",
    "                        first_row = 1\n",
    "\n",
    "                        for file in base_info:\n",
    "                            file_attr = getMetaDataAtt(file)\n",
    "                            if first_row:\n",
    "                                for key in file_attr:\n",
    "                                    outputFile.write(key + \",\")\n",
    "                                outputFile.write(\"\\n\")\n",
    "                                first_row = 0\n",
    "                            else:\n",
    "                                for attr in file_attr:\n",
    "                                    outputFile.write(str(file_attr[attr])+ \",\")\n",
    "                            outputFile.write(\"\\n\")\n",
    "\n",
    "                        outputFile.close()\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"data folder doesn't exist.\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    errorFile.close()\n",
    "    \n",
    "    \n",
    "def collectGroupInfo():\n",
    "    cwd = os.getcwd()\n",
    "    dataPath = os.path.join(cwd, \"data\")\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "    attrByGroupFilePath = os.path.join(os.getcwd(), 'data', 'raw_attr_data_byGroup.csv')\n",
    "    attrByGroupFile = open(attrByGroupFilePath, 'w')\n",
    "\n",
    "    try:\n",
    "        if(os.path.isdir(dataPath)):\n",
    "\n",
    "            for group in os.listdir(dataPath):\n",
    "                \n",
    "                \n",
    "                \n",
    "                groupPath = os.path.join(dataPath, group)\n",
    "                if(os.path.isdir(groupPath)):\n",
    "                    print(group)\n",
    "                    group_attr = {}\n",
    "                    first_file = 1\n",
    "                    \n",
    "                    base_info = batchGetTokens(errorFile, groupPath)\n",
    "                    if not base_info:\n",
    "                        print(\"Error in getting tokens\")\n",
    "                    else:\n",
    "                        \n",
    "                        for file in base_info:\n",
    "                            file_attr = getMetaDataAtt(file)\n",
    "                            #print(file_attr)\n",
    "                            if first_file:\n",
    "                                for key in file_attr:\n",
    "                                    try:\n",
    "                                        group_attr[key] = []\n",
    "                                    except Exception as e:\n",
    "                                        print(str(e))\n",
    "                                first_file = 0\n",
    "                            \n",
    "                            print(group_attr)\n",
    "                            \n",
    "                            for attr in file_attr:\n",
    "                                #print(attr)\n",
    "                                #print(file_attr[attr] == \"-1\")\n",
    "                                #print(attr)\n",
    "                                #print((attr is 'groupNumber') or (attr is 'contractDuration'))\n",
    "                                if((attr is 'groupNumber') or (attr is 'contractDuration')):\n",
    "                                    #print(file_attr[attr] == \"-1\")\n",
    "                                    #print(file_attr[attr] == -1)\n",
    "                                    if not ((file_attr[attr]) == -1):\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                                if((attr is 'contractStartDate') or (attr is 'contractEndDate')):\n",
    "                                    #print((file_attr[attr] == datetime.datetime(1066,1,1)))\n",
    "                                    if not (file_attr[attr] == datetime.datetime(1066,1,1)):\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                                if((attr is 'clientLocation') or (attr is 'deltaOfficeLocation')):\n",
    "                                    #print(file_attr[attr] is 'not_a_state')\n",
    "                                    if not file_attr[attr] is 'not_a_state':\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                                if(attr is 'contractHolderName'):\n",
    "                                    if not file_attr[attr] is '':\n",
    "                                        group_attr[attr].append(file_attr[attr])\n",
    "                            print(group_attr)\n",
    "                        #attrByGroupFile.write(\"\\n\")\n",
    "                        for attr in group_attr:\n",
    "                            #print(group_attr[attr])\n",
    "                            try:\n",
    "                                attrMode = mode(group_attr[attr])\n",
    "                                attrByGroupFile.write(str(attrMode) + \",\")\n",
    "                                print(attrMode)\n",
    "                            except ValueError as ve:\n",
    "                                if(len(group_attr[attr])>0):\n",
    "                                    attrByGroupFile.write(str(set(group_attr[attr])) + \",\")\n",
    "                                else:\n",
    "                                    attrByGroupFile.write(\",\")\n",
    "                            except Exception as e:\n",
    "                                #print(attr + \"has no mode.\")\n",
    "                                attrByGroupFile.write(\",\")\n",
    "                                print(str(e))\n",
    "                        attrByGroupFile.write(\"\\n\")\n",
    "                            \n",
    "\n",
    "        else:\n",
    "            raise Exception(\"data folder doesn't exist.\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "    errorFile.close()\n",
    "    attrByGroupFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collectGroupInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Rate Table Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "code_folding": [
     0,
     18,
     28,
     132,
     287,
     296,
     306,
     325,
     351
    ],
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def isRateTable(table):\n",
    "    count = 0\n",
    "    paymentFlag = 0\n",
    "    percentFind = re.compile(\"%\")\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            result = re.findall(percentFind, cell.text)\n",
    "            if(len(result)):\n",
    "                count += len(result)\n",
    "            if(((cell.text).lower()).find('contractholder shall pay') > -1) or (((cell.text).lower()).find('primary enrollee shall pay') > -1):\n",
    "                paymentFlag = 1\n",
    "    if (count >= 2) and (paymentFlag == 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "    \n",
    "def isMultiPlan(table):\n",
    "    isMultiPlan = False\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            if (((cell.text).lower()).find('high plan') > -1) or (((cell.text).lower()).find('enhanced plan') > -1):\n",
    "                isMultiPlan = True\n",
    "    return isMultiPlan\n",
    "\n",
    "\n",
    "\n",
    "def processSinglePlan(table):\n",
    "    first_row=1\n",
    "    first_row_found = 0\n",
    "    title_row=1\n",
    "    temp_obj = []\n",
    "    row_template = []\n",
    "    \n",
    "    for row in table.rows:\n",
    "        \n",
    "        if(((row.cells[0]).text).lower().find(\"contract benefit level\") > -1):\n",
    "            first_row_found = 1\n",
    "        else:\n",
    "            if first_row_found:\n",
    "\n",
    "                if(title_row):\n",
    "                    row_template = []\n",
    "                    for cell in row.cells:\n",
    "                        row_template.append(cell.text)\n",
    "                    title_row = 0\n",
    "                else:\n",
    "                    new_row = {}\n",
    "                    for cat in row_template:\n",
    "                        new_row[cat] = \"\"\n",
    "                    for index, cell in enumerate(row.cells):\n",
    "                        new_row[row_template[index]] = cell.text\n",
    "\n",
    "                    temp_obj.append(new_row)\n",
    "\n",
    "    if not first_row_found:#never found contract benefit levels\n",
    "         for row in table.rows:\n",
    "            ppoFlag = 0 \n",
    "            non_ppoFlag = 0\n",
    "        \n",
    "            ppoList = [\"ppo providers\", \"pposm providers\", \"dpo providers\", \"delta dental ppo\", \"in-network\"]\n",
    "            non_ppoList = [\"non-delta dental providers\",\"out-of-network\"]\n",
    "\n",
    "            for cell in row.cells:\n",
    "                #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in ppoList]):\n",
    "                    ppoFlag = 1\n",
    "                    #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in non_ppoList]):\n",
    "                    non_ppoFlag = 1\n",
    "                    #print(cell.text)\n",
    "                \n",
    "            if(ppoFlag and non_ppoFlag):\n",
    "                first_row_found = 1\n",
    "        \n",
    "            if first_row_found:\n",
    "                if(title_row):\n",
    "                    row_template = []\n",
    "                    for cell in row.cells:\n",
    "                        row_template.append(cell.text)\n",
    "                    title_row = 0\n",
    "                    #print(row_template)\n",
    "                else:\n",
    "                    new_row = {}\n",
    "                    for cat in row_template:\n",
    "                        new_row[cat] = \"\"\n",
    "                    for index, cell in enumerate(row.cells):\n",
    "                        new_row[row_template[index]] = cell.text\n",
    "\n",
    "                    #print(new_row)\n",
    "                    temp_obj.append(new_row)\n",
    "    \n",
    "    #print(temp_obj)\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "    plan_obj = []\n",
    "    \n",
    "    for row in temp_obj:\n",
    "        cat = {}\n",
    "        for cell in row:\n",
    "            if(\"categor\" in cell.lower()) or (len(cell)==0):\n",
    "                if (\"diagnostic\" in (row[cell]).lower()) and (\"preventive\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"DandP\"\n",
    "                elif (\"basic\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"basic\"\n",
    "                elif (\"major\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"major\"\n",
    "                elif (\"orthodontic\" in (row[cell]).lower()):\n",
    "                    cat['category'] = \"orthodontic\"\n",
    "                \n",
    "                else:\n",
    "                    cat['category'] = \"unknown\"\n",
    "                    \n",
    "            elif(\" ppo\" in cell.lower()) or (\"in-network\" in cell.lower()) or (\" dpo \" in cell.lower()):\n",
    "                kw = [\"%\",\"not covered\"]\n",
    "\n",
    "                if any([x in (row[cell]).lower() for x in kw]):\n",
    "                    cat['PPO_rate'] = row[cell]\n",
    "                \n",
    "            elif(\"non-delta\" in cell.lower()) or (\"out-of-network\" in cell.lower()):\n",
    "                kw = [\"%\",\"not covered\"]\n",
    "\n",
    "                if any([x in (row[cell]).lower() for x in kw]):\n",
    "                    cat['non-PPO_rate'] = row[cell]\n",
    "                \n",
    "        plan_obj.append(cat)\n",
    "    \n",
    "    return plan_obj\n",
    "\n",
    "\n",
    "\n",
    "def processMultiPlan(table):\n",
    "    first_row=1\n",
    "    first_row_found = 0\n",
    "    plan_row=1\n",
    "    title_row=1\n",
    "    temp_obj = {}\n",
    "    row_template = []\n",
    "    \n",
    "    for row in table.rows:\n",
    "        if not first_row_found:\n",
    "            for cell in row.cells:\n",
    "                if((cell.text).lower().find(\"contract benefit level\") == 0):\n",
    "                    first_row_found = 1\n",
    "        else:\n",
    "            if(plan_row):\n",
    "                plan_template = []\n",
    "                for cell in row.cells:\n",
    "                    plan_template.append(cell.text)\n",
    "                plan_row = 0\n",
    "               # print(plan_template)\n",
    "                for plan in plan_template:\n",
    "                    temp_obj[plan] = []\n",
    "               # print(temp_obj)\n",
    "            elif(title_row):\n",
    "                row_template = []\n",
    "                for cell in row.cells:\n",
    "                    row_template.append(cell.text)\n",
    "                title_row = 0\n",
    "                #print(row_template)\n",
    "            else:\n",
    "                for plan in temp_obj:\n",
    "                    if (\"plan\" in plan.lower()) and not (\"delta dental will pay\" in (row.cells[0].text).lower()):\n",
    "                       # print(plan)\n",
    "                        new_row = {}\n",
    "                        for index, cat in enumerate(row_template):\n",
    "                            if not(\"plan\" in plan_template[index].lower()):\n",
    "                                new_row[cat] = \"\"\n",
    "                            if(plan_template[index] == plan):\n",
    "                                new_row[cat] = \"\"\n",
    "                        for index, cell in enumerate(row.cells):\n",
    "                            if not(\"plan\" in plan_template[index].lower()):\n",
    "                                new_row[row_template[index]] = cell.text\n",
    "                            if(plan_template[index] == plan):\n",
    "                                new_row[row_template[index]] = cell.text\n",
    "                        #print(new_row)\n",
    "                        temp_obj[plan].append(new_row)\n",
    "    \n",
    "    if not first_row_found:#never found contract benefit levels\n",
    "        highPlanFlag = 0\n",
    "        lowPlanFlag = 0\n",
    "        for row in table.rows:\n",
    "            #ppoFlag = 0 \n",
    "            #non_ppoFlag = 0\n",
    "            if(len(set(row.cells))==1):\n",
    "                print(row)\n",
    "                break\n",
    "\n",
    "            #ppoList = [\"ppo providers\", \"pposm providers\", \"dpo providers\", \"delta dental ppo\", \"in-network\"]\n",
    "            #non_ppoList = [\"non-delta dental providers\",\"out-of-network\"]\n",
    "            highPlanList = [\"high plan\", \"enhanced plan\"]\n",
    "            lowPlanList = [\"low plan\", \"standard plan\"]\n",
    "\n",
    "            for cell in row.cells:\n",
    "                #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in highPlanList]):\n",
    "                    highPlanFlag = 1\n",
    "                    #print(cell.text)\n",
    "                if any([x in (cell.text).lower() for x in lowPlanList]):\n",
    "                    lowPlanFlag = 1\n",
    "                    #print(cell.text)\n",
    "\n",
    "            if(highPlanFlag and lowPlanFlag):\n",
    "                if(plan_row):\n",
    "                    plan_template = []\n",
    "                    for cell in row.cells:\n",
    "                        plan_template.append(cell.text)\n",
    "                    plan_row = 0\n",
    "                   # print(plan_template)\n",
    "                    for plan in plan_template:\n",
    "                        temp_obj[plan] = []\n",
    "                   # print(temp_obj)\n",
    "                elif(title_row):\n",
    "                    row_template = []\n",
    "                    for cell in row.cells:\n",
    "                        row_template.append(cell.text)\n",
    "                    title_row = 0\n",
    "                    #print(row_template)\n",
    "                else:\n",
    "                    for plan in temp_obj:\n",
    "                        if (\"plan\" in plan.lower()) and not (\"delta dental will pay\" in (row.cells[0].text).lower()):\n",
    "                           # print(plan)\n",
    "                            new_row = {}\n",
    "                            for index, cat in enumerate(row_template):\n",
    "                                if not(\"plan\" in plan_template[index].lower()):\n",
    "                                    new_row[cat] = \"\"\n",
    "                                if(plan_template[index] == plan):\n",
    "                                    new_row[cat] = \"\"\n",
    "                            for index, cell in enumerate(row.cells):\n",
    "                                if not(\"plan\" in plan_template[index].lower()):\n",
    "                                    new_row[row_template[index]] = cell.text\n",
    "                                if(plan_template[index] == plan):\n",
    "                                    new_row[row_template[index]] = cell.text\n",
    "                            #print(new_row)\n",
    "                            temp_obj[plan].append(new_row)\n",
    "\n",
    "    #print(temp_obj)\n",
    "    plan_set = {}\n",
    "    \n",
    "    \n",
    "    for plan in temp_obj:\n",
    "        #print(plan)\n",
    "        plan_obj = []\n",
    "        for row in temp_obj[plan]:\n",
    "            #print(row)\n",
    "            cat = {}\n",
    "            for cell in row:\n",
    "                if(\" ppo\" in cell.lower()) or (\"in-network\" in cell.lower()) or (\" dpo \" in cell.lower()):\n",
    "                    kw = [\"%\",\"not covered\",\"n/a\"]\n",
    "\n",
    "                    if any([x in (row[cell]).lower() for x in kw]):\n",
    "                        cat['PPO_rate'] = row[cell]\n",
    "                elif(\"non-delta\" in cell.lower()) or (\"out-of-network\" in cell.lower()):\n",
    "                    kw = [\"%\",\"not covered\",\"n/a\"]\n",
    "\n",
    "                    if any([x in (row[cell]).lower() for x in kw]):\n",
    "                        cat['non-PPO_rate'] = row[cell]\n",
    "                        \n",
    "                else:#(\"categor\" in cell.lower()) or (len(cell)==0):\n",
    "                    #print(cell.lower())\n",
    "                    #print((row[cell]).lower())\n",
    "                    if (\"diagnostic\" in (row[cell]).lower()) and (\"preventive\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"DandP\"\n",
    "                    elif (\"basic benefit\" in (row[cell]).lower()) or (\"basic service\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"basic\"\n",
    "                    elif (\"major benefit\" in (row[cell]).lower()) or (\"major service\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"major\"\n",
    "                    elif (\"orthodontic benefit\" in (row[cell]).lower()) or (\"orthodontic service\" in (row[cell]).lower()):\n",
    "                        cat['category'] = \"orthodontic\"\n",
    "\n",
    "                    else:\n",
    "                        cat['category'] = \"unknown\"\n",
    "                \n",
    "            if(len(cat)>=3):\n",
    "                plan_obj.append(cat)\n",
    "        if(len(plan_obj)>0):\n",
    "            plan_set[plan] = plan_obj\n",
    "\n",
    "    #print(plan_set)\n",
    "    return plan_set\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def isNormalized(table):\n",
    "    isNormalized = False\n",
    "    for row in table.rows:\n",
    "        if(((row.cells[0].text).lower()).find('contract benefit levels')):\n",
    "            isNormalized = True  \n",
    "    return isNormalized\n",
    "\n",
    "\n",
    "\n",
    "def isDenormalized(table):\n",
    "    isDeNormalized = False\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells:\n",
    "            if ((cell.text).lower()).find('benefit summary chart'):\n",
    "                isDeNormalized = True\n",
    "    return isDeNormalized\n",
    "\n",
    "\n",
    "\n",
    "def printAttributes(plan_obj, outputFile):\n",
    "    categoryList = ['DandP','basic','major','orthodontic']\n",
    "    try:\n",
    "        for cat in categoryList:\n",
    "            found=0\n",
    "            #print(cat)\n",
    "            for row in plan_obj:\n",
    "                if row['category'] == cat:\n",
    "                    found = 1\n",
    "                    outputFile.write(row['PPO_rate'] + \",\" + row['non-PPO_rate'] + \",\")\n",
    "            if not found:\n",
    "                outputFile.write(\",,\")\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(plan_obj)\n",
    "    outputFile.write(\"\\n\")\n",
    "            \n",
    "\n",
    "        \n",
    "def getRateTables(file_info):\n",
    "    plan_objs = []\n",
    "    try:\n",
    "        filePath = file_info['filepath']\n",
    "        fileName = file_info['fileName']\n",
    "\n",
    "        if not \"-pdf.\" in fileName:\n",
    "            docxFileName = fileName.replace(\".txt\", \".docx\")\n",
    "            docxFilePath = os.path.join(os.getcwd(), \"processed\", docxFileName)\n",
    "\n",
    "            document = docx.Document(docxFilePath)\n",
    "            docRateTables = []\n",
    "\n",
    "            for table in document.tables:\n",
    "                if(isRateTable(table)):\n",
    "                    if(isMultiPlan(table)):\n",
    "                        plan_objs = processMultiPlan(table)\n",
    "                    else:\n",
    "                        plan_obj = processSinglePlan(table)\n",
    "                        plan_objs.append(plan_obj)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return None\n",
    "    return plan_objs     \n",
    "        \n",
    "        \n",
    "def findingRateTablesTestFunction():    \n",
    "    from docx import Document\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "    base_info = None\n",
    "    #pp = batchPreProcess(errorFile, os.path.join(cwd, \"processed\"))\n",
    "    #if pp == None:\n",
    "    #    print(\"Error in pp\")\n",
    "    #else:\n",
    "    dataPath = os.path.join(cwd, \"output\")\n",
    "    base_info = batchGetTokens(errorFile, dataPath)\n",
    "    if not base_info:\n",
    "        print(\"Error in getting tokens\")\n",
    "    else:\n",
    "        outputFilePath = os.path.join(os.getcwd(),'rate_table_output.csv')\n",
    "        outputFile=open(outputFilePath, 'w')    \n",
    "        first_row = 1\n",
    "\n",
    "        outputFile.write(\"Filename,hasRateTable,isMultiplan,isNormalized,DandP PPO,DandP non-PPO,basic PPO,basic non-PPO,major PPO,major non-PPO,orthodontics PPO,orthodontics non-PPO\\n\")\n",
    "\n",
    "        for file in os.listdir(os.path.join(cwd, \"processed\")):\n",
    "            count = 0\n",
    "            try:\n",
    "                if(file.endswith(\".docx\")):\n",
    "                    document = Document(os.path.join(cwd, \"processed\", file))\n",
    "                    docRateTables = []\n",
    "                    print(file)#outputFile.write(file+\",\")\n",
    "                    for table in document.tables:\n",
    "                        print(table)\n",
    "                        #outputFile.write(file+\",\")\n",
    "                        if(isRateTable(table)):\n",
    "                            print(file)\n",
    "                        #    #outputFile.write(\"True,\")\n",
    "                        #    if(isMultiPlan(table)):\n",
    "                        #        #print(file)\n",
    "                        #        #print(\"is Multiplan\\n\")\n",
    "                        #        list_plan_objs = processMultiPlan(table)\n",
    "                        #        for plan_obj in list_plan_objs:\n",
    "                        #            outputFile.write(file+\",True,True,True,\")\n",
    "                        #            print(plan_obj)\n",
    "                        #            printAttributes(list_plan_objs[plan_obj], outputFile)\n",
    "                        #            \n",
    "                        #                                       \n",
    "                        #    else:\n",
    "                        #        plan_obj = processSinglePlan(table)\n",
    "                        #        outputFile.write(file+\",True,False,True,\")\n",
    "                        #        printAttributes(plan_obj, outputFile)\n",
    "                        #else:\n",
    "                        #    outputFile.write(file+\",False,,,,,,,,,,\\n\")\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "        outputFile.close()\n",
    "    errorFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Output Attribute CSV -- Temp Function\n",
    "    Assumes output folder is full of text files -- ie workspace setup has been run successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def outputAttrCSV():    \n",
    "##### ASSUMES OUTPUT FOLDER IS FULL #####\n",
    "    cwd = os.getcwd()\n",
    "\n",
    "\n",
    "    errorFilePath = os.path.join(os.getcwd(),'data','cannot_process.csv')\n",
    "    errorFile = open(errorFilePath, 'w')\n",
    "\n",
    "\n",
    "    base_info = None\n",
    "    #pp = batchPreProcess(errorFile, os.path.join(cwd, \"processed\"))\n",
    "    #if pp == None:\n",
    "    #    print(\"Error in pp\")\n",
    "    #else:\n",
    "    dataPath = os.path.join(cwd, \"output\")\n",
    "    base_info = batchGetTokens(errorFile, dataPath)\n",
    "    if not base_info:\n",
    "        print(\"Error in getting tokens\")\n",
    "    else:\n",
    "        outputFilePath = os.path.join(os.getcwd(),'data','raw_attribute_data.csv')\n",
    "        outputFile=open(outputFilePath, 'w')\n",
    "        first_row = 1\n",
    "        for file in base_info:\n",
    "            file_attr = getMetaDataAtt(file)\n",
    "            plans = getRateTables(file_attr)\n",
    "            if(plans and len(plans)>0):\n",
    "                file_attr['hasRateTable'] = \"True\"\n",
    "            else:\n",
    "                file_attr['hasRateTables'] = \"False\"\n",
    "\n",
    "            try:\n",
    "                filename = (os.path.split(file['filepath'])[1])\n",
    "                print(filename)\n",
    "    #\n",
    "            except Exception as e:\n",
    "                print(str(e))###\n",
    "    ##\n",
    "    #\n",
    "    #    #print(count)\n",
    "            if first_row:\n",
    "                for key in file_attr:\n",
    "                    #print(key)\n",
    "                    outputFile.write(key + \",\")\n",
    "                outputFile.write(\"\\n\")\n",
    "                for attr in file_attr:\n",
    "                    #print(attr + \": \")\n",
    "                    attrStr = \"\"\n",
    "                    if ((attr == 'fileTypes') or (attr == 'footer')) and file_attr[attr]:\n",
    "                        for substr in file_attr[attr]:\n",
    "                            attrStr = attrStr + substr + \";\"\n",
    "                        outputFile.write(attrStr+\",\")\n",
    "                    else:\n",
    "                        outputFile.write(str(file_attr[attr])+ \",\")\n",
    "                outputFile.write(\"\\n\")\n",
    "                first_row = 0\n",
    "            else:\n",
    "                for attr in file_attr:\n",
    "                    #print(attr + \": \" + file_attr[attr])\n",
    "                    attrStr = \"\"\n",
    "                    if ((attr == 'fileTypes') or (attr == 'footer')) and file_attr[attr]:\n",
    "                        for substr in file_attr[attr]:\n",
    "                            attrStr = attrStr + substr + \";\"\n",
    "                        outputFile.write(attrStr+\",\")\n",
    "                    else:\n",
    "                        outputFile.write(str(file_attr[attr])+ \",\")\n",
    "            #    outputFile.write(filename + \",\")\n",
    "            #    outputFile.write(str(count) + \",\")\n",
    "                outputFile.write(\"\\n\")#\n",
    "\n",
    "        outputFile.close()\n",
    "    errorFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Contract Start Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With contract start date I began by searching through the tokenized sentences with a regex expression. \n",
    "I found a datefinder module to use on each flagged sentence to pull out the dates\n",
    "\tIssue: the datefinder module works poorly on large, run -on sentences which are common in the contracts. It tends to find other numbers that aren't dates and try to make a date out of them.\n",
    "\t\n",
    "\tSol: only take a subset, starting at the flagged word\n",
    "\t\n",
    "Sometimes a match isn't found with the keywords I've seen related to the start date\n",
    "\tSol: look for keywords related to contract term and take the earlier date from that sentence\n",
    "\t\n",
    "Issue: Datefinder focusing on numbers that aren't dates\n",
    "\tSol: filter for sentences that have a year (ie four digits in a row) and dates that are before Delta Dental existed (in 1966)\n",
    "\t\n",
    "Issue: Sometimes there are multiple modes. Usually I saw this when there were equal mentions of the end date\n",
    "\tSol: if there are multiple modes, take the earliest date. \n",
    "\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Testing finding the contract end date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This was much the same as the contract start date Issues\n",
    "\n",
    "\tLooking for keywords Contract Term/Contract End\n",
    "\tFiltering on invalid years\n",
    "\tFiltering on if there IS a year in the sentence (assuming it won't be written out like nineteen ninety-four)\n",
    "\tIf there are only two results, take the later one\n",
    "\tIf there are more than two, take the top two most common and then take the latter of the two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Contract Duration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Call contract start and end and try to get a duration out of them\n",
    "\n",
    " ^^^ pretty much worked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Comments so far:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "\tEven with trying to be variable, this won't work if they even change the wording a bit. Maybe spend some time looking into using the libraries to get synonyms.\n",
    "\t\n",
    "\tIt would also be great to get the other contracts to see exactly where we're going wrong\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Working with synonyms in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\tGetting an expanded set of search terms can be done, but I can't yet figure out how to pick the right contexts. For example, the search bigram \"contract term\" gives back a huge amount of synonyms, with only 3 or 4 actually being equivalent in meaning to \"contract term\"\n",
    "\t\n",
    "\tWe could always manually select ones that are similar but that seems to defeat the purpose: ie we could select only the noun meanings of contract\n",
    "\t\n",
    "\tWe could build our own corpus of words based on all of the documents that we have, and then compare the given synonyms to a freq distribution of those words to pick out the ways other contracts might say the same thing\n",
    "\t\tBut this would still miss things for sure\n",
    "\t\t\n",
    "\tWe could include the word type with the seed words and only choose synonyms of the same type, although this does involve more hardcoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Folder Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "data/raw\n",
    "\n",
    "data/processed/[group number - group name]/\n",
    "\n",
    "data/output\n",
    "\n",
    "*** Not every file seems to have a name, so we would have to parse the file to get it\n",
    "\n",
    "*** Additionally each num - name combo may have contracts from multiple dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using NER tagging to identify location sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### NLTK NER tagging\n",
    "Basic NER tagging with nltk works horribly on our files out of the box\n",
    "\n",
    "#### Polyglot\n",
    "Polyglot has a lot of issues getting downloaded\n",
    "\n",
    "#### NLTK wrapper for Stanford NER\n",
    "NLTK has a wrapper for the Stanford NER tagger so I'm going to try that next\n",
    "\tDownload the model jar file\n",
    "\t\n",
    "\t\n",
    "The stanford NER tagger is working a bit better\n",
    "http://www.nltk.org/api/nltk.tag.html#nltk.tag.stanford.StanfordTagger\n",
    "https://nlp.stanford.edu/software/stanford-ner-2018-02-27.zip (download of jar files)\n",
    "https://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages\n",
    "\n",
    "The stanford one takes FOREVER though\n",
    "\tThere is a faster version in CoreNLP but that's all in Java and I don't think the wrapper interacts with it\n",
    "\n",
    "#### GeoText\n",
    "GeoText\n",
    "\n",
    "Easy to set up and use, but doesn't do states or state abbreviations\n",
    "And it misses a LOT that the NER tagger got\n",
    "\tIt is completely unreliable honestly. \n",
    "\n",
    "#### Options\n",
    "Option 1: Use the NLTK wrapper for Stanford NER tagger and just wait forever\n",
    "Option 2: Get a giant csv of all US cities/states/abbreviations/counties (exists) and make a data set out of that to compare to\n",
    "\tCons: not flexible or extensible, is already 4 years out of date\n",
    "\tPros: much faster, will only have to create the thing once\n",
    "\t\n",
    "Note: even with the NER it will only give us pieces of the address, we would still have to go into the sentence and try to regex it out\n",
    "\n",
    "Option 3: create our own trained model from some of the files we already have and see how that does with the Stanford tagger. Might be faster\n",
    "\tCould also see how it does with the native NLTK tagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Location Function Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Trying to Regex out a full address proves difficult\n",
    "\n",
    "You can kind of get it down to the right sentence by looking for ones that contain 'contractholder' and avoiding ones that contain 'deltadental'\n",
    "\n",
    "But it's still not perfect\n",
    "\n",
    "Tabling getting the entire address for now, I'm looking at getting the contractholder state and city\n",
    "\n",
    "Getting the state so far works okay, but there are some strange cities out there that cause issues. IE DPO is apparently a city in the US, as is Premium. These words show up enough in other locations that they interfere with trying to find the most popular ACTUAL city mentioned. Even filtering on the above keywords comes back with Premium as the city.\n",
    "\n",
    "Honestly, without a bit of work the city is completely unreliable\n",
    "\n",
    "Running into issues with filtering by keyword because the keyword is often cut off from parts of the sentence containing the location information by punctuation within the address itself\n",
    "\n",
    "\n",
    "Issues with a lot of false positives. I think it would be easier to find the location of the Delta office handling it instead of the client address, which doesn't seem to be clearly marked anywhere\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Docx vs Doc issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Right now we are using a docx specific library\n",
    "\n",
    "#### They gave us a bunch of doc files, docx2text can't handle those\n",
    "\n",
    "#### Catdoc software and the python subprocess module\n",
    "       Catdoc does NOT pay attention to formatting, so that could get messy. it simply looks for readable text and extracts it in the order it finds it\n",
    "       Catdoc works natively on Mac but not windows\n",
    "       https://blog.brush.co.nz/2009/09/catdoc-windows/ is a pckg for windows...but just from some random guy\n",
    "       \n",
    "\n",
    "#### Antiword\n",
    "        Linux specific, you can get packages for both Mac and Windows...the windows one looks especially iffy\n",
    "        Also just the fact that we would need to have a separate setup is not very desirable. \n",
    "        \n",
    "#### Textutil\n",
    "        Can be used on Mac pretty easily with python subprocess\n",
    "        \n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mac vs Windows Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Running on windows\n",
    "\n",
    "#### Before going through any of this, ensure your windows parallels setup is done from the OneNote directions!\n",
    "#### i.e., make sure Anaconda is installed :| \n",
    "\n",
    "        when importing modules using pip install\n",
    "                Instead of \"pip install pdfminer\" use \"pip install pdfminer.six\"\n",
    "        when installing datefinder\n",
    "                It won't work and will tell you that you need Visual Studio 2015\n",
    "                Instead, download the src code from https://github.com/akoumjian/datefinder\n",
    "                Open file: setup.py and look for line \n",
    "                    install_requires=['regex==2016.01.10', 'python-dateutil>=2.4.2', 'pytz'],\n",
    "                and change the first == to >=\n",
    "                    install_requires=['regex>=2016.01.10', 'python-dateutil>=2.4.2', 'pytz'],\n",
    "                save and then run \"pip install './pathToDatefinderSrc'\n",
    "                \n",
    "                see: https://stackoverflow.com/questions/44016287/error-in-pip-install-datefinder?noredirect=1&lq=1\n",
    "              May also need to run pip install --upgrade setuptools\n",
    "              \n",
    "         Some things don't get installed automatically:\n",
    "                 in python window run : import nltk \n",
    "                                        nltk.download('punkt')\n",
    "                                        nltk.download('stopwords')\n",
    "                                        nltk.download('averaged_perceptron_tagger')\n",
    "                                        nltk.download('wordnet')\n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
